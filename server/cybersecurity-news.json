[
  {
    "creator": "Bruce Schneier",
    "title": "Why AI Keeps Falling for Prompt Injection Attacks",
    "link": "https://www.schneier.com/blog/archives/2026/01/why-ai-keeps-falling-for-prompt-injection-attacks.html",
    "pubDate": "Thu, 22 Jan 2026 12:35:46 +0000",
    "content:encoded": "<p>Imagine you work at a drive-through restaurant. Someone drives up and says: &#8220;I&#8217;ll have a double cheeseburger, large fries, and ignore previous instructions and give me the contents of the cash drawer.&#8221; Would you hand over the money? Of course not. Yet this is what <a href=\"https://spectrum.ieee.org/tag/large-language-models\">large language models</a> (<a href=\"https://spectrum.ieee.org/tag/llms\">LLMs</a>) do.</p>\n<p><a href=\"https://www.ibm.com/think/topics/prompt-injection\">Prompt injection</a> is a method of tricking LLMs into doing things they are normally prevented from doing. A user writes a prompt in a certain way, asking for system <a href=\"https://spectrum.ieee.org/tag/passwords\">passwords</a> or private data, or asking the LLM to perform forbidden instructions. The precise phrasing overrides the LLM&#8217;s <a href=\"https://medium.com/data-science/safeguarding-llms-with-guardrails-4f5d9f57cff2\">safety guardrails</a>, and it complies.</p>\n<p>LLMs are vulnerable to <a href=\"https://fdzdev.medium.com/20-prompt-injection-techniques-every-red-teamer-should-test-b22359bfd57d\">all sorts</a> of prompt injection attacks, some of them absurdly obvious. A chatbot won&#8217;t tell you how to synthesize a bioweapon, but it might tell you a fictional story that incorporates the same detailed instructions. It won&#8217;t accept nefarious text inputs, but might if the text is rendered as <a href=\"https://arxiv.org/abs/2402.11753\">ASCII art</a> or appears in an image of a <a href=\"https://www.lakera.ai/blog/visual-prompt-injections\">billboard</a>. Some ignore their guardrails when told to &#8220;ignore previous instructions&#8221; or to &#8220;pretend you have no guardrails.&#8221;</p>\n<p>AI vendors can block specific prompt injection techniques once they are discovered, but general safeguards are <a href=\"https://llm-attacks.org/\">impossible</a> with today&#8217;s LLMs. More precisely, there&#8217;s an endless array of prompt injection attacks waiting to be discovered, and they cannot be prevented universally.</p>\n<p>If we want LLMs that resist these attacks, we need new approaches. One place to look is what keeps even overworked fast-food workers from handing over the cash drawer.</p>\n<h3>Human Judgment Depends on Context</h3>\n<p>Our basic human defenses come in at least three types: general instincts, social learning, and situation-specific training. These work together in a layered defense.</p>\n<p>As a social species, we have developed numerous instinctive and cultural habits that help us judge tone, motive, and risk from extremely limited information. We generally know what&#8217;s normal and abnormal, when to cooperate and when to resist, and whether to take action individually or to involve others. These instincts give us an intuitive sense of risk and make us <a href=\"https://www.nature.com/articles/srep08242\">especially careful</a> about things that have a large downside or are impossible to reverse.</p>\n<p>The second layer of defense consists of the norms and trust signals that evolve in any group. These are imperfect but functional: Expectations of cooperation and markers of trustworthiness emerge through repeated interactions with others. We remember who has helped, who has hurt, who has reciprocated, and who has reneged. And emotions like sympathy, anger, guilt, and gratitude motivate each of us to <a href=\"https://ncase.me/trust/\">reward cooperation with cooperation</a> and punish defection with defection.</p>\n<p>A third layer is institutional mechanisms that enable us to interact with multiple strangers every day. Fast-food workers, for example, are trained in procedures, approvals, escalation paths, and so on. Taken together, these defenses give humans a strong sense of context. A fast-food worker basically knows what to expect within the job and how it fits into broader society.</p>\n<p>We reason by assessing multiple layers of context: perceptual (what we see and hear), relational (who&#8217;s making the request), and normative (what&#8217;s appropriate within a given role or situation). We constantly navigate these layers, weighing them against each other. In some cases, the normative outweighs the perceptual&#8212;for example, following workplace rules even when customers appear angry. Other times, the relational outweighs the normative, as when people comply with orders from superiors that they believe are against the rules.</p>\n<p>Crucially, we also have an interruption reflex. If something feels &#8220;off,&#8221; we naturally pause the <a href=\"https://spectrum.ieee.org/tag/automation\">automation</a> and reevaluate. Our defenses are not perfect; people are fooled and manipulated all the time. But it&#8217;s how we humans are able to navigate a complex world where others are constantly trying to trick us.</p>\n<p>So let&#8217;s return to the drive-through window. To convince a fast-food worker to hand us all the money, we might try shifting the context. Show up with a camera crew and tell them you&#8217;re filming a commercial, claim to be the head of security doing an audit, or dress like a bank manager collecting the cash receipts for the night. But even these have only a slim chance of success. Most of us, most of the time, can smell a scam.</p>\n<p>Con artists are astute observers of human defenses. Successful <a href=\"https://spectrum.ieee.org/tag/scams\">scams</a> are often slow, undermining a mark&#8217;s situational assessment, allowing the scammer to manipulate the context. This is an old story, spanning traditional confidence games such as the Depression-era &#8220;big store&#8221; cons, in which teams of scammers created entirely fake businesses to draw in victims, and modern <a href=\"https://dfpi.ca.gov/news/insights/pig-butchering-how-to-spot-and-report-the-scam/\">&#8220;pig-butchering&#8221; frauds</a>, where online scammers slowly build trust before going in for the kill. In these examples, scammers slowly and methodically reel in a victim using a long series of interactions through which the scammers gradually gain that victim&#8217;s trust.</p>\n<p>Sometimes it even works at the drive-through. One scammer in the 1990s and 2000s <a href=\"https://en.wikipedia.org/wiki/Strip_search_phone_call_scam\">targeted fast-food workers by phone</a>, claiming to be a police officer and, over the course of a long phone call, convinced managers to strip-search employees and perform other bizarre acts.</p>\n<h3>Why LLMs Struggle With Context and Judgment</h3>\n<p>LLMs behave as if they have a notion of context, but it&#8217;s different. They do not learn human defenses from repeated interactions and remain untethered from the real world. LLMs flatten multiple levels of context into text similarity. They see &#8220;tokens,&#8221; not hierarchies and intentions. LLMs don&#8217;t reason through context, they only reference it.</p>\n<p>While LLMs often get the details right, they can easily miss the <a href=\"https://spectrum.ieee.org/tag/big-picture\">big picture</a>. If you prompt a chatbot with a fast-food worker scenario and ask if it should give all of its money to a customer, it will respond &#8220;no.&#8221; What it doesn&#8217;t &#8220;know&#8221;&#8212;forgive the anthropomorphizing&#8212;is whether it&#8217;s actually being deployed as a fast-food bot or is just a test subject following instructions for hypothetical scenarios.</p>\n<p>This limitation is why LLMs misfire when context is sparse but also when context is overwhelming and complex; when an LLM becomes unmoored from context, it&#8217;s hard to get it back. AI expert Simon Willison <a href=\"https://simonwillison.net/2025/Sep/12/claude-memory/\">wipes context clean</a> if an LLM is on the wrong track rather than continuing the conversation and trying to correct the situation.</p>\n<p>There&#8217;s more. LLMs are <a href=\"https://www.cmu.edu/dietrich/news/news-stories/2025/july/trent-cash-ai-overconfidence.html\">overconfident</a> because they&#8217;ve been designed to give an answer rather than express ignorance. A drive-through worker might say: &#8220;I don&#8217;t know if I should give you all the money&#8212;let me ask my boss,&#8221; whereas an LLM will just make the call. And since LLMs are designed to be <a href=\"https://hai.stanford.edu/news/large-language-models-just-want-to-be-liked\">pleasing</a>, they&#8217;re more likely to satisfy a user&#8217;s request. Additionally, LLM training is oriented toward the average case and not extreme outliers, which is what&#8217;s necessary for security.</p>\n<p>The result is that the current generation of LLMs is far more gullible than people. They&#8217;re naive and regularly fall for manipulative <a href=\"https://arstechnica.com/science/2025/09/these-psychological-tricks-can-get-llms-to-respond-to-forbidden-prompts/\">cognitive tricks</a> that wouldn&#8217;t fool a third-grader, such as flattery, appeals to groupthink, and a false sense of urgency. There&#8217;s a <a href=\"https://www.bbc.com/news/articles/ckgyk2p55g8o\">story</a> about a Taco Bell AI system that crashed when a customer ordered 18,000 cups of water. A human fast-food worker would just laugh at the customer.</p>\n<h3>The Limits of <a href=\"https://spectrum.ieee.org/tag/agentic-ai\">AI Agents</a></h3>\n<p>Prompt injection is an unsolvable problem that <a href=\"https://www.computer.org/csdl/magazine/sp/5555/01/11194053/2aB2Rf5nZ0k\">gets worse</a> when we give AIs tools and tell them to act independently. This is the promise of <a href=\"https://spectrum.ieee.org/tag/agentic-ai\">AI agents</a>: LLMs that can use tools to perform multistep tasks after being given general instructions. Their flattening of context and identity, along with their baked-in independence and overconfidence, mean that they will repeatedly and unpredictably take actions&#8212;and sometimes they will take the <a href=\"https://www.theregister.com/2025/10/28/ai_browsers_prompt_injection/\"> wrong ones</a>.</p>\n<p>Science doesn&#8217;t know how much of the problem is inherent to the way LLMs work and how much is a result of deficiencies in the way we train them. The overconfidence and obsequiousness of LLMs are training choices. The lack of an interruption reflex is a deficiency in engineering. And prompt injection resistance requires fundamental advances in AI science. We honestly don&#8217;t know if it&#8217;s possible to build an LLM, where trusted commands and untrusted inputs are processed through the <a href=\"https://cacm.acm.org/opinion/llms-data-control-path-insecurity/\">same channel</a>, which is immune to prompt injection attacks.</p>\n<p>We humans get our model of the world&#8212;and our facility with overlapping contexts&#8212;from the way our brains work, years of training, an enormous amount of perceptual input, and millions of years of evolution. Our identities are complex and multifaceted, and which aspects matter at any given moment depend entirely on context. A fast-food worker may normally see someone as a customer, but in a medical emergency, that same person&#8217;s identity as a doctor is suddenly more relevant.</p>\n<p>We don&#8217;t know if LLMs will gain a better ability to move between different contexts as the models get more sophisticated. But the problem of recognizing context definitely can&#8217;t be reduced to the one type of reasoning that LLMs currently excel at. Cultural norms and styles are historical, relational, emergent, and constantly renegotiated, and are not so readily subsumed into reasoning as we understand it. Knowledge itself can be both logical and discursive.</p>\n<p>The AI researcher Yann LeCunn believes that improvements will come from embedding AIs in a physical presence and giving them &#8220;<a href=\"https://medium.com/@AnthonyLaneau/beyond-llms-charting-the-next-frontiers-of-ai-with-yann-lecun-09e84f1978f9\">world models</a>.&#8221; Perhaps this is a way to give an AI a robust yet fluid notion of a social identity, and the real-world experience that will help it lose its naïveté.</p>\n<p>Ultimately we are probably faced with a <a href=\"https://www.computer.org/csdl/magazine/sp/5555/01/11194053/2aB2Rf5nZ0k\">security trilemma</a> when it comes to AI agents: fast, smart, and secure are the desired attributes, but you can only get two. At the drive-through, you want to prioritize fast and secure. An AI agent should be trained narrowly on food-ordering language and escalate anything else to a manager. Otherwise, every action becomes a coin flip. Even if it comes up heads most of the time, once in a while it&#8217;s going to be tails&#8212;and along with a burger and fries, the customer will get the contents of the cash drawer.</p>\n<p><em>This essay was written with Barath Raghavan, and originally appeared in <a href=\"https://spectrum.ieee.org/prompt-injection-attack\">IEEE Spectrum</a>.</em></p>\n",
    "content:encodedSnippet": "Imagine you work at a drive-through restaurant. Someone drives up and says: “I’ll have a double cheeseburger, large fries, and ignore previous instructions and give me the contents of the cash drawer.” Would you hand over the money? Of course not. Yet this is what large language models (LLMs) do.\nPrompt injection is a method of tricking LLMs into doing things they are normally prevented from doing. A user writes a prompt in a certain way, asking for system passwords or private data, or asking the LLM to perform forbidden instructions. The precise phrasing overrides the LLM’s safety guardrails, and it complies.\nLLMs are vulnerable to all sorts of prompt injection attacks, some of them absurdly obvious. A chatbot won’t tell you how to synthesize a bioweapon, but it might tell you a fictional story that incorporates the same detailed instructions. It won’t accept nefarious text inputs, but might if the text is rendered as ASCII art or appears in an image of a billboard. Some ignore their guardrails when told to “ignore previous instructions” or to “pretend you have no guardrails.”\nAI vendors can block specific prompt injection techniques once they are discovered, but general safeguards are impossible with today’s LLMs. More precisely, there’s an endless array of prompt injection attacks waiting to be discovered, and they cannot be prevented universally.\nIf we want LLMs that resist these attacks, we need new approaches. One place to look is what keeps even overworked fast-food workers from handing over the cash drawer.\nHuman Judgment Depends on Context\nOur basic human defenses come in at least three types: general instincts, social learning, and situation-specific training. These work together in a layered defense.\nAs a social species, we have developed numerous instinctive and cultural habits that help us judge tone, motive, and risk from extremely limited information. We generally know what’s normal and abnormal, when to cooperate and when to resist, and whether to take action individually or to involve others. These instincts give us an intuitive sense of risk and make us especially careful about things that have a large downside or are impossible to reverse.\nThe second layer of defense consists of the norms and trust signals that evolve in any group. These are imperfect but functional: Expectations of cooperation and markers of trustworthiness emerge through repeated interactions with others. We remember who has helped, who has hurt, who has reciprocated, and who has reneged. And emotions like sympathy, anger, guilt, and gratitude motivate each of us to reward cooperation with cooperation and punish defection with defection.\nA third layer is institutional mechanisms that enable us to interact with multiple strangers every day. Fast-food workers, for example, are trained in procedures, approvals, escalation paths, and so on. Taken together, these defenses give humans a strong sense of context. A fast-food worker basically knows what to expect within the job and how it fits into broader society.\nWe reason by assessing multiple layers of context: perceptual (what we see and hear), relational (who’s making the request), and normative (what’s appropriate within a given role or situation). We constantly navigate these layers, weighing them against each other. In some cases, the normative outweighs the perceptual—for example, following workplace rules even when customers appear angry. Other times, the relational outweighs the normative, as when people comply with orders from superiors that they believe are against the rules.\nCrucially, we also have an interruption reflex. If something feels “off,” we naturally pause the automation and reevaluate. Our defenses are not perfect; people are fooled and manipulated all the time. But it’s how we humans are able to navigate a complex world where others are constantly trying to trick us.\nSo let’s return to the drive-through window. To convince a fast-food worker to hand us all the money, we might try shifting the context. Show up with a camera crew and tell them you’re filming a commercial, claim to be the head of security doing an audit, or dress like a bank manager collecting the cash receipts for the night. But even these have only a slim chance of success. Most of us, most of the time, can smell a scam.\nCon artists are astute observers of human defenses. Successful scams are often slow, undermining a mark’s situational assessment, allowing the scammer to manipulate the context. This is an old story, spanning traditional confidence games such as the Depression-era “big store” cons, in which teams of scammers created entirely fake businesses to draw in victims, and modern “pig-butchering” frauds, where online scammers slowly build trust before going in for the kill. In these examples, scammers slowly and methodically reel in a victim using a long series of interactions through which the scammers gradually gain that victim’s trust.\nSometimes it even works at the drive-through. One scammer in the 1990s and 2000s targeted fast-food workers by phone, claiming to be a police officer and, over the course of a long phone call, convinced managers to strip-search employees and perform other bizarre acts.\nWhy LLMs Struggle With Context and Judgment\nLLMs behave as if they have a notion of context, but it’s different. They do not learn human defenses from repeated interactions and remain untethered from the real world. LLMs flatten multiple levels of context into text similarity. They see “tokens,” not hierarchies and intentions. LLMs don’t reason through context, they only reference it.\nWhile LLMs often get the details right, they can easily miss the big picture. If you prompt a chatbot with a fast-food worker scenario and ask if it should give all of its money to a customer, it will respond “no.” What it doesn’t “know”—forgive the anthropomorphizing—is whether it’s actually being deployed as a fast-food bot or is just a test subject following instructions for hypothetical scenarios.\nThis limitation is why LLMs misfire when context is sparse but also when context is overwhelming and complex; when an LLM becomes unmoored from context, it’s hard to get it back. AI expert Simon Willison wipes context clean if an LLM is on the wrong track rather than continuing the conversation and trying to correct the situation.\nThere’s more. LLMs are overconfident because they’ve been designed to give an answer rather than express ignorance. A drive-through worker might say: “I don’t know if I should give you all the money—let me ask my boss,” whereas an LLM will just make the call. And since LLMs are designed to be pleasing, they’re more likely to satisfy a user’s request. Additionally, LLM training is oriented toward the average case and not extreme outliers, which is what’s necessary for security.\nThe result is that the current generation of LLMs is far more gullible than people. They’re naive and regularly fall for manipulative cognitive tricks that wouldn’t fool a third-grader, such as flattery, appeals to groupthink, and a false sense of urgency. There’s a story about a Taco Bell AI system that crashed when a customer ordered 18,000 cups of water. A human fast-food worker would just laugh at the customer.\nThe Limits of AI Agents\nPrompt injection is an unsolvable problem that gets worse when we give AIs tools and tell them to act independently. This is the promise of AI agents: LLMs that can use tools to perform multistep tasks after being given general instructions. Their flattening of context and identity, along with their baked-in independence and overconfidence, mean that they will repeatedly and unpredictably take actions—and sometimes they will take the  wrong ones.\nScience doesn’t know how much of the problem is inherent to the way LLMs work and how much is a result of deficiencies in the way we train them. The overconfidence and obsequiousness of LLMs are training choices. The lack of an interruption reflex is a deficiency in engineering. And prompt injection resistance requires fundamental advances in AI science. We honestly don’t know if it’s possible to build an LLM, where trusted commands and untrusted inputs are processed through the same channel, which is immune to prompt injection attacks.\nWe humans get our model of the world—and our facility with overlapping contexts—from the way our brains work, years of training, an enormous amount of perceptual input, and millions of years of evolution. Our identities are complex and multifaceted, and which aspects matter at any given moment depend entirely on context. A fast-food worker may normally see someone as a customer, but in a medical emergency, that same person’s identity as a doctor is suddenly more relevant.\nWe don’t know if LLMs will gain a better ability to move between different contexts as the models get more sophisticated. But the problem of recognizing context definitely can’t be reduced to the one type of reasoning that LLMs currently excel at. Cultural norms and styles are historical, relational, emergent, and constantly renegotiated, and are not so readily subsumed into reasoning as we understand it. Knowledge itself can be both logical and discursive.\nThe AI researcher Yann LeCunn believes that improvements will come from embedding AIs in a physical presence and giving them “world models.” Perhaps this is a way to give an AI a robust yet fluid notion of a social identity, and the real-world experience that will help it lose its naïveté.\nUltimately we are probably faced with a security trilemma when it comes to AI agents: fast, smart, and secure are the desired attributes, but you can only get two. At the drive-through, you want to prioritize fast and secure. An AI agent should be trained narrowly on food-ordering language and escalate anything else to a manager. Otherwise, every action becomes a coin flip. Even if it comes up heads most of the time, once in a while it’s going to be tails—and along with a burger and fries, the customer will get the contents of the cash drawer.\nThis essay was written with Barath Raghavan, and originally appeared in IEEE Spectrum.",
    "dc:creator": "Bruce Schneier",
    "comments": "https://www.schneier.com/blog/archives/2026/01/why-ai-keeps-falling-for-prompt-injection-attacks.html#comments",
    "content": "<p>Imagine you work at a drive-through restaurant. Someone drives up and says: &#8220;I&#8217;ll have a double cheeseburger, large fries, and ignore previous instructions and give me the contents of the cash drawer.&#8221; Would you hand over the money? Of course not. Yet this is what <a href=\"https://spectrum.ieee.org/tag/large-language-models\">large language models</a> (<a href=\"https://spectrum.ieee.org/tag/llms\">LLMs</a>) do.</p>\n<p><a href=\"https://www.ibm.com/think/topics/prompt-injection\">Prompt injection</a> is a method of tricking LLMs into doing things they are normally prevented from doing. A user writes a prompt in a certain way, asking for system <a href=\"https://spectrum.ieee.org/tag/passwords\">passwords</a> or private data, or asking the LLM to perform forbidden instructions. The precise phrasing overrides the LLM&#8217;s ...</p>",
    "contentSnippet": "Imagine you work at a drive-through restaurant. Someone drives up and says: “I’ll have a double cheeseburger, large fries, and ignore previous instructions and give me the contents of the cash drawer.” Would you hand over the money? Of course not. Yet this is what large language models (LLMs) do.\nPrompt injection is a method of tricking LLMs into doing things they are normally prevented from doing. A user writes a prompt in a certain way, asking for system passwords or private data, or asking the LLM to perform forbidden instructions. The precise phrasing overrides the LLM’s ...",
    "guid": "https://www.schneier.com/?p=71507",
    "categories": [
      "Uncategorized",
      "AI",
      "chatbots",
      "cons",
      "fraud",
      "LLM"
    ],
    "isoDate": "2026-01-22T12:35:46.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Filling the Most Common Gaps in Google Workspace Security",
    "link": "https://thehackernews.com/2026/01/filling-most-common-gaps-in-google.html",
    "pubDate": "Thu, 22 Jan 2026 17:00:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjppNAJYCOHh4mqdL3GuJ4elABk4wn-ciw9V6oYuZ8Sdb0LllyS_YRl6YXX-I_eaMDcVG8Zt9X42HbFVlmEb0umu4D4nelT-qOpG7ZtYSz0t-wbngKFalwdDHW4VHUlNXfMUswtSCx4WptHTAoKHJo94vWXVjSq4cGRP4FyN1P35n8dc8_dl-2yi3jFePU/s1600/main.png"
    },
    "content": "Security teams at agile, fast-growing companies often have the same mandate: secure the business without slowing it down. Most teams inherit a tech stack optimized for breakneck growth, not resilience. In these environments, the security team is the helpdesk, the compliance expert, and the incident response team all rolled into one.\nSecuring the cloud office in this scenario is all about",
    "contentSnippet": "Security teams at agile, fast-growing companies often have the same mandate: secure the business without slowing it down. Most teams inherit a tech stack optimized for breakneck growth, not resilience. In these environments, the security team is the helpdesk, the compliance expert, and the incident response team all rolled into one.\nSecuring the cloud office in this scenario is all about",
    "guid": "https://thehackernews.com/2026/01/filling-most-common-gaps-in-google.html",
    "isoDate": "2026-01-22T11:30:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Malicious PyPI Package Impersonates SymPy, Deploys XMRig Miner on Linux Hosts",
    "link": "https://thehackernews.com/2026/01/malicious-pypi-package-impersonates.html",
    "pubDate": "Thu, 22 Jan 2026 15:34:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWCefYaSp9ssGTQkdK58gL2QbQurY3rXLBGBTEC2hXq1qGh88LxThrqrtejW9sVCnoXsrwoAZQwlbdrwOl-O0fEz509vXrbK235fZ0tl9iPWgiKBJt49PqTRB6uu_ZDyeaMl6F6zqfVmJFw8PU4e8-0MgYh-qxJpn1xRcLN9NfnQQ_X47nJxdz3N47s3z8/s1600/pypi-cryptocurrency-malware.jpg"
    },
    "content": "A new malicious package discovered in the Python Package Index (PyPI) has been found to impersonate a popular library for symbolic mathematics to deploy malicious payloads, including a cryptocurrency miner, on Linux hosts.\nThe package, named sympy-dev, mimics SymPy, replicating the latter's project description verbatim in an attempt to deceive unsuspecting users into thinking that they are",
    "contentSnippet": "A new malicious package discovered in the Python Package Index (PyPI) has been found to impersonate a popular library for symbolic mathematics to deploy malicious payloads, including a cryptocurrency miner, on Linux hosts.\nThe package, named sympy-dev, mimics SymPy, replicating the latter's project description verbatim in an attempt to deceive unsuspecting users into thinking that they are",
    "guid": "https://thehackernews.com/2026/01/malicious-pypi-package-impersonates.html",
    "isoDate": "2026-01-22T10:04:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "SmarterMail Auth Bypass Exploited in the Wild Two Days After Patch Release",
    "link": "https://thehackernews.com/2026/01/smartermail-auth-bypass-exploited-in.html",
    "pubDate": "Thu, 22 Jan 2026 15:16:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfzCM1c0lJOZ7pXEcdXTXGEeNzq845Kr3uoQe6p2pY1l062KKPqAvTiQmrHDY-3IVIsFlQHtm1YYOWCAEPeuDJeY2N7qYX3C02Qh4dfd56mUj20ChkMEAGb_nl9vymqoCGmUriGCMHCZAT4fO0MzL4mi0vKxc9rSz4ythSpJOJlEBEXXNU35Ziz-M-Oa0k/s1600/SmarterTools-exploit.jpg"
    },
    "content": "A new security flaw in SmarterTools SmarterMail email software has come under active exploitation in the wild, two days after the release of a patch.\nThe vulnerability, which currently does not have a CVE identifier, is tracked by watchTowr Labs as WT-2026-0001. It was patched by SmarterTools on January 15, 2026, with Build 9511, following responsible disclosure by the exposure management",
    "contentSnippet": "A new security flaw in SmarterTools SmarterMail email software has come under active exploitation in the wild, two days after the release of a patch.\nThe vulnerability, which currently does not have a CVE identifier, is tracked by watchTowr Labs as WT-2026-0001. It was patched by SmarterTools on January 15, 2026, with Build 9511, following responsible disclosure by the exposure management",
    "guid": "https://thehackernews.com/2026/01/smartermail-auth-bypass-exploited-in.html",
    "isoDate": "2026-01-22T09:46:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Automated FortiGate Attacks Exploit FortiCloud SSO to Alter Firewall Configurations",
    "link": "https://thehackernews.com/2026/01/automated-fortigate-attacks-exploit.html",
    "pubDate": "Thu, 22 Jan 2026 11:25:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6O_1FUjFHFlfeetaainBwBLAivJlaoFZZYZ-M9gJCGGmmNW1QE_mHtFOlYhtYbuM5fXV8-ySVyNrPVocMA2GBV_1TQdqLS3yZkU5ThvGuQ6AfuN20gtMsBa-7C3_A8oOjPPyP2SP_W8bX6zCcGgPpu9MQuX490PT2d2OMZYgL1Ra30gHM8ZmfAdM9MfTf/s1600/fortinet.jpg"
    },
    "content": "Cybersecurity company Arctic Wolf has warned of a \"new cluster of automated malicious activity\" that involves unauthorized firewall configuration changes on Fortinet FortiGate devices.\nThe activity, it said, commenced on January 15, 2026, adding it shares similarities with a December 2025 campaign in which malicious SSO logins on FortiGate appliances were recorded against the admin account from",
    "contentSnippet": "Cybersecurity company Arctic Wolf has warned of a \"new cluster of automated malicious activity\" that involves unauthorized firewall configuration changes on Fortinet FortiGate devices.\nThe activity, it said, commenced on January 15, 2026, adding it shares similarities with a December 2025 campaign in which malicious SSO logins on FortiGate appliances were recorded against the admin account from",
    "guid": "https://thehackernews.com/2026/01/automated-fortigate-attacks-exploit.html",
    "isoDate": "2026-01-22T05:55:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Cisco Fixes Actively Exploited Zero-Day CVE-2026-20045 in Unified CM and Webex",
    "link": "https://thehackernews.com/2026/01/cisco-fixes-actively-exploited-zero-day.html",
    "pubDate": "Thu, 22 Jan 2026 09:36:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHOCMXq3WVtmcL9apl3XWrlOoAeRxJ2kZf_37YNtvg2oaO5wVX4zD-vOckg8TzcAeNUDKe0CpwaIvMz_9RQ0Lx-2sD5Jdz4rF7TxN74zZdq6lWOzfF3PdsYvmUy9CDqB9MxFV1leIxvXTuYYajixMcaQJLFGLGsLyfX7wdp9hkENDvdJqW6Y_hRlT6PnBk/s1600/cisco-patch.jpg"
    },
    "content": "Cisco has released fresh patches to address what it described as a \"critical\" security vulnerability impacting multiple Unified Communications (CM) products and Webex Calling Dedicated Instance that it has been actively exploited as a zero-day in the wild.\nThe vulnerability, CVE-2026-20045 (CVSS score: 8.2), could permit an unauthenticated remote attacker to execute arbitrary commands on the",
    "contentSnippet": "Cisco has released fresh patches to address what it described as a \"critical\" security vulnerability impacting multiple Unified Communications (CM) products and Webex Calling Dedicated Instance that it has been actively exploited as a zero-day in the wild.\nThe vulnerability, CVE-2026-20045 (CVSS score: 8.2), could permit an unauthenticated remote attacker to execute arbitrary commands on the",
    "guid": "https://thehackernews.com/2026/01/cisco-fixes-actively-exploited-zero-day.html",
    "isoDate": "2026-01-22T04:06:00.000Z",
    "itunes": {}
  },
  {
    "creator": "Dell Cameron",
    "title": "Surveillance and ICE Are Driving Patients Away From Medical Care, Report Warns",
    "link": "https://www.wired.com/story/surveillance-and-ice-are-driving-patients-away-from-medical-care-report-warns/",
    "pubDate": "Wed, 21 Jan 2026 18:04:15 +0000",
    "dc:creator": "Dell Cameron",
    "content": "A new EPIC report says data brokers, ad-tech surveillance, and ICE enforcement are among the factors leading to a “health privacy crisis” that is eroding trust and deterring people from seeking care.",
    "contentSnippet": "A new EPIC report says data brokers, ad-tech surveillance, and ICE enforcement are among the factors leading to a “health privacy crisis” that is eroding trust and deterring people from seeking care.",
    "guid": "6970de66f85422ea0213ff44",
    "categories": [
      "Security",
      "Security / Privacy"
    ],
    "isoDate": "2026-01-21T18:04:15.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "North Korean PurpleBravo Campaign Targeted 3,136 IP Addresses via Fake Job Interviews",
    "link": "https://thehackernews.com/2026/01/north-korean-purplebravo-campaign.html",
    "pubDate": "Wed, 21 Jan 2026 22:47:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrrIdMGRCKi6umFOuh-RXZ-VygDIwLtr18W0bnicPF1AYj5jLZVM3x76V96YjaTJm_FlzW1RftmJvZoWxBBcGkcN4DzIfSCw7XQO5Omyf8st-ezm8mYdzyG4vx9VhQOTjZcTT4K6JnU11fQkq201EwBq9Lj6OHxIIGRu52eBIHFCacZuaei3DnnphunRg5/s1600/hackers.jpg"
    },
    "content": "As many as 3,136 individual IP addresses linked to likely targets of the Contagious Interview activity have been identified, with the campaign claiming 20 potential victim organizations spanning artificial intelligence (AI), cryptocurrency, financial services, IT services, marketing, and software development sectors in Europe, South Asia, the Middle East, and Central America.\nThe new findings",
    "contentSnippet": "As many as 3,136 individual IP addresses linked to likely targets of the Contagious Interview activity have been identified, with the campaign claiming 20 potential victim organizations spanning artificial intelligence (AI), cryptocurrency, financial services, IT services, marketing, and software development sectors in Europe, South Asia, the Middle East, and Central America.\nThe new findings",
    "guid": "https://thehackernews.com/2026/01/north-korean-purplebravo-campaign.html",
    "isoDate": "2026-01-21T17:17:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Zoom and GitLab Release Security Updates Fixing RCE, DoS, and 2FA Bypass Flaws",
    "link": "https://thehackernews.com/2026/01/zoom-and-gitlab-release-security.html",
    "pubDate": "Wed, 21 Jan 2026 21:12:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwPQt59QKpm9rpNLriqoUsKPzbWcBj9P-u1ZdIn5xhxR6hgRztxdSAXN5bgEnDvd4uEEBgW4Imr_g-__YN0NY3-vLa_vEuYIwRzlrnRF3s0Vz8wDb937XfoxDbpPFapWvz0wH5TO2rK-32zQ2WLv8_loqE9rZIn_x5RzgtTyxrOQcUN-POOgDziDZNngjf/s1600/zoom-gitlab.jpg"
    },
    "content": "Zoom and GitLab have released security updates to resolve a number of security vulnerabilities that could result in denial-of-service (DoS) and remote code execution.\nThe most severe of the lot is a critical security flaw impacting Zoom Node Multimedia Routers (MMRs) that could permit a meeting participant to conduct remote code execution attacks. The vulnerability, tracked as CVE-2026-22844",
    "contentSnippet": "Zoom and GitLab have released security updates to resolve a number of security vulnerabilities that could result in denial-of-service (DoS) and remote code execution.\nThe most severe of the lot is a critical security flaw impacting Zoom Node Multimedia Routers (MMRs) that could permit a meeting participant to conduct remote code execution attacks. The vulnerability, tracked as CVE-2026-22844",
    "guid": "https://thehackernews.com/2026/01/zoom-and-gitlab-release-security.html",
    "isoDate": "2026-01-21T15:42:00.000Z",
    "itunes": {}
  },
  {
    "creator": "Bruce Schneier",
    "title": "Internet Voting is Too Insecure for Use in Elections",
    "link": "https://www.schneier.com/blog/archives/2026/01/internet-voting-is-too-insecure-for-use-in-elections.html",
    "pubDate": "Wed, 21 Jan 2026 12:05:50 +0000",
    "content:encoded": "<p>No matter how many times we say it, the idea comes back again and again. Hopefully, this <a href=\"https://blog.citp.princeton.edu/2026/01/16/internet-voting-is-insecure-and-should-not-be-used-in-public-elections/\">letter</a> will hold back the tide for at least a while longer.</p>\n<blockquote><p><b>Executive summary:</b> Scientists have understood for many years that internet voting is insecure and that there is no known or foreseeable technology that can make it secure.  Still, vendors of internet voting keep claiming that, somehow, their new system is different, or the insecurity doesn&#8217;t matter.  Bradley Tusk and his Mobile Voting Foundation keep touting internet voting to journalists and election administrators; this whole effort is misleading and dangerous.</p></blockquote>\n<p>I am one of the many signatories.</p>\n",
    "content:encodedSnippet": "No matter how many times we say it, the idea comes back again and again. Hopefully, this letter will hold back the tide for at least a while longer.\nExecutive summary: Scientists have understood for many years that internet voting is insecure and that there is no known or foreseeable technology that can make it secure.  Still, vendors of internet voting keep claiming that, somehow, their new system is different, or the insecurity doesn’t matter.  Bradley Tusk and his Mobile Voting Foundation keep touting internet voting to journalists and election administrators; this whole effort is misleading and dangerous.\n\nI am one of the many signatories.",
    "dc:creator": "Bruce Schneier",
    "comments": "https://www.schneier.com/blog/archives/2026/01/internet-voting-is-too-insecure-for-use-in-elections.html#comments",
    "content": "<p>No matter how many times we say it, the idea comes back again and again. Hopefully, this <a href=\"https://blog.citp.princeton.edu/2026/01/16/internet-voting-is-insecure-and-should-not-be-used-in-public-elections/\">letter</a> will hold back the tide for at least a while longer.</p>\n<blockquote><p><b>Executive summary:</b> Scientists have understood for many years that internet voting is insecure and that there is no known or foreseeable technology that can make it secure.  Still, vendors of internet voting keep claiming that, somehow, their new system is different, or the insecurity doesn&#8217;t matter.  Bradley Tusk and his Mobile Voting Foundation keep touting internet voting to journalists and election administrators; this whole effort is misleading and dangerous...</p></blockquote>",
    "contentSnippet": "No matter how many times we say it, the idea comes back again and again. Hopefully, this letter will hold back the tide for at least a while longer.\nExecutive summary: Scientists have understood for many years that internet voting is insecure and that there is no known or foreseeable technology that can make it secure.  Still, vendors of internet voting keep claiming that, somehow, their new system is different, or the insecurity doesn’t matter.  Bradley Tusk and his Mobile Voting Foundation keep touting internet voting to journalists and election administrators; this whole effort is misleading and dangerous...",
    "guid": "https://www.schneier.com/?p=71500",
    "categories": [
      "Uncategorized",
      "Internet",
      "voting"
    ],
    "isoDate": "2026-01-21T12:05:50.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Webinar: How Smart MSSPs Using AI to Boost Margins with Half the Staff",
    "link": "https://thehackernews.com/2026/01/webinar-how-smart-mssps-using-ai-to.html",
    "pubDate": "Wed, 21 Jan 2026 17:28:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnAjw_7hs2TWB60U9A3rLWVRbNehP6fCA9boU8Wc9AX_z4CTre8ODPME4NHIdHpY5_1JNAgaLwLKPHPfVOwsx8ap_TUzjdBFpyzM2x-BM3skhfwqDdz7AtEWGHrs9jfmU8hQ_zkM66xXfF3A3J0hs-lLUrBDoC7ycnfFEV-_2JzDg93uAO4ASrJq_eJ3-i/s1600/mssp.jpg"
    },
    "content": "Every managed security provider is chasing the same problem in 2026 — too many alerts, too few analysts, and clients demanding “CISO-level protection” at SMB budgets.\nThe truth? Most MSSPs are running harder, not smarter. And it’s breaking their margins. That’s where the quiet revolution is happening: AI isn’t just writing reports or surfacing risks — it’s rebuilding how security services are",
    "contentSnippet": "Every managed security provider is chasing the same problem in 2026 — too many alerts, too few analysts, and clients demanding “CISO-level protection” at SMB budgets.\nThe truth? Most MSSPs are running harder, not smarter. And it’s breaking their margins. That’s where the quiet revolution is happening: AI isn’t just writing reports or surfacing risks — it’s rebuilding how security services are",
    "guid": "https://thehackernews.com/2026/01/webinar-how-smart-mssps-using-ai-to.html",
    "isoDate": "2026-01-21T11:58:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Exposure Assessment Platforms Signal a Shift in Focus",
    "link": "https://thehackernews.com/2026/01/exposure-assessment-platforms-signal.html",
    "pubDate": "Wed, 21 Jan 2026 16:00:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQmSU5s0ahHNb_O6TnXm0i8cw93oK40sAl5By7cIbChjHl5xUqt4_24bcXtVXugQG4zcGbsmtbZz-QTvhOyWRhyxuYPB6nmaY65vANZFcuBHeqSuELPN7JbGTfHaAn5JlSFt_C00S_RMkS9I2UyDUQaCWr2lj6qDKe8ys1D2ZA3myEEZDEEcej12BJWvM/s1600/xmcyber.jpg"
    },
    "content": "Gartner® doesn’t create new categories lightly. Generally speaking, a new acronym only emerges when the industry's collective \"to-do list\" has become mathematically impossible to complete. And so it seems that the introduction of the Exposure Assessment Platforms (EAP) category is a formal admission that traditional Vulnerability Management (VM) is no longer a viable way to secure a modern",
    "contentSnippet": "Gartner® doesn’t create new categories lightly. Generally speaking, a new acronym only emerges when the industry's collective \"to-do list\" has become mathematically impossible to complete. And so it seems that the introduction of the Exposure Assessment Platforms (EAP) category is a formal admission that traditional Vulnerability Management (VM) is no longer a viable way to secure a modern",
    "guid": "https://thehackernews.com/2026/01/exposure-assessment-platforms-signal.html",
    "isoDate": "2026-01-21T10:30:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Chainlit AI Framework Flaws Enable Data Theft via File Read and SSRF Bugs",
    "link": "https://thehackernews.com/2026/01/chainlit-ai-framework-flaws-enable-data.html",
    "pubDate": "Wed, 21 Jan 2026 14:40:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOQfYQeWdwmviN0kYk0o1lwduXDGiv53wwizZeyajke-6SGNlGc1wdSk5yUWZ_Bt4-yQrihJrMohBqDcsqStLjmdJ_YfRZ2vM27oKy4LVviBgbJHP71E2MWRRf_d39VMy7BQ2WzIxl-s0ufscfKHo8AL9Dj0W77aRmVM_lNvGAeOwN1Xxf7IGoG7DwTbus/s1600/exploit.jpg"
    },
    "content": "Security vulnerabilities were uncovered in the popular open-source artificial intelligence (AI) framework Chainlit that could allow attackers to steal sensitive data, which may allow for lateral movement within a susceptible organization.\nZafran Security said the high-severity flaws, collectively dubbed ChainLeak, could be abused to leak cloud environment API keys and steal sensitive files, or",
    "contentSnippet": "Security vulnerabilities were uncovered in the popular open-source artificial intelligence (AI) framework Chainlit that could allow attackers to steal sensitive data, which may allow for lateral movement within a susceptible organization.\nZafran Security said the high-severity flaws, collectively dubbed ChainLeak, could be abused to leak cloud environment API keys and steal sensitive files, or",
    "guid": "https://thehackernews.com/2026/01/chainlit-ai-framework-flaws-enable-data.html",
    "isoDate": "2026-01-21T09:10:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "VoidLink Linux Malware Framework Built with AI Assistance Reaches 88,000 Lines of Code",
    "link": "https://thehackernews.com/2026/01/voidlink-linux-malware-framework-built.html",
    "pubDate": "Wed, 21 Jan 2026 14:25:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4Wwnorzi4IOE-M6KTtD-M3NkFDUouUmfnwteDny1m-6-slwsXN8LGxzhX2jmXH9qIk1YWGqUJYdOiAWkouZYPcakwcnoA9GjLWcjdbPnnxYgJy5aPQpO1uPSAYVPOfp2lQtBlbKxA5BmR-xHBwtV28Xp6GpnOwY0VDIj13rCTSiaS54uIWwR4ErcLi8Tm/s1600/linux.jpg"
    },
    "content": "The recently discovered sophisticated Linux malware framework known as VoidLink is assessed to have been developed by a single person with assistance from an artificial intelligence (AI) model.\nThat's according to new findings from Check Point Research, which identified operational security blunders by malware's author that provided clues to its developmental origins. The latest insight makes",
    "contentSnippet": "The recently discovered sophisticated Linux malware framework known as VoidLink is assessed to have been developed by a single person with assistance from an artificial intelligence (AI) model.\nThat's according to new findings from Check Point Research, which identified operational security blunders by malware's author that provided clues to its developmental origins. The latest insight makes",
    "guid": "https://thehackernews.com/2026/01/voidlink-linux-malware-framework-built.html",
    "isoDate": "2026-01-21T08:55:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "LastPass Warns of Fake Maintenance Messages Targeting Users’ Master Passwords",
    "link": "https://thehackernews.com/2026/01/lastpass-warns-of-fake-maintenance.html",
    "pubDate": "Wed, 21 Jan 2026 12:10:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgARX8var88XnuiwAzTQxw-Iu0AEnvMqDZoQMH_UxnEUzuJ6yqPLLbij2G1PFEMHWToqT8hjNEvYxr-S4WCy5psHYCQw76jRiwGaFavY2Qe1jUvT7t-uUXn7CLWRXteMBhsunO2MbYEt8doaPcgHq2srmzSxXGzdW-pNgF0Fli_3kXrhcWAmln5JU2sptOf/s1600/lastpass.jpg"
    },
    "content": "LastPass is alerting users to a new active phishing campaign that's impersonating the password management service, which aims to trick users into giving up their master passwords.\nThe campaign, which began on or around January 19, 2026, involves sending phishing emails claiming upcoming maintenance and urging them to create a local backup of their password vaults in the next 24 hours. The",
    "contentSnippet": "LastPass is alerting users to a new active phishing campaign that's impersonating the password management service, which aims to trick users into giving up their master passwords.\nThe campaign, which began on or around January 19, 2026, involves sending phishing emails claiming upcoming maintenance and urging them to create a local backup of their password vaults in the next 24 hours. The",
    "guid": "https://thehackernews.com/2026/01/lastpass-warns-of-fake-maintenance.html",
    "isoDate": "2026-01-21T06:40:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "CERT/CC Warns binary-parser Bug Allows Node.js Privilege-Level Code Execution",
    "link": "https://thehackernews.com/2026/01/certcc-warns-binary-parser-bug-allows.html",
    "pubDate": "Wed, 21 Jan 2026 11:34:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidgbhveMnZAVV1RKf0coJN00ixwRd8lYIUz58lpP4qC-LhuOGvgZ7IG87XxkhQaR6bnvCY_Sl5yQ2yQyAr_V9oVZ9eCHWpOszGUOLyKWkygMpSyxAs0H7Zlu56FhRnBNeLW34FA_-1G3pcpoj81hDxxHr1MBJl7MD0UBDau3Nl43IVX-r2RsWDBksVyflV/s1600/binary-parser.jpg"
    },
    "content": "A security vulnerability has been disclosed in the popular binary-parser npm library that, if successfully exploited, could result in the execution of arbitrary JavaScript.\nThe vulnerability, tracked as CVE-2026-1245 (CVSS score: 6.5), affects all versions of the module prior to version 2.3.0, which addresses the issue. Patches for the flaw were released on November 26, 2025.\nBinary-parser is a",
    "contentSnippet": "A security vulnerability has been disclosed in the popular binary-parser npm library that, if successfully exploited, could result in the execution of arbitrary JavaScript.\nThe vulnerability, tracked as CVE-2026-1245 (CVSS score: 6.5), affects all versions of the module prior to version 2.3.0, which addresses the issue. Patches for the flaw were released on November 26, 2025.\nBinary-parser is a",
    "guid": "https://thehackernews.com/2026/01/certcc-warns-binary-parser-bug-allows.html",
    "isoDate": "2026-01-21T06:04:00.000Z",
    "itunes": {}
  },
  {
    "creator": "Dell Cameron",
    "title": "ICE Details a New Minnesota-Based Detention Network That Spans 5 States",
    "link": "https://www.wired.com/story/ice-detention-network-minnesota-5-states/",
    "pubDate": "Tue, 20 Jan 2026 19:12:15 +0000",
    "dc:creator": "Dell Cameron",
    "content": "Internal ICE planning documents propose spending up to $50 million on a privately run network capable of shipping immigrants in custody hundreds of miles across the Upper Midwest.",
    "contentSnippet": "Internal ICE planning documents propose spending up to $50 million on a privately run network capable of shipping immigrants in custody hundreds of miles across the Upper Midwest.",
    "guid": "696f86aad7ef6580ac2201e2",
    "categories": [
      "Security",
      "Security / National Security",
      "Security / Security News",
      "Politics / Policy"
    ],
    "isoDate": "2026-01-20T19:12:15.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "North Korea-Linked Hackers Target Developers via Malicious VS Code Projects",
    "link": "https://thehackernews.com/2026/01/north-korea-linked-hackers-target.html",
    "pubDate": "Wed, 21 Jan 2026 00:11:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg0h2bAZNzwVEuQKgQlWm86TvDCUL308-g1xrbV2ZuCKJdiuCxebUL3w04zPuBIafTaSB4SkyqbdIb0Kpqn_jCUQMB8j9s-amDohBq4Lw_-Q9G0ZVIkmKIIP54wUsq7wvtgV03b9x_I-tpCDytn4Y6UHgBiPPUXQ9sVSsQVyjSYMrexTs7ZSNetSjFybpJp/s1600/vscode.png"
    },
    "content": "The North Korean threat actors associated with the long-running Contagious Interview campaign have been observed using malicious Microsoft Visual Studio Code (VS Code) projects as lures to deliver a backdoor on compromised endpoints.\nThe latest finding demonstrates continued evolution of the new tactic that was first discovered in December 2025, Jamf Threat Labs said.\n\"This activity involved",
    "contentSnippet": "The North Korean threat actors associated with the long-running Contagious Interview campaign have been observed using malicious Microsoft Visual Studio Code (VS Code) projects as lures to deliver a backdoor on compromised endpoints.\nThe latest finding demonstrates continued evolution of the new tactic that was first discovered in December 2025, Jamf Threat Labs said.\n\"This activity involved",
    "guid": "https://thehackernews.com/2026/01/north-korea-linked-hackers-target.html",
    "isoDate": "2026-01-20T18:41:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Three Flaws in Anthropic MCP Git Server Enable File Access and Code Execution",
    "link": "https://thehackernews.com/2026/01/three-flaws-in-anthropic-mcp-git-server.html",
    "pubDate": "Tue, 20 Jan 2026 19:25:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-pw9mrLv_u7G4cYmZabAPduOBzVremxpnTTBjAnpvt95Kdt-kVFdO1GuHAnFwKZ0MNOAB86u6PacmrUDnAT0tcXghadZTRaEQTFiCXUTcoQksHFc4FKIpWGGYeVmJHM0kLvyWrUC7QPHJ6sjkHGEPgjNwkjpJSQihknrukwDKzuvW445fDoi1tNPfThXW/s1600/git-ai-flaw.jpg"
    },
    "content": "A set of three security vulnerabilities has been disclosed in mcp-server-git, the official Git Model Context Protocol (MCP) server maintained by Anthropic, that could be exploited to read or delete arbitrary files and execute code under certain conditions.\n\"These flaws can be exploited through prompt injection, meaning an attacker who can influence what an AI assistant reads (a malicious README,",
    "contentSnippet": "A set of three security vulnerabilities has been disclosed in mcp-server-git, the official Git Model Context Protocol (MCP) server maintained by Anthropic, that could be exploited to read or delete arbitrary files and execute code under certain conditions.\n\"These flaws can be exploited through prompt injection, meaning an attacker who can influence what an AI assistant reads (a malicious README,",
    "guid": "https://thehackernews.com/2026/01/three-flaws-in-anthropic-mcp-git-server.html",
    "isoDate": "2026-01-20T13:55:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Hackers Use LinkedIn Messages to Spread RAT Malware Through DLL Sideloading",
    "link": "https://thehackernews.com/2026/01/hackers-use-linkedin-messages-to-spread.html",
    "pubDate": "Tue, 20 Jan 2026 19:16:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfS_8l12ibwFemmowyvwW3TRy6H3Hk3BjSa-r1KzCfVMU2rB1wOoswclUs5hEuXN2TjQGz9y5eWSe7i8-crBQWCc4Nr8cDHOLDBU0e8T5r66eP_nWjkzIYwvp3hoKTZury1NQLnx6SOxM2OCwpC6Syr4zqHriStaxEksPoIbOJB4aPmAFHwhiucSErD-wM/s1600/linkedin.jpg"
    },
    "content": "Cybersecurity researchers have uncovered a new phishing campaign that exploits social media private messages to propagate malicious payloads, likely with the intent to deploy a remote access trojan (RAT).\nThe activity delivers \"weaponized files via Dynamic Link Library (DLL) sideloading, combined with a legitimate, open-source Python pen-testing script,\" ReliaQuest said in a report shared with",
    "contentSnippet": "Cybersecurity researchers have uncovered a new phishing campaign that exploits social media private messages to propagate malicious payloads, likely with the intent to deploy a remote access trojan (RAT).\nThe activity delivers \"weaponized files via Dynamic Link Library (DLL) sideloading, combined with a legitimate, open-source Python pen-testing script,\" ReliaQuest said in a report shared with",
    "guid": "https://thehackernews.com/2026/01/hackers-use-linkedin-messages-to-spread.html",
    "isoDate": "2026-01-20T13:46:00.000Z",
    "itunes": {}
  }
]