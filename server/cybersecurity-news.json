[
  {
    "creator": "Bruce Schneier",
    "title": "Could ChatGPT Convince You to Buy Something?",
    "link": "https://www.schneier.com/blog/archives/2026/01/could-chatgpt-convince-you-to-buy-something.html",
    "pubDate": "Tue, 20 Jan 2026 12:08:09 +0000",
    "content:encoded": "<p>Eighteen months ago, it was plausible that artificial intelligence might take a <a href=\"https://www.technologyreview.com/2024/03/13/1089729/lets-not-make-the-same-mistakes-with-ai-that-we-made-with-social-media/\">different path</a> than social media. Back then, AI&#8217;s development hadn&#8217;t consolidated under a small number of big tech firms. Nor had it capitalized on consumer attention, surveilling users and delivering ads.</p>\n<p>Unfortunately, the AI industry is now taking a page from the social media playbook and has set its sights on monetizing consumer attention. When OpenAI launched its <a href=\"https://openai.com/index/introducing-chatgpt-search/\">ChatGPT Search</a> feature in late 2024 and its browser, <a href=\"https://openai.com/index/introducing-chatgpt-atlas/\">ChatGPT Atlas</a>, in October 2025, it kicked off a <a href=\"https://www.adweek.com/media/openai-takes-on-google-with-atlas-ai-browser/\">race to capture online behavioral data</a> to power advertising. It&#8217;s part of a yearslong <a href=\"https://digiday.com/marketing/from-hatred-to-hiring-openais-advertising-change-of-heart/\">turnabout by OpenAI</a>, whose CEO Sam Altman once called the combination of ads and AI &#8220;unsettling&#8221; and now promises that <a href=\"https://searchengineland.com/chatgpt-ads-coming-some-point-464388\">ads can be deployed in AI apps</a> while preserving trust. The rampant <a href=\"https://www.engadget.com/ai/openais-head-of-chatgpt-says-posts-appearing-to-show-in-app-ads-are-not-real-or-not-ads-190454584.html\">speculation among OpenAI users</a> who believe they see paid placements in ChatGPT responses suggests they are not convinced.</p>\n<p>In 2024, AI search company Perplexity started <a href=\"https://www.perplexity.ai/hub/blog/why-we-re-experimenting-with-advertising\">experimenting with ads</a> in its offerings. A few months after that, Microsoft <a href=\"https://www.windowscentral.com/software-apps/microsoft-integrates-showroom-ads-in-copilot-ai-simulating-brick-and-mortar-stores\">introduced ads to its Copilot</a> AI. Google&#8217;s <a href=\"https://searchengineland.com/google-ads-inside-ai-mode-tests-expand-464979\">AI Mode for search</a> now increasingly features ads, <a href=\"https://adage.com/technology/amazon/aa-ai-ads-sponsored-prompts/\">as does Amazon&#8217;s Rufus chatbot</a>. OpenAI announced on Jan. 16, 2026, that it will soon begin <a href=\"https://openai.com/index/our-approach-to-advertising-and-expanding-access/\">testing ads in the unpaid version of ChatGPT</a>.</p>\n<p>As a <a href=\"https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C22&amp;q=Bruce+Schneier&amp;btnG=\">security expert</a> and <a href=\"https://scholar.google.com/citations?hl=en&amp;user=LlKKQyIAAAAJ&amp;view_op=list_works&amp;sortby=pubdate\">data scientist</a>, we see these examples as harbingers of a future where AI companies profit from manipulating their users&#8217; behavior for the benefit of their advertisers and investors. It&#8217;s also a reminder that time to steer the direction of AI development away from private exploitation and toward public benefit is quickly running out.</p>\n<p>The functionality of ChatGPT Search and its Atlas browser is not really new. <a href=\"https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html\">Meta</a>, commercial AI competitor <a href=\"https://www.nytimes.com/2024/02/01/technology/perplexity-search-ai-google.html\">Perplexity</a> and even <a href=\"https://www.theverge.com/2023/9/27/23892781/openai-chatgpt-live-web-results-browse-with-bing\">ChatGPT</a> itself have had similar AI search features for years, and both <a href=\"https://gemini.google/overview/gemini-in-chrome/\">Google</a> and <a href=\"https://blogs.windows.com/msedgedev/2023/05/23/microsoft-edge-build-2023-innovations-in-ai-productivity-management-sidebar-apps/\">Microsoft</a> beat OpenAI to the punch by integrating AI with their browsers. But OpenAI&#8217;s <a href=\"https://www.washingtonpost.com/technology/2024/10/31/openai-chatgpt-search-ai-upgrade-google/\">business positioning</a> signals a shift.</p>\n<p>We believe the ChatGPT Search and Atlas announcements are worrisome because there is really only one way to make money on search: the advertising model <a href=\"https://law.stanford.edu/publications/why-google-dominates-advertising-markets/\">pioneered ruthlessly by Google</a>.</p>\n<h3>Advertising model</h3>\n<p>Ruled <a href=\"https://www.nytimes.com/2024/08/05/technology/google-antitrust-ruling.html\">a monopolist</a> in U.S. federal court, Google has earned more than <a href=\"https://www.statista.com/statistics/266249/advertising-revenue-of-google/\">US$1.6 trillion in advertising revenue</a> since 2001. You may think of Google as a web search company, or a streaming video company (YouTube), or an email company (Gmail), or a mobile phone company (Android, Pixel), or maybe even an AI company (Gemini). But those products are ancillary to Google&#8217;s bottom line. The advertising segment typically accounts for <a href=\"https://www.statista.com/statistics/1093781/distribution-of-googles-revenues-by-segment/\">80% to 90% of its total revenue</a>. Everything else is there to <a href=\"https://www.cnbc.com/2021/05/18/how-does-google-make-money-advertising-business-breakdown-.html\">collect users&#8217; data and direct users&#8217; attention</a> to its advertising revenue stream.</p>\n<p>After two decades in this monopoly position, Google&#8217;s search product is much more tuned to the company&#8217;s needs than those of its users. When Google Search first arrived decades ago, it was revelatory in its ability to instantly find useful information across the still-nascent web. In 2025, its search result pages are <a href=\"https://www.404media.co/google-search-really-has-gotten-worse-researchers-find/\">dominated by low-quality</a> and often AI-generated content, spam sites that exist solely to drive traffic to Amazon sales&#8212;a tactic known as <a href=\"https://www.investopedia.com/terms/a/affiliate-marketing.asp\">affiliate marketing</a>&#8212;and paid ad placements, which at times are <a href=\"https://www.cnbc.com/2020/01/24/google-will-iterate-the-design-that-made-it-harder-to-tell-ads-from-search-results.html\">indistinguishable from organic results</a>.</p>\n<p>Plenty of <a href=\"https://searchengineland.com/ai-powered-search-paid-placements-395084\">advertisers</a> and <a href=\"https://professional.dce.harvard.edu/blog/ai-will-shape-the-future-of-marketing/\">observers</a> seem to think AI-powered advertising is the future of the ad business.</p>\n<h3>Highly persuasive</h3>\n<p>Paid advertising in AI search, and AI models generally, could look very different from traditional web search. It has the potential to influence your thinking, spending patterns and even personal beliefs in much more subtle ways. Because AI can engage in active dialogue, addressing your specific questions, concerns and ideas rather than just filtering static content, its potential for influence is much greater. It&#8217;s like the difference between reading a textbook and having a conversation with its author.</p>\n<p>Imagine you&#8217;re conversing with your AI agent about an upcoming vacation. Did it recommend a particular airline or hotel chain because they really are best for you, or does the company get a kickback for every mention? If you ask about a political issue, does the model bias its answer based on which political party has paid the company a fee, or based on the bias of the model&#8217;s corporate owners?</p>\n<p>There is mounting evidence that AI models are at least as effective as people at persuading users to do things. A December 2023 meta-analysis of 121 randomized trials reported that AI models are <a href=\"https://doi.org/10.1093/joc/jqad024\">as good as humans</a> at shifting people&#8217;s perceptions, attitudes and behaviors. A more recent meta-analysis of eight studies <a href=\"https://doi.org/10.21203/rs.3.rs-7435265/v1\">similarly concluded</a> there was &#8220;no significant overall difference in persuasive performance between (large language models) and humans.&#8221;</p>\n<p>This influence may go well beyond shaping what products you buy or who you vote for. As with the field of search engine optimization, the incentive for humans to perform for AI models might <a href=\"https://www.theatlantic.com/technology/archive/2024/04/generative-ai-search-llmo/678154/\">shape the way people write</a> and communicate with each other. How we express ourselves online is likely to be increasingly directed to win the attention of AIs and earn placement in the responses they return to users.</p>\n<h3>A different way forward</h3>\n<p>Much of this is discouraging, but there is much that can be done to change it.</p>\n<p>First, it&#8217;s important to recognize that today&#8217;s AI is <a href=\"https://gizmodo.com/ai-chatgpt-can-we-build-trustworthy-ai-1850405280\">fundamentally untrustworthy</a>, for the same reasons that search engines and social media platforms are.</p>\n<p>The problem is not the technology itself; fast ways to find information and communicate with friends and family can be wonderful capabilities. The problem is the priorities of the corporations who own these platforms and for whose benefit they are operated. Recognize that you don&#8217;t have control over what data is fed to the AI, who it is shared with and how it is used. It&#8217;s important to keep that in mind when you connect devices and services to AI platforms, ask them questions, or consider buying or doing the things they suggest.</p>\n<p>There is also a lot that people can demand of governments to restrain harmful corporate uses of AI. In the U.S., Congress could <a href=\"https://www.reuters.com/legal/legalindustry/us-data-privacy-laws-enter-new-era-2023-2023-01-12/\">enshrine consumers&#8217; rights</a> to control their own personal data, as the EU already has. It could also create a data protection <a href=\"https://epic.org/campaigns/dpa/\">enforcement agency</a>, as <a href=\"https://iapp.org/resources/global-privacy-directory\">essentially every other</a> developed nation has.</p>\n<p>Governments worldwide could <a href=\"https://www.brookings.edu/articles/how-public-ai-can-strengthen-democracy/#:%7E:text=Publicly%20developed%20and%20owned%20AI,and%20sustainability%20of%20AI%20technology.\">invest in Public AI</a>&#8212;models built by public agencies offered universally for public benefit and transparently under public oversight. They could also restrict how corporations can collude to exploit people using AI, for example by barring advertisements for dangerous products such as cigarettes and requiring disclosure of paid endorsements.</p>\n<p>Every technology company seeks to differentiate itself from competitors, particularly in an era when yesterday&#8217;s groundbreaking AI quickly becomes a commodity that will run on any kid&#8217;s phone. One differentiator is in building a trustworthy service. It remains to be seen whether companies such as OpenAI and Anthropic can sustain profitable businesses on the back of subscription AI services like the premium editions of ChatGPT, Plus and Pro, and Claude Pro. If they are going to continue convincing consumers and businesses to pay for these premium services, they will need to build trust.</p>\n<p>That will require making real commitments to consumers on transparency, privacy, reliability and security that are followed through consistently and verifiably.</p>\n<p>And while no one knows what the future business models for AI will be, we can be certain that consumers do not want to be exploited by AI, secretly or otherwise.</p>\n<p><em>This essay was written with Nathan E. Sanders, and originally appeared in <a href=\"https://theconversation.com/could-chatgpt-convince-you-to-buy-something-threat-of-manipulation-looms-as-ai-companies-gear-up-to-sell-ads-272859\">The Conversation</a>.</em></p>\n",
    "content:encodedSnippet": "Eighteen months ago, it was plausible that artificial intelligence might take a different path than social media. Back then, AI’s development hadn’t consolidated under a small number of big tech firms. Nor had it capitalized on consumer attention, surveilling users and delivering ads.\nUnfortunately, the AI industry is now taking a page from the social media playbook and has set its sights on monetizing consumer attention. When OpenAI launched its ChatGPT Search feature in late 2024 and its browser, ChatGPT Atlas, in October 2025, it kicked off a race to capture online behavioral data to power advertising. It’s part of a yearslong turnabout by OpenAI, whose CEO Sam Altman once called the combination of ads and AI “unsettling” and now promises that ads can be deployed in AI apps while preserving trust. The rampant speculation among OpenAI users who believe they see paid placements in ChatGPT responses suggests they are not convinced.\nIn 2024, AI search company Perplexity started experimenting with ads in its offerings. A few months after that, Microsoft introduced ads to its Copilot AI. Google’s AI Mode for search now increasingly features ads, as does Amazon’s Rufus chatbot. OpenAI announced on Jan. 16, 2026, that it will soon begin testing ads in the unpaid version of ChatGPT.\nAs a security expert and data scientist, we see these examples as harbingers of a future where AI companies profit from manipulating their users’ behavior for the benefit of their advertisers and investors. It’s also a reminder that time to steer the direction of AI development away from private exploitation and toward public benefit is quickly running out.\nThe functionality of ChatGPT Search and its Atlas browser is not really new. Meta, commercial AI competitor Perplexity and even ChatGPT itself have had similar AI search features for years, and both Google and Microsoft beat OpenAI to the punch by integrating AI with their browsers. But OpenAI’s business positioning signals a shift.\nWe believe the ChatGPT Search and Atlas announcements are worrisome because there is really only one way to make money on search: the advertising model pioneered ruthlessly by Google.\nAdvertising model\nRuled a monopolist in U.S. federal court, Google has earned more than US$1.6 trillion in advertising revenue since 2001. You may think of Google as a web search company, or a streaming video company (YouTube), or an email company (Gmail), or a mobile phone company (Android, Pixel), or maybe even an AI company (Gemini). But those products are ancillary to Google’s bottom line. The advertising segment typically accounts for 80% to 90% of its total revenue. Everything else is there to collect users’ data and direct users’ attention to its advertising revenue stream.\nAfter two decades in this monopoly position, Google’s search product is much more tuned to the company’s needs than those of its users. When Google Search first arrived decades ago, it was revelatory in its ability to instantly find useful information across the still-nascent web. In 2025, its search result pages are dominated by low-quality and often AI-generated content, spam sites that exist solely to drive traffic to Amazon sales—a tactic known as affiliate marketing—and paid ad placements, which at times are indistinguishable from organic results.\nPlenty of advertisers and observers seem to think AI-powered advertising is the future of the ad business.\nHighly persuasive\nPaid advertising in AI search, and AI models generally, could look very different from traditional web search. It has the potential to influence your thinking, spending patterns and even personal beliefs in much more subtle ways. Because AI can engage in active dialogue, addressing your specific questions, concerns and ideas rather than just filtering static content, its potential for influence is much greater. It’s like the difference between reading a textbook and having a conversation with its author.\nImagine you’re conversing with your AI agent about an upcoming vacation. Did it recommend a particular airline or hotel chain because they really are best for you, or does the company get a kickback for every mention? If you ask about a political issue, does the model bias its answer based on which political party has paid the company a fee, or based on the bias of the model’s corporate owners?\nThere is mounting evidence that AI models are at least as effective as people at persuading users to do things. A December 2023 meta-analysis of 121 randomized trials reported that AI models are as good as humans at shifting people’s perceptions, attitudes and behaviors. A more recent meta-analysis of eight studies similarly concluded there was “no significant overall difference in persuasive performance between (large language models) and humans.”\nThis influence may go well beyond shaping what products you buy or who you vote for. As with the field of search engine optimization, the incentive for humans to perform for AI models might shape the way people write and communicate with each other. How we express ourselves online is likely to be increasingly directed to win the attention of AIs and earn placement in the responses they return to users.\nA different way forward\nMuch of this is discouraging, but there is much that can be done to change it.\nFirst, it’s important to recognize that today’s AI is fundamentally untrustworthy, for the same reasons that search engines and social media platforms are.\nThe problem is not the technology itself; fast ways to find information and communicate with friends and family can be wonderful capabilities. The problem is the priorities of the corporations who own these platforms and for whose benefit they are operated. Recognize that you don’t have control over what data is fed to the AI, who it is shared with and how it is used. It’s important to keep that in mind when you connect devices and services to AI platforms, ask them questions, or consider buying or doing the things they suggest.\nThere is also a lot that people can demand of governments to restrain harmful corporate uses of AI. In the U.S., Congress could enshrine consumers’ rights to control their own personal data, as the EU already has. It could also create a data protection enforcement agency, as essentially every other developed nation has.\nGovernments worldwide could invest in Public AI—models built by public agencies offered universally for public benefit and transparently under public oversight. They could also restrict how corporations can collude to exploit people using AI, for example by barring advertisements for dangerous products such as cigarettes and requiring disclosure of paid endorsements.\nEvery technology company seeks to differentiate itself from competitors, particularly in an era when yesterday’s groundbreaking AI quickly becomes a commodity that will run on any kid’s phone. One differentiator is in building a trustworthy service. It remains to be seen whether companies such as OpenAI and Anthropic can sustain profitable businesses on the back of subscription AI services like the premium editions of ChatGPT, Plus and Pro, and Claude Pro. If they are going to continue convincing consumers and businesses to pay for these premium services, they will need to build trust.\nThat will require making real commitments to consumers on transparency, privacy, reliability and security that are followed through consistently and verifiably.\nAnd while no one knows what the future business models for AI will be, we can be certain that consumers do not want to be exploited by AI, secretly or otherwise.\nThis essay was written with Nathan E. Sanders, and originally appeared in The Conversation.",
    "dc:creator": "Bruce Schneier",
    "comments": "https://www.schneier.com/blog/archives/2026/01/could-chatgpt-convince-you-to-buy-something.html#comments",
    "content": "<p>Eighteen months ago, it was plausible that artificial intelligence might take a <a href=\"https://www.technologyreview.com/2024/03/13/1089729/lets-not-make-the-same-mistakes-with-ai-that-we-made-with-social-media/\">different path</a> than social media. Back then, AI&#8217;s development hadn&#8217;t consolidated under a small number of big tech firms. Nor had it capitalized on consumer attention, surveilling users and delivering ads.</p>\n<p>Unfortunately, the AI industry is now taking a page from the social media playbook and has set its sights on monetizing consumer attention. When OpenAI launched its <a href=\"https://openai.com/index/introducing-chatgpt-search/\">ChatGPT Search</a> feature in late 2024 and its browser, <a href=\"https://openai.com/index/introducing-chatgpt-atlas/\">ChatGPT Atlas</a>, in October 2025, it kicked off a ...</p>",
    "contentSnippet": "Eighteen months ago, it was plausible that artificial intelligence might take a different path than social media. Back then, AI’s development hadn’t consolidated under a small number of big tech firms. Nor had it capitalized on consumer attention, surveilling users and delivering ads.\nUnfortunately, the AI industry is now taking a page from the social media playbook and has set its sights on monetizing consumer attention. When OpenAI launched its ChatGPT Search feature in late 2024 and its browser, ChatGPT Atlas, in October 2025, it kicked off a ...",
    "guid": "https://www.schneier.com/?p=71484",
    "categories": [
      "Uncategorized",
      "AI",
      "Google",
      "LLM",
      "search engines"
    ],
    "isoDate": "2026-01-20T12:08:09.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "The Hidden Risk of Orphan Accounts",
    "link": "https://thehackernews.com/2026/01/the-hidden-risk-of-orphan-accounts.html",
    "pubDate": "Tue, 20 Jan 2026 17:28:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-z9xF6QsTXxdgaSXZJQKDVTmBJGusdJ30GBYqWZ4w29KDo3yRA3HGE8BsxyQoctkOWtsv-b90dH698y-H015umuI0oOCHaPApTLj3UMcaMV9aYIqA_Oel0ZHHOy4g9qufvVc4kmRKyafIvCBKCHPsnVdx4ppZssttmenN8gEw0emeoNyTzYuuZur5s_Y/s1600/Orphan.gif"
    },
    "content": "The Problem: The Identities Left Behind\nAs organizations grow and evolve, employees, contractors, services, and systems come and go - but their accounts often remain. These abandoned or “orphan” accounts sit dormant across applications, platforms, assets, and cloud consoles.\nThe reason they persist isn’t negligence - it’s fragmentation.&nbsp;\nTraditional IAM and IGA systems are designed",
    "contentSnippet": "The Problem: The Identities Left Behind\nAs organizations grow and evolve, employees, contractors, services, and systems come and go - but their accounts often remain. These abandoned or “orphan” accounts sit dormant across applications, platforms, assets, and cloud consoles.\nThe reason they persist isn’t negligence - it’s fragmentation. \nTraditional IAM and IGA systems are designed",
    "guid": "https://thehackernews.com/2026/01/the-hidden-risk-of-orphan-accounts.html",
    "isoDate": "2026-01-20T11:58:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Evelyn Stealer Malware Abuses VS Code Extensions to Steal Developer Credentials and Crypto",
    "link": "https://thehackernews.com/2026/01/evelyn-stealer-malware-abuses-vs-code.html",
    "pubDate": "Tue, 20 Jan 2026 17:18:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXd0cDqljLdjUTKJVODHzhlibAZe2-m53DWQpXWjKLY2pZyxDivUWJFky8KJbZ48kAKDTbwuh7FtNGmI9JKsaNivp5Xr-HrK803vlk0PsfbSYX2cxkTQoOBue2mRQ6ExYZpdLi3RQUvbuMjhndYKUAaFQxNTIm69cKuKFm5AgyAs6sUerbkWXs6RwTikXV/s1600/keys.jpg"
    },
    "content": "Cybersecurity researchers have disclosed details of a malware campaign that's targeting software developers with a new information stealer called Evelyn Stealer by weaponizing the Microsoft Visual Studio Code (VS Code) extension ecosystem.\n\"The malware is designed to exfiltrate sensitive information, including developer credentials and cryptocurrency-related data. Compromised developer",
    "contentSnippet": "Cybersecurity researchers have disclosed details of a malware campaign that's targeting software developers with a new information stealer called Evelyn Stealer by weaponizing the Microsoft Visual Studio Code (VS Code) extension ecosystem.\n\"The malware is designed to exfiltrate sensitive information, including developer credentials and cryptocurrency-related data. Compromised developer",
    "guid": "https://thehackernews.com/2026/01/evelyn-stealer-malware-abuses-vs-code.html",
    "isoDate": "2026-01-20T11:48:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Cloudflare Fixes ACME Validation Bug Allowing WAF Bypass to Origin Servers",
    "link": "https://thehackernews.com/2026/01/cloudflare-fixes-acme-validation-bug.html",
    "pubDate": "Tue, 20 Jan 2026 16:42:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFMNWyLouRnOIXsad4n8lgA97dY7GEYeVjCrVWqYokwEg-es4kDCyjPCwEKZ_q0LCOCjcufDbeBu-O_aJIuKYu6IY2g9ZMoMMxhkYgCWlsjeIA0ByqwBVJX-HaF_G3oNlBuUWsTk5b4K35K60lUrjrWQ-dhMtEymdsWX84cuPUERd5balmQJMZQqsfKhnu/s1600/acme.jpg"
    },
    "content": "Cloudflare has addressed a security vulnerability impacting its Automatic Certificate Management Environment (ACME) validation logic that made it possible to bypass security controls and access origin servers.&nbsp;\n\"The vulnerability was rooted in how our edge network processed requests destined for the ACME HTTP-01 challenge path (/.well-known/acme-challenge/*),\" the web infrastructure",
    "contentSnippet": "Cloudflare has addressed a security vulnerability impacting its Automatic Certificate Management Environment (ACME) validation logic that made it possible to bypass security controls and access origin servers. \n\"The vulnerability was rooted in how our edge network processed requests destined for the ACME HTTP-01 challenge path (/.well-known/acme-challenge/*),\" the web infrastructure",
    "guid": "https://thehackernews.com/2026/01/cloudflare-fixes-acme-validation-bug.html",
    "isoDate": "2026-01-20T11:12:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Why Secrets in JavaScript Bundles are Still Being Missed",
    "link": "https://thehackernews.com/2026/01/why-secrets-in-javascript-bundles-are.html",
    "pubDate": "Tue, 20 Jan 2026 16:15:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEieV5wPGUwZgsm79ntLb9Sz_6xAPAV54CQ1SztbNfpLe-ovKEkTrCsLBRsD52c87yFiByHZGandN1pYNNBn1OLUpc5iEFJnGP0YRn_WWeWod1xxQUNhjlD1vFMh-4EI2fq06Gt86H_U7rX7FBjhLoXrhtzQKqjLRa3X6c7W86BFrR5xfDX80ve6UDs4zU8/s1600/main.jpg"
    },
    "content": "Leaked API keys are no longer unusual, nor are the breaches that follow. So why are sensitive tokens still being so easily exposed?\nTo find out, Intruder’s research team looked at what traditional vulnerability scanners actually cover and built a new secrets detection method to address gaps in existing approaches.&nbsp;\nApplying this at scale by scanning 5 million applications revealed over",
    "contentSnippet": "Leaked API keys are no longer unusual, nor are the breaches that follow. So why are sensitive tokens still being so easily exposed?\nTo find out, Intruder’s research team looked at what traditional vulnerability scanners actually cover and built a new secrets detection method to address gaps in existing approaches. \nApplying this at scale by scanning 5 million applications revealed over",
    "guid": "https://thehackernews.com/2026/01/why-secrets-in-javascript-bundles-are.html",
    "isoDate": "2026-01-20T10:45:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Tudou Guarantee Marketplace Halts Telegram Transactions After Processing Over $12 Billion",
    "link": "https://thehackernews.com/2026/01/tudou-guarantee-marketplace-halts.html",
    "pubDate": "Tue, 20 Jan 2026 13:10:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAig5-Somu-FrJikNOP0x03HGZ7BNOjkJuWqVOJpyVr1jyPxPtgCIZqqrDEn9G3FPkgryml3vqK5FeSnKaVMTWypZTvChn6hLRgJ5cjLzOS39BW0U_OW38ggPj7izAgiU5IOjATbCl6A2fRC156dZQ2Pn-Yman12ABm-uFtRN3oGofTeCZk6lpdQgll6eY/s1600/botcoin.jpg"
    },
    "content": "A Telegram-based guarantee marketplace known for advertising a broad range of illicit services appears to be winding down its operations, according to new findings from Elliptic.\nThe blockchain intelligence company said Tudou Guarantee has effectively ceased transactions through its public Telegram groups following a period of significant growth. The marketplace is estimated to have processed",
    "contentSnippet": "A Telegram-based guarantee marketplace known for advertising a broad range of illicit services appears to be winding down its operations, according to new findings from Elliptic.\nThe blockchain intelligence company said Tudou Guarantee has effectively ceased transactions through its public Telegram groups following a period of significant growth. The marketplace is estimated to have processed",
    "guid": "https://thehackernews.com/2026/01/tudou-guarantee-marketplace-halts.html",
    "isoDate": "2026-01-20T07:40:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Google Gemini Prompt Injection Flaw Exposed Private Calendar Data via Malicious Invites",
    "link": "https://thehackernews.com/2026/01/google-gemini-prompt-injection-flaw.html",
    "pubDate": "Mon, 19 Jan 2026 22:51:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRoKH8kODtvGyUQmuWC8vWW2D9wJIlBhYnl2hlvLUojFSV7sVmZJ3nqnxDHHMhIqzIpXmcFy_x6GOxTYsrvTa9lJw4tE0vPvx8OPrRV7bmpBp_z7babFHy88b09t9fVw_Xw326BOFDFZ54PwPeihsITRugqHdOTqQjvTqvjmufoPqD5RmtLWy8WUNsHuOn/s1600/gem.jpg"
    },
    "content": "Cybersecurity researchers have disclosed details of a security flaw that leverages indirect prompt injection targeting Google Gemini as a way to bypass authorization guardrails and use Google Calendar as a data extraction mechanism.\nThe vulnerability, Miggo Security's Head of Research, Liad Eliyahu, said, made it possible to circumvent Google Calendar's privacy controls by hiding a dormant",
    "contentSnippet": "Cybersecurity researchers have disclosed details of a security flaw that leverages indirect prompt injection targeting Google Gemini as a way to bypass authorization guardrails and use Google Calendar as a data extraction mechanism.\nThe vulnerability, Miggo Security's Head of Research, Liad Eliyahu, said, made it possible to circumvent Google Calendar's privacy controls by hiding a dormant",
    "guid": "https://thehackernews.com/2026/01/google-gemini-prompt-injection-flaw.html",
    "isoDate": "2026-01-19T17:21:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "⚡ Weekly Recap: Fortinet Exploits, RedLine Clipjack, NTLM Crack, Copilot Attack & More",
    "link": "https://thehackernews.com/2026/01/weekly-recap-fortinet-exploits-redline.html",
    "pubDate": "Mon, 19 Jan 2026 18:47:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfn1SBAkKZzvUvFxUuVea_4KYXozKlXG-6dDyXBZDwvc_b3-rDxdmhyphenhyphen1BVrjoSsCReCbC28V3ta7Wo00qmg6zk3cuiezDIp3g8Noqbx4sEbmDlCN7aF6LCFZWLn5a0RS5iaKfd4pyU2M7MiIF63xpRv9XbPL1dP08O2L0UyGKCf22cFB6IsH39VQTIhQ7E/s1600/recap.jpg"
    },
    "content": "In cybersecurity, the line between a normal update and a serious incident keeps getting thinner. Systems that once felt reliable are now under pressure from constant change. New AI tools, connected devices, and automated systems quietly create more ways in, often faster than security teams can react. This week’s stories show how easily a small mistake or hidden service can turn into a real",
    "contentSnippet": "In cybersecurity, the line between a normal update and a serious incident keeps getting thinner. Systems that once felt reliable are now under pressure from constant change. New AI tools, connected devices, and automated systems quietly create more ways in, often faster than security teams can react. This week’s stories show how easily a small mistake or hidden service can turn into a real",
    "guid": "https://thehackernews.com/2026/01/weekly-recap-fortinet-exploits-redline.html",
    "isoDate": "2026-01-19T13:17:00.000Z",
    "itunes": {}
  },
  {
    "creator": "Bruce Schneier",
    "title": "AI-Powered Surveillance in Schools",
    "link": "https://www.schneier.com/blog/archives/2026/01/ai-powered-surveillance-in-schools.html",
    "pubDate": "Mon, 19 Jan 2026 12:02:24 +0000",
    "content:encoded": "<p>It all sounds <a href=\"https://www.forbes.com/sites/thomasbrewster/2025/12/16/ai-bathroom-monitors-welcome-to-americas-new-surveillance-high-schools/\">pretty dystopian</a>:</p>\n<blockquote><p>Inside a white stucco building in Southern California, video cameras compare faces of passersby against a facial recognition database. Behavioral analysis AI reviews the footage for signs of violent behavior. Behind a bathroom door, a smoke detector-shaped device captures audio, listening for sounds of distress. Outside, drones stand ready to be deployed and provide intel from above, and license plate readers from $8.5 billion surveillance behemoth Flock Safety ensure the cars entering and exiting the parking lot aren&#8217;t driven by criminals.</p>\n<p>This isn&#8217;t a high-security government facility. It&#8217;s Beverly Hills High School.</p></blockquote>\n",
    "content:encodedSnippet": "It all sounds pretty dystopian:\nInside a white stucco building in Southern California, video cameras compare faces of passersby against a facial recognition database. Behavioral analysis AI reviews the footage for signs of violent behavior. Behind a bathroom door, a smoke detector-shaped device captures audio, listening for sounds of distress. Outside, drones stand ready to be deployed and provide intel from above, and license plate readers from $8.5 billion surveillance behemoth Flock Safety ensure the cars entering and exiting the parking lot aren’t driven by criminals.\nThis isn’t a high-security government facility. It’s Beverly Hills High School.",
    "dc:creator": "Bruce Schneier",
    "comments": "https://www.schneier.com/blog/archives/2026/01/ai-powered-surveillance-in-schools.html#comments",
    "content": "<p>It all sounds <a href=\"https://www.forbes.com/sites/thomasbrewster/2025/12/16/ai-bathroom-monitors-welcome-to-americas-new-surveillance-high-schools/\">pretty dystopian</a>:</p>\n<blockquote><p>Inside a white stucco building in Southern California, video cameras compare faces of passersby against a facial recognition database. Behavioral analysis AI reviews the footage for signs of violent behavior. Behind a bathroom door, a smoke detector-shaped device captures audio, listening for sounds of distress. Outside, drones stand ready to be deployed and provide intel from above, and license plate readers from $8.5 billion surveillance behemoth Flock Safety ensure the cars entering and exiting the parking lot aren&#8217;t driven by criminals...</p></blockquote>",
    "contentSnippet": "It all sounds pretty dystopian:\nInside a white stucco building in Southern California, video cameras compare faces of passersby against a facial recognition database. Behavioral analysis AI reviews the footage for signs of violent behavior. Behind a bathroom door, a smoke detector-shaped device captures audio, listening for sounds of distress. Outside, drones stand ready to be deployed and provide intel from above, and license plate readers from $8.5 billion surveillance behemoth Flock Safety ensure the cars entering and exiting the parking lot aren’t driven by criminals...",
    "guid": "https://www.schneier.com/?p=71478",
    "categories": [
      "Uncategorized",
      "AI",
      "privacy",
      "schools",
      "surveillance"
    ],
    "isoDate": "2026-01-19T12:02:24.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "DevOps & SaaS Downtime: The High (and Hidden) Costs for Cloud-First Businesses",
    "link": "https://thehackernews.com/2026/01/high-costs-of-devops-saas-downtime.html",
    "pubDate": "Mon, 19 Jan 2026 17:25:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-NFsj68xaf2p3zjA44oQOJv4NBoDt21KPCna8vnSvswE0hTcKyO7_ExzxpSX_PadfPFzlnA2bQ2gNpsMDNKqwIT_Rlj5GgtssnvnSfnbHCgJl4hH7u4XNRhfm7Yqd7wTnZ-8pU_THZ6wt7sFCQGxzXwFlczbU2DH5fQ7BakcnJ15uc7_lMefQlkrEpOw/s1600/devops.jpg"
    },
    "content": "Just a few years ago, the cloud was touted as the “magic pill” for any cyber threat or performance issue. Many were lured by the “always-on” dream, trading granular control for the convenience of managed services.\nIn recent years, many of us have learned (often the hard way) that public cloud service providers are not immune to attacks and SaaS downtime, hiding behind the Shared Responsibility",
    "contentSnippet": "Just a few years ago, the cloud was touted as the “magic pill” for any cyber threat or performance issue. Many were lured by the “always-on” dream, trading granular control for the convenience of managed services.\nIn recent years, many of us have learned (often the hard way) that public cloud service providers are not immune to attacks and SaaS downtime, hiding behind the Shared Responsibility",
    "guid": "https://thehackernews.com/2026/01/high-costs-of-devops-saas-downtime.html",
    "isoDate": "2026-01-19T11:55:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "New StackWarp Hardware Flaw Breaks AMD SEV-SNP Protections on Zen 1–5 CPUs",
    "link": "https://thehackernews.com/2026/01/new-stackwarp-hardware-flaw-breaks-amd.html",
    "pubDate": "Mon, 19 Jan 2026 17:01:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjvBKIwExgNlDEHvCub_5W4y4uhPhsKYtGX8oPC_DrwzV5Wl5i_QPN2oEgBoSHBAAbsurdEryR3O56kclGi5uN38Y50ftQUAPzj1Y-r0bAS3rQ_lqRm88M4umTl1e4bAiCAGArNy2v70l31w5IArnzj8rUqxCAL2Xd3C8j2POKMP8tr6iAhsynSMNPAZH6/s1600/stack.jpg"
    },
    "content": "A team of academics from the CISPA Helmholtz Center for Information Security in Germany has disclosed the details of a new hardware vulnerability affecting AMD processors.\nThe security flaw, codenamed StackWarp, can allow bad actors with privileged control over a host server to run malicious code within confidential virtual machines (CVMs), undermining the integrity guarantees provided by AMD",
    "contentSnippet": "A team of academics from the CISPA Helmholtz Center for Information Security in Germany has disclosed the details of a new hardware vulnerability affecting AMD processors.\nThe security flaw, codenamed StackWarp, can allow bad actors with privileged control over a host server to run malicious code within confidential virtual machines (CVMs), undermining the integrity guarantees provided by AMD",
    "guid": "https://thehackernews.com/2026/01/new-stackwarp-hardware-flaw-breaks-amd.html",
    "isoDate": "2026-01-19T11:31:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "CrashFix Chrome Extension Delivers ModeloRAT Using ClickFix-Style Browser Crash Lures",
    "link": "https://thehackernews.com/2026/01/crashfix-chrome-extension-delivers.html",
    "pubDate": "Mon, 19 Jan 2026 14:39:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHk6v6YRLFG3a9vVJQ5OLdPuN9cBdDklQzJHf91XqJUjDiKB8IT7VEmxB7VyKcSXbahTKXr-seTEvP3hdRWT2ws4xUteRY4tfouGxjDrH22aIxauonv9sb7Gw6TgCRh4GfoMCZLnp6gL79_feODNKp7LHF8nkte3e6mvt4d3UERPwMxUvHgnF1e6uB2BSf/s1600/edges.jpg"
    },
    "content": "Cybersecurity researchers have disclosed details of an ongoing campaign dubbed KongTuke that used a malicious Google Chrome extension masquerading as an ad blocker to deliberately crash the web browser and trick victims into running arbitrary commands using ClickFix-like lures to deliver a previously undocumented remote access trojan (RAT) dubbed ModeloRAT.\nThis new escalation of ClickFix,",
    "contentSnippet": "Cybersecurity researchers have disclosed details of an ongoing campaign dubbed KongTuke that used a malicious Google Chrome extension masquerading as an ad blocker to deliberately crash the web browser and trick victims into running arbitrary commands using ClickFix-like lures to deliver a previously undocumented remote access trojan (RAT) dubbed ModeloRAT.\nThis new escalation of ClickFix,",
    "guid": "https://thehackernews.com/2026/01/crashfix-chrome-extension-delivers.html",
    "isoDate": "2026-01-19T09:09:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Security Bug in StealC Malware Panel Let Researchers Spy on Threat Actor Operations",
    "link": "https://thehackernews.com/2026/01/security-bug-in-stealc-malware-panel.html",
    "pubDate": "Mon, 19 Jan 2026 12:23:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWgXm3VbZRhqFOmPi88AiXeiRLKHkASKs6cVSYmMLmlZiGyP3Xqdd8X1J5qkppk2zMYcKK5-rkza0xId4pky69xHymWs7aX3DXRaMGmDZm0WgtfPRjw3OyD4G2VfuB_4WveR3GQ3b0das7-37IWLkZbEQDG_jiheSCynC23sxLFLhW4Lnxhlfa_X2EAqzd/s1600/uno.jpg"
    },
    "content": "Cybersecurity researchers have disclosed a cross-site scripting (XSS) vulnerability in the web-based control panel used by operators of the StealC information stealer, allowing them to gather crucial insights on one of the threat actors using the malware in their operations.\n\"By exploiting it, we were able to collect system fingerprints, monitor active sessions, and – in a twist that will",
    "contentSnippet": "Cybersecurity researchers have disclosed a cross-site scripting (XSS) vulnerability in the web-based control panel used by operators of the StealC information stealer, allowing them to gather crucial insights on one of the threat actors using the malware in their operations.\n\"By exploiting it, we were able to collect system fingerprints, monitor active sessions, and – in a twist that will",
    "guid": "https://thehackernews.com/2026/01/security-bug-in-stealc-malware-panel.html",
    "isoDate": "2026-01-19T06:53:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Black Basta Ransomware Leader Added to EU Most Wanted and INTERPOL Red Notice",
    "link": "https://thehackernews.com/2026/01/black-basta-ransomware-hacker-leader.html",
    "pubDate": "Sat, 17 Jan 2026 21:56:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh3VY2I87McGcsRT2S52vO2sa1A9ZvLo_H6YYLk5sCypOvjoX1H4Ee9QkvAiFJIvy0qW0dvgi_db78HgDvWHJKW9Fkaf5sLOt-RA2sP8fooQrHtv0qFf6Cazo7YjXAKY7NXJdNotK4w8aiiuz1nzNzmGbiJ6l81rbrU5-Sa7oOlMiRl08-4_4p9XtsjB1Am/s1600/Ukrainian-hacker.jpg"
    },
    "content": "Ukrainian and German law enforcement authorities have identified two Ukrainians suspected of working for the Russia-linked ransomware-as-a-service (RaaS) group Black Basta.\nIn addition, the group's alleged leader, a 35-year-old Russian national named Oleg Evgenievich Nefedov (Нефедов Олег Евгеньевич), has been added to the European Union's Most Wanted and INTERPOL's Red Notice lists, authorities",
    "contentSnippet": "Ukrainian and German law enforcement authorities have identified two Ukrainians suspected of working for the Russia-linked ransomware-as-a-service (RaaS) group Black Basta.\nIn addition, the group's alleged leader, a 35-year-old Russian national named Oleg Evgenievich Nefedov (Нефедов Олег Евгеньевич), has been added to the European Union's Most Wanted and INTERPOL's Red Notice lists, authorities",
    "guid": "https://thehackernews.com/2026/01/black-basta-ransomware-hacker-leader.html",
    "isoDate": "2026-01-17T16:26:00.000Z",
    "itunes": {}
  },
  {
    "creator": "Andy Greenberg, Maddy Varner, Lily Hay Newman",
    "title": "US Hackers Reportedly Caused a Blackout in Venezuela",
    "link": "https://www.wired.com/story/security-news-this-week-us-hackers-reportedly-caused-a-blackout-in-venezuela/",
    "pubDate": "Sat, 17 Jan 2026 11:30:00 +0000",
    "dc:creator": "Andy Greenberg, Maddy Varner, Lily Hay Newman",
    "content": "Plus: AI reportedly caused ICE to send agents into the field without training, Palantir’s app for targeting immigrants gets exposed, and more.",
    "contentSnippet": "Plus: AI reportedly caused ICE to send agents into the field without training, Palantir’s app for targeting immigrants gets exposed, and more.",
    "guid": "696a75fdb12ced811baf1872",
    "categories": [
      "Security",
      "Security / Cyberattacks and Hacks",
      "Security / National Security",
      "Security / Privacy",
      "Security / Security News"
    ],
    "isoDate": "2026-01-17T11:30:00.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "OpenAI to Show Ads in ChatGPT for Logged-In U.S. Adults on Free and Go Plans",
    "link": "https://thehackernews.com/2026/01/openai-to-show-ads-in-chatgpt-for.html",
    "pubDate": "Sat, 17 Jan 2026 14:04:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhSamhve9k7iL2Jd0BpqUkreaa3K7itkI4XIHJVBxqZ6l07U24t76eNbuhKMnRtu7eJ66TmiqgVQbAATH-pHha8jEcBz2PePIWaQDrWPky4mT6KFKVxjxe0N-yTFVEjXa3263Nt2IO_jmvJOjld503G-i99zT-7xeNf-BiAYFtyVdzV8QOu6tMvRyQbvYtn/s1600/chatgpt.jpg"
    },
    "content": "OpenAI on Friday said it would start showing ads in ChatGPT to logged-in adult U.S. users in both the free and ChatGPT Go tiers in the coming weeks, as the artificial intelligence (AI) company expanded access to its low-cost subscription globally.\n\"You need to know that your data and conversations are protected and never sold to advertisers,\" OpenAI said. \"And we need to keep a high bar and give",
    "contentSnippet": "OpenAI on Friday said it would start showing ads in ChatGPT to logged-in adult U.S. users in both the free and ChatGPT Go tiers in the coming weeks, as the artificial intelligence (AI) company expanded access to its low-cost subscription globally.\n\"You need to know that your data and conversations are protected and never sold to advertisers,\" OpenAI said. \"And we need to keep a high bar and give",
    "guid": "https://thehackernews.com/2026/01/openai-to-show-ads-in-chatgpt-for.html",
    "isoDate": "2026-01-17T08:34:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "GootLoader Malware Uses 500–1,000 Concatenated ZIP Archives to Evade Detection",
    "link": "https://thehackernews.com/2026/01/gootloader-malware-uses-5001000.html",
    "pubDate": "Fri, 16 Jan 2026 23:29:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9KFkvTcZ_PkWZffX2HE8olvWbZAOrsco48R3JqZQsB4hUc2GGS7xh9a6suu7I_eIe4NU8M5sjecperqY7v7V4nSuAffVy024xnlN38Ig-LPae-RiwXXxMUnf8FvvNX9HVjsf1yHr4cZaDxww_pa53yqGlSNfQtnsA6NZGwELbUyE5pPKOp96QK-BQI3GB/s1600/code.jpg"
    },
    "content": "The JavaScript (aka JScript) malware loader called GootLoader has been observed using a malformed ZIP archive that's designed to sidestep detection efforts by concatenating anywhere from 500 to 1,000 archives.\n\"The actor creates a malformed archive as an anti-analysis technique,\" Expel security researcher Aaron Walton said in a report shared with The Hacker News. \"That is, many unarchiving tools",
    "contentSnippet": "The JavaScript (aka JScript) malware loader called GootLoader has been observed using a malformed ZIP archive that's designed to sidestep detection efforts by concatenating anywhere from 500 to 1,000 archives.\n\"The actor creates a malformed archive as an anti-analysis technique,\" Expel security researcher Aaron Walton said in a report shared with The Hacker News. \"That is, many unarchiving tools",
    "guid": "https://thehackernews.com/2026/01/gootloader-malware-uses-5001000.html",
    "isoDate": "2026-01-16T17:59:00.000Z",
    "itunes": {}
  },
  {
    "creator": "Bruce Schneier",
    "title": "AI and the Corporate Capture of Knowledge",
    "link": "https://www.schneier.com/blog/archives/2026/01/ai-and-the-corporate-capture-of-knowledge.html",
    "pubDate": "Fri, 16 Jan 2026 14:44:27 +0000",
    "content:encoded": "<p>More than a decade after <a href=\"https://www.sfgate.com/technology/article/Open-access-tributes-to-Aaron-Swartz-4193965.php\">Aaron Swartz&#8217;s death</a>, the United States is still living inside the contradiction that destroyed him.</p>\n<p>Swartz believed that knowledge, especially publicly funded knowledge, should be freely accessible. Acting on that, he downloaded thousands of academic articles from the <a href=\"https://www.jstor.org/\">JSTOR</a> archive with the intention of making them publicly available. For this, the federal government charged him with a felony and threatened decades in prison. After two years of prosecutorial pressure, Swartz died by suicide on Jan. 11, 2013.</p>\n<p>The still-unresolved questions raised by his case have resurfaced in today&#8217;s debates over artificial intelligence, copyright and the ultimate control of knowledge.</p>\n<p>At the time of Swartz&#8217;s prosecution, vast amounts of research were funded by taxpayers, conducted at public institutions and intended to advance public understanding. But access to that research was, and still is, locked behind expensive paywalls. People are unable to read work they helped fund without paying private journals and research websites.</p>\n<p>Swartz considered this hoarding of knowledge to be neither accidental nor inevitable. It was the result of legal, economic and political choices. His actions challenged those choices directly. And for that, the government treated him as a criminal.</p>\n<p>Today&#8217;s AI arms race involves a far more expansive, profit-driven form of information appropriation. The tech giants ingest vast amounts of copyrighted material: books, journalism, academic papers, art, music and personal writing. This data is scraped at industrial scale, often without consent, compensation or transparency, and then used to train large AI models.</p>\n<p>AI companies then sell their proprietary systems, built on public and private knowledge, back to the people who funded it. But this time, the government&#8217;s response has been markedly different. There are no criminal prosecutions, no threats of decades-long prison sentences. Lawsuits proceed slowly, enforcement remains uncertain and policymakers signal caution, given AI&#8217;s perceived economic and strategic importance. Copyright infringement is reframed as an unfortunate but necessary step toward &#8220;innovation.&#8221;</p>\n<p>Recent developments underscore this imbalance. In 2025, <a href=\"https://www.npr.org/2025/09/05/nx-s1-5529404/anthropic-settlement-authors-copyright-ai\">Anthropic</a> reached a settlement with publishers over allegations that its AI systems were trained on copyrighted books without authorization. The agreement reportedly valued infringement at roughly $3,000 per book across an estimated 500,000 works, coming at a cost of over $1.5 billion. Plagiarism disputes between artists and accused infringers routinely settle for hundreds of thousands, or even millions, of dollars when prominent works are involved. Scholars estimate Anthropic avoided over <a href=\"https://www.lawfaremedia.org/article/anthropic-s-settlement-shows-the-u.s.-can-t-afford-ai-copyright-lawsuits\">$1 trillion in liability costs</a>. For well-capitalized AI firms, such settlements are likely being factored as a predictable cost of doing business.</p>\n<p>As AI becomes a larger part of America&#8217;s economy, one can see the writing on the wall. Judges will twist themselves into knots to justify an innovative technology premised on literally stealing the works of artists, poets, musicians, all of academia and the internet, and vast expanses of literature. But if Swartz&#8217;s actions were criminal, it is worth asking: What standard are we now applying to AI companies?</p>\n<p>The question is not simply whether copyright law applies to AI. It is why the law appears to operate so differently depending on who is doing the extracting and for what purpose.</p>\n<p>The stakes extend beyond copyright law or past injustices. They concern who controls the infrastructure of knowledge going forward and what that control means for democratic participation, accountability and public trust.</p>\n<p>Systems trained on vast bodies of publicly funded research are increasingly becoming the primary way people learn about science, law, medicine and public policy. As search, synthesis and explanation are mediated through AI models, control over training data and infrastructure translates into control over what questions can be asked, what answers are surfaced, and whose expertise is treated as authoritative. If public knowledge is absorbed into proprietary systems that the public cannot inspect, audit or meaningfully challenge, then access to information is no longer governed by democratic norms but by corporate priorities.</p>\n<p>Like the early internet, AI is often described as a democratizing force. But also like the internet, AI&#8217;s current trajectory suggests something closer to consolidation. Control over data, models and computational infrastructure is concentrated in the hands of a small number of powerful tech companies. They will decide who gets access to knowledge, under what conditions and at what price.</p>\n<p>Swartz&#8217;s fight was not simply about access, but about whether knowledge should be governed by openness or corporate capture, and who that knowledge is ultimately for. He understood that access to knowledge is a prerequisite for democracy. A society cannot meaningfully debate policy, science or justice if information is locked away behind paywalls or controlled by proprietary algorithms. If we allow AI companies to profit from mass appropriation while claiming immunity, we are choosing a future in which access to knowledge is governed by corporate power rather than democratic values.</p>\n<p>How we treat knowledge&#8212;who may access it, who may profit from it and who is punished for sharing it&#8212;has become a test of our democratic commitments. We should be honest about what those choices say about us.</p>\n<p><em>This essay was written with J. B. Branch, and originally appeared in the <a href=\"https://www.sfchronicle.com/opinion/openforum/article/ai-copyright-research-law-21282101.php\">San Francisco Chronicle</a>.</em></p>\n",
    "content:encodedSnippet": "More than a decade after Aaron Swartz’s death, the United States is still living inside the contradiction that destroyed him.\nSwartz believed that knowledge, especially publicly funded knowledge, should be freely accessible. Acting on that, he downloaded thousands of academic articles from the JSTOR archive with the intention of making them publicly available. For this, the federal government charged him with a felony and threatened decades in prison. After two years of prosecutorial pressure, Swartz died by suicide on Jan. 11, 2013.\nThe still-unresolved questions raised by his case have resurfaced in today’s debates over artificial intelligence, copyright and the ultimate control of knowledge.\nAt the time of Swartz’s prosecution, vast amounts of research were funded by taxpayers, conducted at public institutions and intended to advance public understanding. But access to that research was, and still is, locked behind expensive paywalls. People are unable to read work they helped fund without paying private journals and research websites.\nSwartz considered this hoarding of knowledge to be neither accidental nor inevitable. It was the result of legal, economic and political choices. His actions challenged those choices directly. And for that, the government treated him as a criminal.\nToday’s AI arms race involves a far more expansive, profit-driven form of information appropriation. The tech giants ingest vast amounts of copyrighted material: books, journalism, academic papers, art, music and personal writing. This data is scraped at industrial scale, often without consent, compensation or transparency, and then used to train large AI models.\nAI companies then sell their proprietary systems, built on public and private knowledge, back to the people who funded it. But this time, the government’s response has been markedly different. There are no criminal prosecutions, no threats of decades-long prison sentences. Lawsuits proceed slowly, enforcement remains uncertain and policymakers signal caution, given AI’s perceived economic and strategic importance. Copyright infringement is reframed as an unfortunate but necessary step toward “innovation.”\nRecent developments underscore this imbalance. In 2025, Anthropic reached a settlement with publishers over allegations that its AI systems were trained on copyrighted books without authorization. The agreement reportedly valued infringement at roughly $3,000 per book across an estimated 500,000 works, coming at a cost of over $1.5 billion. Plagiarism disputes between artists and accused infringers routinely settle for hundreds of thousands, or even millions, of dollars when prominent works are involved. Scholars estimate Anthropic avoided over $1 trillion in liability costs. For well-capitalized AI firms, such settlements are likely being factored as a predictable cost of doing business.\nAs AI becomes a larger part of America’s economy, one can see the writing on the wall. Judges will twist themselves into knots to justify an innovative technology premised on literally stealing the works of artists, poets, musicians, all of academia and the internet, and vast expanses of literature. But if Swartz’s actions were criminal, it is worth asking: What standard are we now applying to AI companies?\nThe question is not simply whether copyright law applies to AI. It is why the law appears to operate so differently depending on who is doing the extracting and for what purpose.\nThe stakes extend beyond copyright law or past injustices. They concern who controls the infrastructure of knowledge going forward and what that control means for democratic participation, accountability and public trust.\nSystems trained on vast bodies of publicly funded research are increasingly becoming the primary way people learn about science, law, medicine and public policy. As search, synthesis and explanation are mediated through AI models, control over training data and infrastructure translates into control over what questions can be asked, what answers are surfaced, and whose expertise is treated as authoritative. If public knowledge is absorbed into proprietary systems that the public cannot inspect, audit or meaningfully challenge, then access to information is no longer governed by democratic norms but by corporate priorities.\nLike the early internet, AI is often described as a democratizing force. But also like the internet, AI’s current trajectory suggests something closer to consolidation. Control over data, models and computational infrastructure is concentrated in the hands of a small number of powerful tech companies. They will decide who gets access to knowledge, under what conditions and at what price.\nSwartz’s fight was not simply about access, but about whether knowledge should be governed by openness or corporate capture, and who that knowledge is ultimately for. He understood that access to knowledge is a prerequisite for democracy. A society cannot meaningfully debate policy, science or justice if information is locked away behind paywalls or controlled by proprietary algorithms. If we allow AI companies to profit from mass appropriation while claiming immunity, we are choosing a future in which access to knowledge is governed by corporate power rather than democratic values.\nHow we treat knowledge—who may access it, who may profit from it and who is punished for sharing it—has become a test of our democratic commitments. We should be honest about what those choices say about us.\nThis essay was written with J. B. Branch, and originally appeared in the San Francisco Chronicle.",
    "dc:creator": "Bruce Schneier",
    "comments": "https://www.schneier.com/blog/archives/2026/01/ai-and-the-corporate-capture-of-knowledge.html#comments",
    "content": "<p>More than a decade after <a href=\"https://www.sfgate.com/technology/article/Open-access-tributes-to-Aaron-Swartz-4193965.php\">Aaron Swartz&#8217;s death</a>, the United States is still living inside the contradiction that destroyed him.</p>\n<p>Swartz believed that knowledge, especially publicly funded knowledge, should be freely accessible. Acting on that, he downloaded thousands of academic articles from the <a href=\"https://www.jstor.org/\">JSTOR</a> archive with the intention of making them publicly available. For this, the federal government charged him with a felony and threatened decades in prison. After two years of prosecutorial pressure, Swartz died by suicide on Jan. 11, 2013.</p>\n<p>The still-unresolved questions raised by his case have resurfaced in today&#8217;s debates over artificial intelligence, copyright and the ultimate control of knowledge...</p>",
    "contentSnippet": "More than a decade after Aaron Swartz’s death, the United States is still living inside the contradiction that destroyed him.\nSwartz believed that knowledge, especially publicly funded knowledge, should be freely accessible. Acting on that, he downloaded thousands of academic articles from the JSTOR archive with the intention of making them publicly available. For this, the federal government charged him with a felony and threatened decades in prison. After two years of prosecutorial pressure, Swartz died by suicide on Jan. 11, 2013.\nThe still-unresolved questions raised by his case have resurfaced in today’s debates over artificial intelligence, copyright and the ultimate control of knowledge...",
    "guid": "https://www.schneier.com/?p=71475",
    "categories": [
      "Uncategorized",
      "Aaron Swartz",
      "AI",
      "copyright",
      "LLM"
    ],
    "isoDate": "2026-01-16T14:44:27.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Five Malicious Chrome Extensions Impersonate Workday and NetSuite to Hijack Accounts",
    "link": "https://thehackernews.com/2026/01/five-malicious-chrome-extensions.html",
    "pubDate": "Fri, 16 Jan 2026 19:39:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh7kvgFz8ELXceV5_LsBgcodkjSM6meRhVnuxiijpgc1_2XcMb9Gh660H5HEknLGskzkx9L81bbOM651BUVAzu2u6bf36gf5r2x3ndhltKVFZAforGzT-dQhny2-zEk3cih03DU1pfyRFiJLlNXuv_Ml-glQC1dGl95L1KpYHBXnMOfXZwfd4bWGaUH3uel/s1600/chrome.jpg"
    },
    "content": "Cybersecurity researchers have discovered five new malicious Google Chrome web browser extensions that masquerade as human resources (HR) and enterprise resource planning (ERP) platforms like Workday, NetSuite, and SuccessFactors to take control of victim accounts.\n\"The extensions work in concert to steal authentication tokens, block incident response capabilities, and enable complete account",
    "contentSnippet": "Cybersecurity researchers have discovered five new malicious Google Chrome web browser extensions that masquerade as human resources (HR) and enterprise resource planning (ERP) platforms like Workday, NetSuite, and SuccessFactors to take control of victim accounts.\n\"The extensions work in concert to steal authentication tokens, block incident response capabilities, and enable complete account",
    "guid": "https://thehackernews.com/2026/01/five-malicious-chrome-extensions.html",
    "isoDate": "2026-01-16T14:09:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Your Digital Footprint Can Lead Right to Your Front Door",
    "link": "https://thehackernews.com/2026/01/your-digital-footprint-can-lead-right.html",
    "pubDate": "Fri, 16 Jan 2026 16:12:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_20kAAwaCZD2Lh6-cSZ4bMJ5QJq16p3GRg-tw994MYxsXjW6EikJ7cJbFhb-I6H3hZQRN0blLVlY_zd1993WBJixaFKtO0eXeskqu6HKzE25lZWtfHQckAbh-doxDoBQkcuEazU3GkBSXWP-uNKXvjXdK9NSAGo_BEOPYD7vntOgaSAjVuRI0_lOSwRU/s1600/Incogni.jpg"
    },
    "content": "You lock your doors at night. You avoid sketchy phone calls. You’re careful about what you post on social media.\nBut what about the information about you that’s already out there—without your permission?\nYour name. Home address. Phone number. Past jobs. Family members. Old usernames.\nIt’s all still online, and it’s a lot easier to find than you think.\nThe hidden safety threat lurking online\nMost",
    "contentSnippet": "You lock your doors at night. You avoid sketchy phone calls. You’re careful about what you post on social media.\nBut what about the information about you that’s already out there—without your permission?\nYour name. Home address. Phone number. Past jobs. Family members. Old usernames.\nIt’s all still online, and it’s a lot easier to find than you think.\nThe hidden safety threat lurking online\nMost",
    "guid": "https://thehackernews.com/2026/01/your-digital-footprint-can-lead-right.html",
    "isoDate": "2026-01-16T10:42:00.000Z",
    "itunes": {}
  }
]