[
  {
    "creator": "Jacob Roach",
    "title": "6 Best VPN Services (2025), Tested and Reviewed",
    "link": "https://www.wired.com/gallery/best-vpn/",
    "pubDate": "Mon, 20 Oct 2025 13:00:00 +0000",
    "dc:creator": "Jacob Roach",
    "content": "Every VPN says it’s the best, but only some of them are telling the truth.",
    "contentSnippet": "Every VPN says it’s the best, but only some of them are telling the truth.",
    "guid": "68af63e252be2b76ffa4c01a",
    "categories": [
      "Gear",
      "Gear / Buying Guides",
      "Gear / Products / Online Services",
      "Security / Privacy"
    ],
    "isoDate": "2025-10-20T13:00:00.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "⚡ Weekly Recap: F5 Breached, Linux Rootkits, Pixnapping Attack, EtherHiding & More",
    "link": "https://thehackernews.com/2025/10/weekly-recap-f5-breached-linux-rootkits.html",
    "pubDate": "Mon, 20 Oct 2025 17:57:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN5kly7iFKUp7UBUq7oHKiUGJ80W3o91xqoevd_txiAmElk-EIlQ0ElPbz3_WlYGw67bLyZJkDPWPIUSGosXJk28DbZU930KYLdyO0p3xzqAcS4tFWCQUq2Q-5HrxdZOcBwuCud9lzkLD4LDecrNp0p6-A4P0w-A0GXBpWyzq9T6bWVTpblMlIpOo_Infg/s1600/recap-main.jpg"
    },
    "content": "It’s easy to think your defenses are solid — until you realize attackers have been inside them the whole time. The latest incidents show that long-term, silent breaches are becoming the norm. The best defense now isn’t just patching fast, but watching smarter and staying alert for what you don’t expect.\nHere’s a quick look at this week’s top threats, new tactics, and security stories shaping",
    "contentSnippet": "It’s easy to think your defenses are solid — until you realize attackers have been inside them the whole time. The latest incidents show that long-term, silent breaches are becoming the norm. The best defense now isn’t just patching fast, but watching smarter and staying alert for what you don’t expect.\nHere’s a quick look at this week’s top threats, new tactics, and security stories shaping",
    "guid": "https://thehackernews.com/2025/10/weekly-recap-f5-breached-linux-rootkits.html",
    "isoDate": "2025-10-20T12:27:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Analysing ClickFix: 3 Reasons Why Copy/Paste Attacks Are Driving Security Breaches",
    "link": "https://thehackernews.com/2025/10/analysing-clickfix-3-reasons-why.html",
    "pubDate": "Mon, 20 Oct 2025 17:25:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX5038UqY-M1JonfIYTCdPFsPCsb09z9DKaRKFYnUKX8zHbn3_Sm1REhOYuMZmA-kMjLMSO9b3W9-8IRdWHqb4MDCP4s0o4t1FW7CvWbbCboLOQE8tlewbQ4BxBONZAMKRfJIc4rwCgejK5hp-_zhV9W5cuO44S38f6pVa0TAfKd8F86l5PjAZUel1Xuk/s1600/push.jpg"
    },
    "content": "ClickFix, FileFix, fake CAPTCHA — whatever you call it, attacks where users interact with malicious scripts in their web browser are a fast-growing source of security breaches.&nbsp;\nClickFix attacks prompt the user to solve some kind of problem or challenge in the browser — most commonly a CAPTCHA, but also things like fixing an error on a webpage.&nbsp;\nThe name is a little misleading, though",
    "contentSnippet": "ClickFix, FileFix, fake CAPTCHA — whatever you call it, attacks where users interact with malicious scripts in their web browser are a fast-growing source of security breaches. \nClickFix attacks prompt the user to solve some kind of problem or challenge in the browser — most commonly a CAPTCHA, but also things like fixing an error on a webpage. \nThe name is a little misleading, though",
    "guid": "https://thehackernews.com/2025/10/analysing-clickfix-3-reasons-why.html",
    "isoDate": "2025-10-20T11:55:00.000Z",
    "itunes": {}
  },
  {
    "creator": "Bruce Schneier",
    "title": "Agentic AI’s OODA Loop Problem",
    "link": "https://www.schneier.com/blog/archives/2025/10/agentic-ais-ooda-loop-problem.html",
    "pubDate": "Mon, 20 Oct 2025 11:00:28 +0000",
    "content:encoded": "<p><b>The OODA loop&#8212;for observe, orient, decide, act&#8212;is a framework to understand decision-making in adversarial situations. We apply the same framework to artificial intelligence agents, who have to make their decisions with untrustworthy observations and orientation. To solve this problem, we need new systems of input, processing, and output integrity.</b></p>\n<p>Many decades ago, U.S. Air Force Colonel John Boyd introduced the concept of the &#8220;OODA loop,&#8221; for Observe, Orient, Decide, and Act. These are the four steps of real-time continuous decision-making. Boyd developed it for fighter pilots, but it&#8217;s long been applied in artificial intelligence (AI) and robotics. An AI agent, like a pilot, executes the loop over and over, accomplishing its goals iteratively within an ever-changing environment. This is Anthropic&#8217;s definition: &#8220;Agents are models using tools in a loop.&#8221;<sup><a id=\"ref1back\" href=\"#ref1\">1</a></sup></p>\n<h3>OODA Loops for Agentic AI</h3>\n<p>Traditional OODA analysis assumes trusted inputs and outputs, in the same way that classical AI assumed trusted sensors, controlled environments, and physical boundaries. This no longer holds true. AI agents don&#8217;t just execute OODA loops; they embed untrusted actors within them. Web-enabled large language models (LLMs) can query adversary-controlled sources mid-loop. Systems that allow AI to use large corpora of content, such as retrieval-augmented generation (<a href=\"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\">https://en.wikipedia.org/wiki/Retrieval-augmented_generation</a>), can ingest poisoned documents. Tool-calling application programming interfaces can execute untrusted code. Modern AI sensors can encompass the entire Internet; their environments are inherently adversarial. That means that fixing AI hallucination is insufficient because even if the AI accurately interprets its inputs and produces corresponding output, it can be fully corrupt.</p>\n<p>In 2022, Simon Willison identified a new class of attacks against AI systems: &#8220;prompt injection.&#8221;<sup><a id=\"ref2back\" href=\"#ref2\">2</a></sup> Prompt injection is possible because an AI mixes untrusted inputs with trusted instructions and then confuses one for the other. Willison&#8217;s insight was that this isn&#8217;t just a filtering problem; it&#8217;s architectural. There is no privilege separation, and there is no separation between the data and control paths. The very mechanism that makes modern AI powerful&#8212;treating all inputs uniformly&#8212;is what makes it vulnerable. The security challenges we face today are structural consequences of using AI for everything.</p>\n<ol>\n<li>Insecurities can have far-reaching effects. A single poisoned piece of training data can affect millions of downstream applications. In this environment, security debt accrues like technical debt.</li>\n<li>AI security has a temporal asymmetry. The temporal disconnect between training and deployment creates unauditable vulnerabilities. Attackers can poison a model&#8217;s training data and then deploy an exploit years later. Integrity violations are frozen in the model. Models aren&#8217;t aware of previous compromises since each inference starts fresh and is equally vulnerable.</li>\n<li>AI increasingly maintains state&#8212;in the form of chat history and key-value caches. These states accumulate compromises. Every iteration is potentially malicious, and cache poisoning persists across interactions.</li>\n<li>Agents compound the risks. Pretrained OODA loops running in one or a dozen AI agents inherit all of these upstream compromises. Model Context Protocol (MCP) and similar systems that allow AI to use tools create their own vulnerabilities that interact with each other. Each tool has its own OODA loop, which nests, interleaves, and races. Tool descriptions become injection vectors. Models can&#8217;t verify tool semantics, only syntax. &#8220;Submit SQL query&#8221; might mean &#8220;exfiltrate database&#8221; because an agent can be corrupted in prompts, training data, or tool definitions to do what the attacker wants. The abstraction layer itself can be adversarial.</li>\n</ol>\n<p>For example, an attacker might want AI agents to leak all the secret keys that the AI knows to the attacker, who might have a collector running in bulletproof hosting in a poorly regulated jurisdiction. They could plant coded instructions in easily scraped web content, waiting for the next AI training set to include it. Once that happens, they can activate the behavior through the front door: tricking AI agents (think a lowly chatbot or an analytics engine or a coding bot or anything in between) that are increasingly taking their own actions, in an OODA loop, using untrustworthy input from a third-party user. This compromise persists in the conversation history and cached responses, spreading to multiple future interactions and even to other AI agents. All this requires us to reconsider risks to the agentic AI OODA loop, from top to bottom.</p>\n<ul>\n<li><em>Observe:</em> The risks include adversarial examples, prompt injection, and sensor spoofing. A sticker fools computer vision, a string fools an LLM. The observation layer lacks authentication and integrity.</li>\n<li><em>Orient:</em> The risks include training data poisoning, context manipulation, and semantic backdoors. The model&#8217;s worldview&#8212;its orientation&#8212;can be influenced by attackers months before deployment. Encoded behavior activates on trigger phrases.</li>\n<li><em>Decide:</em> The risks include logic corruption via fine-tuning attacks, reward hacking, and objective misalignment. The decision process itself becomes the payload. Models can be manipulated to trust malicious sources preferentially.</li>\n<li><em>Act:</em> The risks include output manipulation, tool confusion, and action hijacking. MCP and similar protocols multiply attack surfaces. Each tool call trusts prior stages implicitly.</li>\n</ul>\n<p>AI gives the old phrase &#8220;inside your adversary&#8217;s OODA loop&#8221; new meaning. For Boyd&#8217;s fighter pilots, it meant that you were operating faster than your adversary, able to act on current data while they were still on the previous iteration. With agentic AI, adversaries aren&#8217;t just metaphorically inside; they&#8217;re literally providing the observations and manipulating the output. We want adversaries inside our loop because that&#8217;s where the data are. AI&#8217;s OODA loops must observe untrusted sources to be useful. The competitive advantage, accessing web-scale information, is identical to the attack surface. The speed of your OODA loop is irrelevant when the adversary controls your sensors and actuators.</p>\n<p>Worse, speed can itself be a vulnerability. The faster the loop, the less time for verification. Millisecond decisions result in millisecond compromises.</p>\n<h3>The Source of the Problem</h3>\n<p>The fundamental problem is that AI must compress reality into model-legible forms. In this setting, adversaries can exploit the compression. They don&#8217;t have to attack the territory; they can attack the map. Models lack local contextual knowledge. They process symbols, not meaning. A human sees a suspicious URL; an AI sees valid syntax. And that semantic gap becomes a security gap.</p>\n<p>Prompt injection might be unsolvable in today&#8217;s LLMs. LLMs process token sequences, but no mechanism exists to mark token privileges. Every solution proposed introduces new injection vectors: Delimiter? Attackers include delimiters. Instruction hierarchy? Attackers claim priority. Separate models? Double the attack surface. Security requires boundaries, but LLMs dissolve boundaries. More generally, existing mechanisms to improve models won&#8217;t help protect against attack. Fine-tuning preserves backdoors. Reinforcement learning with human feedback adds human preferences without removing model biases. Each training phase compounds prior compromises.</p>\n<p>This is Ken Thompson&#8217;s &#8220;trusting trust&#8221; attack all over again.<sup><a id=\"ref3back\" href=\"#ref3\">3</a></sup> Poisoned states generate poisoned outputs, which poison future states. Try to summarize the conversation history? The summary includes the injection. Clear the cache to remove the poison? Lose all context. Keep the cache for continuity? Keep the contamination. Stateful systems can&#8217;t forget attacks, and so memory becomes a liability. Adversaries can craft inputs that corrupt future outputs.</p>\n<p>This is the agentic AI security trilemma. Fast, smart, secure; pick any two. Fast and smart&#8212;you can&#8217;t verify your inputs. Smart and secure&#8212;you check everything, slowly, because AI itself can&#8217;t be used for this. Secure and fast&#8212;you&#8217;re stuck with models with intentionally limited capabilities.</p>\n<p>This trilemma isn&#8217;t unique to AI. Some autoimmune disorders are examples of molecular mimicry&#8212;when biological recognition systems fail to distinguish self from nonself. The mechanism designed for protection becomes the pathology as T cells attack healthy tissue or fail to attack pathogens and bad cells. AI exhibits the same kind of recognition failure. No digital immunological markers separate trusted instructions from hostile input. The model&#8217;s core capability, following instructions in natural language, is inseparable from its vulnerability. Or like oncogenes, the normal function and the malignant behavior share identical machinery.</p>\n<p>Prompt injection is semantic mimicry: adversarial instructions that resemble legitimate prompts, which trigger self-compromise. The immune system can&#8217;t add better recognition without rejecting legitimate cells. AI can&#8217;t filter malicious prompts without rejecting legitimate instructions. Immune systems can&#8217;t verify their own recognition mechanisms, and AI systems can&#8217;t verify their own integrity because the verification system uses the same corrupted mechanisms.</p>\n<p>In security, we often assume that foreign/hostile code looks different from legitimate instructions, and we use signatures, patterns, and statistical anomaly detection to detect it. But getting inside someone’s AI OODA loop uses the system&#8217;s native language. The attack is indistinguishable from normal operation because it is normal operation. The vulnerability isn&#8217;t a defect&#8212;it&#8217;s the feature working correctly.</p>\n<h3>Where to Go Next?</h3>\n<p>The shift to an AI-saturated world has been dizzying. Seemingly overnight, we have AI in every technology product, with promises of even more&#8212;and agents as well. So where does that leave us with respect to security?</p>\n<p>Physical constraints protected Boyd&#8217;s fighter pilots. Radar returns couldn&#8217;t lie about physics; fooling them, through stealth or jamming, constituted some of the most successful attacks against such systems that are still in use today. Observations were authenticated by their presence. Tampering meant physical access. But semantic observations have no physics. When every AI observation is potentially corrupted, integrity violations span the stack. Text can claim anything, and images can show impossibilities. In training, we face poisoned datasets and backdoored models. In inference, we face adversarial inputs and prompt injection. During operation, we face a contaminated context and persistent compromise. We need semantic integrity: verifying not just data but interpretation, not just content but context, not just information but understanding. We can add checksums, signatures, and audit logs. But how do you checksum a thought? How do you sign semantics? How do you audit attention?</p>\n<p>Computer security has evolved over the decades. We addressed availability despite failures through replication and decentralization. We addressed confidentiality despite breaches using authenticated encryption. Now we need to address integrity despite corruption.<sup><a id=\"ref4back\" href=\"#ref4\">4</a></sup></p>\n<p>Trustworthy AI agents require integrity because we can&#8217;t build reliable systems on unreliable foundations. The question isn&#8217;t whether we can add integrity to AI but whether the architecture permits integrity at all.</p>\n<p>AI OODA loops and integrity aren&#8217;t fundamentally opposed, but today&#8217;s AI agents observe the Internet, orient via statistics, decide probabilistically, and act without verification. We built a system that trusts everything, and now we hope for a semantic firewall to keep it safe. The adversary isn&#8217;t inside the loop by accident; it&#8217;s there by architecture. Web-scale AI means web-scale integrity failure. Every capability corrupts.</p>\n<p style=\"padding-top: 1em;\">Integrity isn&#8217;t a feature you add; it&#8217;s an architecture you choose. So far, we have built AI systems where &#8220;fast&#8221; and &#8220;smart&#8221; preclude &#8220;secure.&#8221; We optimized for capability over verification, for accessing web-scale data over ensuring trust. AI agents will be even more powerful&#8212;and increasingly autonomous. And without integrity, they will also be dangerous.</p>\n<h4>References</h4>\n<p><a id=\"ref1\" href=\"#ref1back\">1</a>. S. Willison, <cite>Simon Willison&#8217;s Weblog</cite>, May 22, 2025. [Online]. Available: <a href=\"https://simonwillison.net/2025/May/22/tools-in-a-loop/\">https://simonwillison.net/2025/May/22/tools-in-a-loop/</a></p>\n<p><a id=\"ref2\" href=\"#ref2back\">2</a>. S. Willison, &#8220;Prompt injection attacks against GPT-3,&#8221; <cite>Simon Willison&#8217;s Weblog</cite>, Sep. 12, 2022. [Online]. Available: <a href=\"https://simonwillison.net/2022/Sep/12/prompt-injection/\">https://simonwillison.net/2022/Sep/12/prompt-injection/</a></p>\n<p><a id=\"ref3\" href=\"#ref3back\">3</a>. K. Thompson, &#8220;Reflections on trusting trust,&#8221; <cite>Commun. ACM</cite>, vol. 27, no. 8, Aug. 1984. [Online]. Available: <a href=\"https://www.cs.cmu.edu/%7Erdriley/487/papers/Thompson_1984_ReflectionsonTrustingTrust.pdf\">https://www.cs.cmu.edu/~rdriley/487/papers/Thompson_1984_ReflectionsonTrustingTrust.pdf</a></p>\n<p><a id=\"ref4\" href=\"#ref4back\">4</a>. B. Schneier, &#8220;The age of integrity,&#8221; <cite>IEEE Security &amp; Privacy</cite>, vol. 23, no. 3, p. 96, May/Jun. 2025. [Online]. Available: <a href=\"https://www.computer.org/csdl/magazine/sp/2025/03/11038984/27COaJtjDOM\">https://www.computer.org/csdl/magazine/sp/2025/03/11038984/27COaJtjDOM</a></p>\n<p><em>This essay was written with Barath Raghavan, and originally appeared in <a href=\"https://www.computer.org/csdl/magazine/sp/5555/01/11194053/2aB2Rf5nZ0k\">IEEE Security &amp; Privacy</a>.</em></p>\n",
    "content:encodedSnippet": "The OODA loop—for observe, orient, decide, act—is a framework to understand decision-making in adversarial situations. We apply the same framework to artificial intelligence agents, who have to make their decisions with untrustworthy observations and orientation. To solve this problem, we need new systems of input, processing, and output integrity.\nMany decades ago, U.S. Air Force Colonel John Boyd introduced the concept of the “OODA loop,” for Observe, Orient, Decide, and Act. These are the four steps of real-time continuous decision-making. Boyd developed it for fighter pilots, but it’s long been applied in artificial intelligence (AI) and robotics. An AI agent, like a pilot, executes the loop over and over, accomplishing its goals iteratively within an ever-changing environment. This is Anthropic’s definition: “Agents are models using tools in a loop.”1\nOODA Loops for Agentic AI\nTraditional OODA analysis assumes trusted inputs and outputs, in the same way that classical AI assumed trusted sensors, controlled environments, and physical boundaries. This no longer holds true. AI agents don’t just execute OODA loops; they embed untrusted actors within them. Web-enabled large language models (LLMs) can query adversary-controlled sources mid-loop. Systems that allow AI to use large corpora of content, such as retrieval-augmented generation (https://en.wikipedia.org/wiki/Retrieval-augmented_generation), can ingest poisoned documents. Tool-calling application programming interfaces can execute untrusted code. Modern AI sensors can encompass the entire Internet; their environments are inherently adversarial. That means that fixing AI hallucination is insufficient because even if the AI accurately interprets its inputs and produces corresponding output, it can be fully corrupt.\nIn 2022, Simon Willison identified a new class of attacks against AI systems: “prompt injection.”2 Prompt injection is possible because an AI mixes untrusted inputs with trusted instructions and then confuses one for the other. Willison’s insight was that this isn’t just a filtering problem; it’s architectural. There is no privilege separation, and there is no separation between the data and control paths. The very mechanism that makes modern AI powerful—treating all inputs uniformly—is what makes it vulnerable. The security challenges we face today are structural consequences of using AI for everything.\nInsecurities can have far-reaching effects. A single poisoned piece of training data can affect millions of downstream applications. In this environment, security debt accrues like technical debt.\nAI security has a temporal asymmetry. The temporal disconnect between training and deployment creates unauditable vulnerabilities. Attackers can poison a model’s training data and then deploy an exploit years later. Integrity violations are frozen in the model. Models aren’t aware of previous compromises since each inference starts fresh and is equally vulnerable.\nAI increasingly maintains state—in the form of chat history and key-value caches. These states accumulate compromises. Every iteration is potentially malicious, and cache poisoning persists across interactions.\nAgents compound the risks. Pretrained OODA loops running in one or a dozen AI agents inherit all of these upstream compromises. Model Context Protocol (MCP) and similar systems that allow AI to use tools create their own vulnerabilities that interact with each other. Each tool has its own OODA loop, which nests, interleaves, and races. Tool descriptions become injection vectors. Models can’t verify tool semantics, only syntax. “Submit SQL query” might mean “exfiltrate database” because an agent can be corrupted in prompts, training data, or tool definitions to do what the attacker wants. The abstraction layer itself can be adversarial.\nFor example, an attacker might want AI agents to leak all the secret keys that the AI knows to the attacker, who might have a collector running in bulletproof hosting in a poorly regulated jurisdiction. They could plant coded instructions in easily scraped web content, waiting for the next AI training set to include it. Once that happens, they can activate the behavior through the front door: tricking AI agents (think a lowly chatbot or an analytics engine or a coding bot or anything in between) that are increasingly taking their own actions, in an OODA loop, using untrustworthy input from a third-party user. This compromise persists in the conversation history and cached responses, spreading to multiple future interactions and even to other AI agents. All this requires us to reconsider risks to the agentic AI OODA loop, from top to bottom.\nObserve: The risks include adversarial examples, prompt injection, and sensor spoofing. A sticker fools computer vision, a string fools an LLM. The observation layer lacks authentication and integrity.\nOrient: The risks include training data poisoning, context manipulation, and semantic backdoors. The model’s worldview—its orientation—can be influenced by attackers months before deployment. Encoded behavior activates on trigger phrases.\nDecide: The risks include logic corruption via fine-tuning attacks, reward hacking, and objective misalignment. The decision process itself becomes the payload. Models can be manipulated to trust malicious sources preferentially.\nAct: The risks include output manipulation, tool confusion, and action hijacking. MCP and similar protocols multiply attack surfaces. Each tool call trusts prior stages implicitly.\nAI gives the old phrase “inside your adversary’s OODA loop” new meaning. For Boyd’s fighter pilots, it meant that you were operating faster than your adversary, able to act on current data while they were still on the previous iteration. With agentic AI, adversaries aren’t just metaphorically inside; they’re literally providing the observations and manipulating the output. We want adversaries inside our loop because that’s where the data are. AI’s OODA loops must observe untrusted sources to be useful. The competitive advantage, accessing web-scale information, is identical to the attack surface. The speed of your OODA loop is irrelevant when the adversary controls your sensors and actuators.\nWorse, speed can itself be a vulnerability. The faster the loop, the less time for verification. Millisecond decisions result in millisecond compromises.\nThe Source of the Problem\nThe fundamental problem is that AI must compress reality into model-legible forms. In this setting, adversaries can exploit the compression. They don’t have to attack the territory; they can attack the map. Models lack local contextual knowledge. They process symbols, not meaning. A human sees a suspicious URL; an AI sees valid syntax. And that semantic gap becomes a security gap.\nPrompt injection might be unsolvable in today’s LLMs. LLMs process token sequences, but no mechanism exists to mark token privileges. Every solution proposed introduces new injection vectors: Delimiter? Attackers include delimiters. Instruction hierarchy? Attackers claim priority. Separate models? Double the attack surface. Security requires boundaries, but LLMs dissolve boundaries. More generally, existing mechanisms to improve models won’t help protect against attack. Fine-tuning preserves backdoors. Reinforcement learning with human feedback adds human preferences without removing model biases. Each training phase compounds prior compromises.\nThis is Ken Thompson’s “trusting trust” attack all over again.3 Poisoned states generate poisoned outputs, which poison future states. Try to summarize the conversation history? The summary includes the injection. Clear the cache to remove the poison? Lose all context. Keep the cache for continuity? Keep the contamination. Stateful systems can’t forget attacks, and so memory becomes a liability. Adversaries can craft inputs that corrupt future outputs.\nThis is the agentic AI security trilemma. Fast, smart, secure; pick any two. Fast and smart—you can’t verify your inputs. Smart and secure—you check everything, slowly, because AI itself can’t be used for this. Secure and fast—you’re stuck with models with intentionally limited capabilities.\nThis trilemma isn’t unique to AI. Some autoimmune disorders are examples of molecular mimicry—when biological recognition systems fail to distinguish self from nonself. The mechanism designed for protection becomes the pathology as T cells attack healthy tissue or fail to attack pathogens and bad cells. AI exhibits the same kind of recognition failure. No digital immunological markers separate trusted instructions from hostile input. The model’s core capability, following instructions in natural language, is inseparable from its vulnerability. Or like oncogenes, the normal function and the malignant behavior share identical machinery.\nPrompt injection is semantic mimicry: adversarial instructions that resemble legitimate prompts, which trigger self-compromise. The immune system can’t add better recognition without rejecting legitimate cells. AI can’t filter malicious prompts without rejecting legitimate instructions. Immune systems can’t verify their own recognition mechanisms, and AI systems can’t verify their own integrity because the verification system uses the same corrupted mechanisms.\nIn security, we often assume that foreign/hostile code looks different from legitimate instructions, and we use signatures, patterns, and statistical anomaly detection to detect it. But getting inside someone’s AI OODA loop uses the system’s native language. The attack is indistinguishable from normal operation because it is normal operation. The vulnerability isn’t a defect—it’s the feature working correctly.\nWhere to Go Next?\nThe shift to an AI-saturated world has been dizzying. Seemingly overnight, we have AI in every technology product, with promises of even more—and agents as well. So where does that leave us with respect to security?\nPhysical constraints protected Boyd’s fighter pilots. Radar returns couldn’t lie about physics; fooling them, through stealth or jamming, constituted some of the most successful attacks against such systems that are still in use today. Observations were authenticated by their presence. Tampering meant physical access. But semantic observations have no physics. When every AI observation is potentially corrupted, integrity violations span the stack. Text can claim anything, and images can show impossibilities. In training, we face poisoned datasets and backdoored models. In inference, we face adversarial inputs and prompt injection. During operation, we face a contaminated context and persistent compromise. We need semantic integrity: verifying not just data but interpretation, not just content but context, not just information but understanding. We can add checksums, signatures, and audit logs. But how do you checksum a thought? How do you sign semantics? How do you audit attention?\nComputer security has evolved over the decades. We addressed availability despite failures through replication and decentralization. We addressed confidentiality despite breaches using authenticated encryption. Now we need to address integrity despite corruption.4\nTrustworthy AI agents require integrity because we can’t build reliable systems on unreliable foundations. The question isn’t whether we can add integrity to AI but whether the architecture permits integrity at all.\nAI OODA loops and integrity aren’t fundamentally opposed, but today’s AI agents observe the Internet, orient via statistics, decide probabilistically, and act without verification. We built a system that trusts everything, and now we hope for a semantic firewall to keep it safe. The adversary isn’t inside the loop by accident; it’s there by architecture. Web-scale AI means web-scale integrity failure. Every capability corrupts.\nIntegrity isn’t a feature you add; it’s an architecture you choose. So far, we have built AI systems where “fast” and “smart” preclude “secure.” We optimized for capability over verification, for accessing web-scale data over ensuring trust. AI agents will be even more powerful—and increasingly autonomous. And without integrity, they will also be dangerous.\nReferences\n1. S. Willison, Simon Willison’s Weblog, May 22, 2025. [Online]. Available: https://simonwillison.net/2025/May/22/tools-in-a-loop/\n2. S. Willison, “Prompt injection attacks against GPT-3,” Simon Willison’s Weblog, Sep. 12, 2022. [Online]. Available: https://simonwillison.net/2022/Sep/12/prompt-injection/\n3. K. Thompson, “Reflections on trusting trust,” Commun. ACM, vol. 27, no. 8, Aug. 1984. [Online]. Available: https://www.cs.cmu.edu/~rdriley/487/papers/Thompson_1984_ReflectionsonTrustingTrust.pdf\n4. B. Schneier, “The age of integrity,” IEEE Security & Privacy, vol. 23, no. 3, p. 96, May/Jun. 2025. [Online]. Available: https://www.computer.org/csdl/magazine/sp/2025/03/11038984/27COaJtjDOM\nThis essay was written with Barath Raghavan, and originally appeared in IEEE Security & Privacy.",
    "dc:creator": "Bruce Schneier",
    "comments": "https://www.schneier.com/blog/archives/2025/10/agentic-ais-ooda-loop-problem.html#comments",
    "content": "<p><b>The OODA loop&#8212;for observe, orient, decide, act&#8212;is a framework to understand decision-making in adversarial situations. We apply the same framework to artificial intelligence agents, who have to make their decisions with untrustworthy observations and orientation. To solve this problem, we need new systems of input, processing, and output integrity.</b></p>\n<p>Many decades ago, U.S. Air Force Colonel John Boyd introduced the concept of the &#8220;OODA loop,&#8221; for Observe, Orient, Decide, and Act. These are the four steps of real-time continuous decision-making. Boyd developed it for fighter pilots, but it&#8217;s long been applied in artificial intelligence (AI) and robotics. An AI agent, like a pilot, executes the loop over and over, accomplishing its goals iteratively within an ever-changing environment. This is Anthropic&#8217;s definition: &#8220;Agents are models using tools in a loop.&#8221;...</p>",
    "contentSnippet": "The OODA loop—for observe, orient, decide, act—is a framework to understand decision-making in adversarial situations. We apply the same framework to artificial intelligence agents, who have to make their decisions with untrustworthy observations and orientation. To solve this problem, we need new systems of input, processing, and output integrity.\nMany decades ago, U.S. Air Force Colonel John Boyd introduced the concept of the “OODA loop,” for Observe, Orient, Decide, and Act. These are the four steps of real-time continuous decision-making. Boyd developed it for fighter pilots, but it’s long been applied in artificial intelligence (AI) and robotics. An AI agent, like a pilot, executes the loop over and over, accomplishing its goals iteratively within an ever-changing environment. This is Anthropic’s definition: “Agents are models using tools in a loop.”...",
    "guid": "https://www.schneier.com/?p=71030",
    "categories": [
      "Uncategorized",
      "AI",
      "integrity",
      "LLM"
    ],
    "isoDate": "2025-10-20T11:00:28.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "131 Chrome Extensions Caught Hijacking WhatsApp Web for Massive Spam Campaign",
    "link": "https://thehackernews.com/2025/10/131-chrome-extensions-caught-hijacking.html",
    "pubDate": "Mon, 20 Oct 2025 16:17:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXakHQ6GIVNqxANLePApKp1lgLfEZJrBJhrZhuDLjh3HVt1QYS9LEa1u9GPp7K_-dD6CLo2sHxW8tbZjUEnq1XcFsdmyomem3hctdRwjIsfi9r4CSWaYrM2zdmeVcE_E8N5RRS6JyVl1v282R3lVGo6nAahd7S1SGG7mWExLdF_VZn8GAc1ahORCjZAg4Q/s1600/whatsapp-web.jpg"
    },
    "content": "Cybersecurity researchers have uncovered a coordinated campaign that leveraged 131 rebranded clones of a WhatsApp Web automation extension for Google Chrome to spam Brazilian users at scale.\nThe 131 spamware extensions share the same codebase, design patterns, and infrastructure, according to supply chain security company Socket. The browser add-ons collectively have about 20,905 active users.\n\"",
    "contentSnippet": "Cybersecurity researchers have uncovered a coordinated campaign that leveraged 131 rebranded clones of a WhatsApp Web automation extension for Google Chrome to spam Brazilian users at scale.\nThe 131 spamware extensions share the same codebase, design patterns, and infrastructure, according to supply chain security company Socket. The browser add-ons collectively have about 20,905 active users.\n\"",
    "guid": "https://thehackernews.com/2025/10/131-chrome-extensions-caught-hijacking.html",
    "isoDate": "2025-10-20T10:47:00.000Z",
    "itunes": {}
  },
  {
    "creator": "Matthew Gault",
    "title": "Anthropic Has a Plan to Keep Its AI From Building a Nuclear Weapon. Will It Work?",
    "link": "https://www.wired.com/story/anthropic-has-a-plan-to-keep-its-ai-from-building-a-nuclear-weapon-will-it-work/",
    "pubDate": "Mon, 20 Oct 2025 09:00:00 +0000",
    "dc:creator": "Matthew Gault",
    "content": "Anthropic partnered with the US government to create a filter meant to block Claude from helping someone build a nuke. Experts are divided on whether its a necessary protection—or a protection at all.",
    "contentSnippet": "Anthropic partnered with the US government to create a filter meant to block Claude from helping someone build a nuke. Experts are divided on whether its a necessary protection—or a protection at all.",
    "guid": "68f243da4dfa654488b864ee",
    "categories": [
      "Security",
      "Security / National Security",
      "Security / Security News",
      "Business / Artificial Intelligence"
    ],
    "isoDate": "2025-10-20T09:00:00.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "MSS Claims NSA Used 42 Cyber Tools in Multi-Stage Attack on Beijing Time Systems",
    "link": "https://thehackernews.com/2025/10/mss-claims-nsa-used-42-cyber-tools-in.html",
    "pubDate": "Mon, 20 Oct 2025 11:02:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnbyTAesJjrKdf7Xi5TAVIcEz8gljv1a1ZyP5GQgZ9EKsEVDL2uYNsQ4EjmVAV0dRsqc4kLV1tgOO8bPe5JlM_jq54R0YD1VrJN6XJmlQ7SfUJN-T4bzvNFtYyX5-OziaQgAdnvXTZ2VTk1RkwynpeYNmEg-HCkmDdD7CwO7ygXX8VIcrvI4VG5-URAiqf/s1600/chinese.jpg"
    },
    "content": "China on Sunday accused the U.S. National Security Agency (NSA) of carrying out a \"premeditated\" cyber attack targeting the National Time Service Center (NTSC), as it described the U.S. as a \"hacker empire\" and the \"greatest source of chaos in cyberspace.\"\nThe Ministry of State Security (MSS), in a WeChat post, said it uncovered \"irrefutable evidence\" of the agency's involvement in the intrusion",
    "contentSnippet": "China on Sunday accused the U.S. National Security Agency (NSA) of carrying out a \"premeditated\" cyber attack targeting the National Time Service Center (NTSC), as it described the U.S. as a \"hacker empire\" and the \"greatest source of chaos in cyberspace.\"\nThe Ministry of State Security (MSS), in a WeChat post, said it uncovered \"irrefutable evidence\" of the agency's involvement in the intrusion",
    "guid": "https://thehackernews.com/2025/10/mss-claims-nsa-used-42-cyber-tools-in.html",
    "isoDate": "2025-10-20T05:32:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Europol Dismantles SIM Farm Network Powering 49 Million Fake Accounts Worldwide",
    "link": "https://thehackernews.com/2025/10/europol-dismantles-sim-farm-network.html",
    "pubDate": "Sun, 19 Oct 2025 11:43:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_I5eP3IIKFpmiSm1phVEKFUbg1o3JZ9DH_6R8w2XAVD5xxHEDx74c28SBq6sRo5d18K4Msnx8iHphoSeUot29zcoUH-6OMwLsjrtDL10S44x7uVIr2d8hh-mez0REVSA6C3qvdVGhlQWcMrIb9uBrX7cKuUvAO3EwHvuW3oSjqRNm9u8diTsWPi3lFMZw/s1600/sim-farm.jpg"
    },
    "content": "Europol on Friday announced the disruption of a sophisticated cybercrime-as-a-service (CaaS) platform that operated a SIM farm and enabled its customers to carry out a broad spectrum of crimes ranging from phishing to investment fraud.\nThe coordinated law enforcement effort, dubbed Operation SIMCARTEL, saw 26 searches carried out, resulting in the arrest of seven suspects and the seizure of",
    "contentSnippet": "Europol on Friday announced the disruption of a sophisticated cybercrime-as-a-service (CaaS) platform that operated a SIM farm and enabled its customers to carry out a broad spectrum of crimes ranging from phishing to investment fraud.\nThe coordinated law enforcement effort, dubbed Operation SIMCARTEL, saw 26 searches carried out, resulting in the arrest of seven suspects and the seizure of",
    "guid": "https://thehackernews.com/2025/10/europol-dismantles-sim-farm-network.html",
    "isoDate": "2025-10-19T06:13:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "New .NET CAPI Backdoor Targets Russian Auto and E-Commerce Firms via Phishing ZIPs",
    "link": "https://thehackernews.com/2025/10/new-net-capi-backdoor-targets-russian.html",
    "pubDate": "Sat, 18 Oct 2025 17:11:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjONqdUuuzUnSmVHZhMcb0nfxYoLC_St8teKiK0-qlwf3oFZ-rYbcUdnFHk7Mgy1EGoVKTbyQxzKaba5HD7dK6BinVxUGsBQUmUUpgzw5mx6ovilIp8qze4uC_8RTbjv7enWakXLBUKJZwdKcHhUS6qVJ-ke9FXhzkBw951sZZYbBRD_QhTQiH4Pas9oRF7/s1600/russian-malware.jpg"
    },
    "content": "Cybersecurity researchers have shed light on a new campaign that has likely targeted the Russian automobile and e-commerce sectors with a previously undocumented .NET malware dubbed CAPI Backdoor.\nAccording to Seqrite Labs, the attack chain involves distributing phishing emails containing a ZIP archive as a way to trigger the infection. The cybersecurity company's analysis is based on the ZIP",
    "contentSnippet": "Cybersecurity researchers have shed light on a new campaign that has likely targeted the Russian automobile and e-commerce sectors with a previously undocumented .NET malware dubbed CAPI Backdoor.\nAccording to Seqrite Labs, the attack chain involves distributing phishing emails containing a ZIP archive as a way to trigger the infection. The cybersecurity company's analysis is based on the ZIP",
    "guid": "https://thehackernews.com/2025/10/new-net-capi-backdoor-targets-russian.html",
    "isoDate": "2025-10-18T11:41:00.000Z",
    "itunes": {}
  },
  {
    "creator": "Andy Greenberg, Matt Burgess",
    "title": "Hackers Dox ICE, DHS, DOJ, and FBI Officials",
    "link": "https://www.wired.com/story/security-news-this-week-hackers-dox-ice-dhs-doj-and-fbi-officials/",
    "pubDate": "Sat, 18 Oct 2025 10:30:00 +0000",
    "dc:creator": "Andy Greenberg, Matt Burgess",
    "content": "Plus: A secret FBI anti-ransomware task force gets exposed, the mystery of the CIA’s Kryptos sculpture is finally solved, North Koreans busted hiding malware in the Ethereum blockchain, and more.",
    "contentSnippet": "Plus: A secret FBI anti-ransomware task force gets exposed, the mystery of the CIA’s Kryptos sculpture is finally solved, North Koreans busted hiding malware in the Ethereum blockchain, and more.",
    "guid": "68f252e51ad7b00832d9075a",
    "categories": [
      "Security",
      "Security / Cyberattacks and Hacks",
      "Security / National Security",
      "Security / Privacy",
      "Security / Security News"
    ],
    "isoDate": "2025-10-18T10:30:00.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Silver Fox Expands Winos 4.0 Attacks to Japan and Malaysia via HoldingHands RAT",
    "link": "https://thehackernews.com/2025/10/silver-fox-expands-winos-40-attacks-to.html",
    "pubDate": "Sat, 18 Oct 2025 12:21:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibiSdv0av_jBT1NvQWK4prfMmJUZXETQesWOp4pATha0asiCgMVGZ6_A_56RBxOX_gPNijoVz4QQN955iyx-KRfAj8k-F48zjIZE5gFopMN_rMyacDt1UFR7g97Hbt7SUnldANyT1T3upYifeVyFQnOHbHplYu3Vw_HtsJURvqfqqe38TU80hgIloRppiL/s1600/pdf-malware.jpg"
    },
    "content": "The threat actors behind a malware family known as Winos 4.0 (aka ValleyRAT) have expanded their targeting footprint from China and Taiwan to target Japan and Malaysia with another remote access trojan (RAT) tracked as HoldingHands RAT (aka Gh0stBins).\n\"The campaign relied on phishing emails with PDFs that contained embedded malicious links,\" Pei Han Liao, researcher with Fortinet's FortiGuard",
    "contentSnippet": "The threat actors behind a malware family known as Winos 4.0 (aka ValleyRAT) have expanded their targeting footprint from China and Taiwan to target Japan and Malaysia with another remote access trojan (RAT) tracked as HoldingHands RAT (aka Gh0stBins).\n\"The campaign relied on phishing emails with PDFs that contained embedded malicious links,\" Pei Han Liao, researcher with Fortinet's FortiGuard",
    "guid": "https://thehackernews.com/2025/10/silver-fox-expands-winos-40-attacks-to.html",
    "isoDate": "2025-10-18T06:51:00.000Z",
    "itunes": {}
  },
  {
    "creator": "Bruce Schneier",
    "title": "Friday Squid Blogging: Squid Inks Philippines Fisherman",
    "link": "https://www.schneier.com/blog/archives/2025/10/friday-squid-blogging-squid-inks-philippines-fisherman.html",
    "pubDate": "Fri, 17 Oct 2025 21:02:47 +0000",
    "content:encoded": "<p>Good <a href=\"https://www.foxweather.com/earth-space/philippines-fisherman-face-full-squid-ink\">video</a>.</p>\n<p>As usual, you can also use this squid post to talk about the security stories in the news that I haven&#8217;t covered.</p>\n<p><a href=\"https://www.schneier.com/blog/archives/2024/06/new-blog-moderation-policy.html\">Blog moderation policy.</a></p>\n",
    "content:encodedSnippet": "Good video.\nAs usual, you can also use this squid post to talk about the security stories in the news that I haven’t covered.\nBlog moderation policy.",
    "dc:creator": "Bruce Schneier",
    "comments": "https://www.schneier.com/blog/archives/2025/10/friday-squid-blogging-squid-inks-philippines-fisherman.html#comments",
    "content": "<p>Good <a href=\"https://www.foxweather.com/earth-space/philippines-fisherman-face-full-squid-ink\">video</a>.</p>\n<p>As usual, you can also use this squid post to talk about the security stories in the news that I haven&#8217;t covered.</p>\n<p><a href=\"https://www.schneier.com/blog/archives/2024/06/new-blog-moderation-policy.html\">Blog moderation policy.</a></p>\n",
    "contentSnippet": "Good video.\nAs usual, you can also use this squid post to talk about the security stories in the news that I haven’t covered.\nBlog moderation policy.",
    "guid": "https://www.schneier.com/?p=71027",
    "categories": [
      "Uncategorized",
      "squid",
      "video"
    ],
    "isoDate": "2025-10-17T21:02:47.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "North Korean Hackers Combine BeaverTail and OtterCookie into Advanced JS Malware",
    "link": "https://thehackernews.com/2025/10/north-korean-hackers-combine-beavertail.html",
    "pubDate": "Fri, 17 Oct 2025 19:03:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEii_qL8ioAKIQ7YOM6kLCcN9eyqE6pS2Put-ayeBXqibcGCOaV1r-2uLa5MFvwy-VAPLR-wkjnzMr1UwbTUAZGHwa0JVewhyMpYDdrhXGNxbU1ZHeY6xlZZut5QdrlFj8CEhd6Mb6nmgVLTSZ7uytoIUIYsI-8e9xOs8XErc2ycwU4H8xgkXyF081I4g_Zy/s1600/malware-code.jpg"
    },
    "content": "The North Korean threat actor linked to the Contagious Interview campaign has been observed merging some of the functionality of two of its malware programs, indicating that the hacking group is actively refining its toolset.\nThat's according to new findings from Cisco Talos, which said recent campaigns undertaken by the hacking group have seen the functions of BeaverTail and OtterCookie coming",
    "contentSnippet": "The North Korean threat actor linked to the Contagious Interview campaign has been observed merging some of the functionality of two of its malware programs, indicating that the hacking group is actively refining its toolset.\nThat's according to new findings from Cisco Talos, which said recent campaigns undertaken by the hacking group have seen the functions of BeaverTail and OtterCookie coming",
    "guid": "https://thehackernews.com/2025/10/north-korean-hackers-combine-beavertail.html",
    "isoDate": "2025-10-17T13:33:00.000Z",
    "itunes": {}
  },
  {
    "creator": "BrianKrebs",
    "title": "Email Bombs Exploit Lax Authentication in Zendesk",
    "link": "https://krebsonsecurity.com/2025/10/email-bombs-exploit-lax-authentication-in-zendesk/",
    "pubDate": "Fri, 17 Oct 2025 11:26:27 +0000",
    "content:encoded": "<p>Cybercriminals are abusing a widespread lack of authentication in the customer service platform <strong>Zendesk</strong> to flood targeted email inboxes with menacing messages that come from hundreds of Zendesk corporate customers simultaneously.</p>\n<p>Zendesk is an automated help desk service designed to make it simple for people to contact companies for customer support issues. Earlier this week, KrebsOnSecurity started receiving thousands of ticket creation notification messages through Zendesk in rapid succession, each bearing the name of different Zendesk customers, such as <strong>CapCom</strong>, <strong>CompTIA</strong>, <strong>Discord</strong>, <strong>GMAC</strong>, <strong>NordVPN</strong>, <strong>The Washington Post</strong>, and<strong> Tinder</strong>.</p>\n<p>The abusive missives sent via Zendesk&#8217;s platform can include any subject line chosen by the abusers. In my case, the messages variously warned about a supposed law enforcement investigation involving KrebsOnSecurity.com, or else contained personal insults.</p>\n<p>Moreover, the automated messages that are sent out from this type of abuse all come from customer domain names &#8212; not from Zendesk. In the example below, replying to any of the junk customer support responses from The Washington Post&#8217;s Zendesk installation shows the reply-to address is help@washpost.com.</p>\n<div id=\"attachment_72398\" style=\"width: 759px\" class=\"wp-caption aligncenter\"><img aria-describedby=\"caption-attachment-72398\" decoding=\"async\" class=\" wp-image-72398\" src=\"https://krebsonsecurity.com/wp-content/uploads/2025/10/zendeskwapo.png\" alt=\"\" width=\"749\" height=\"362\" srcset=\"https://krebsonsecurity.com/wp-content/uploads/2025/10/zendeskwapo.png 2110w, https://krebsonsecurity.com/wp-content/uploads/2025/10/zendeskwapo-768x371.png 768w, https://krebsonsecurity.com/wp-content/uploads/2025/10/zendeskwapo-1536x743.png 1536w, https://krebsonsecurity.com/wp-content/uploads/2025/10/zendeskwapo-2048x990.png 2048w, https://krebsonsecurity.com/wp-content/uploads/2025/10/zendeskwapo-782x378.png 782w\" sizes=\"(max-width: 749px) 100vw, 749px\" /><p id=\"caption-attachment-72398\" class=\"wp-caption-text\">One of dozens of messages sent to me this week by The Washington Post.</p></div>\n<p>Notified about the mass abuse of their platform, Zendesk said the emails were ticket creation notifications from customer accounts that configured their Zendesk instance to allow anyone to submit support requests &#8212; including anonymous users.</p>\n<p>&#8220;These types of support tickets can be part of a customer’s workflow, where a prior verification is not required to allow them to engage and make use of the Support capabilities,&#8221; said <strong>Carolyn Camoens</strong>, communications director at Zendesk. &#8220;Although we recommend our customers to permit only verified users to submit tickets, some Zendesk customers prefer to use an anonymous environment to allow for tickets to be created due to various business reasons.&#8221;</p>\n<p>Camoens said requests that can be submitted in an anonymous manner can also make use of an email address of the submitter&#8217;s choice.</p>\n<p>&#8220;However, this method can also be used for spam requests to be created on behalf of third party email addresses,&#8221; Camoens said. &#8220;If an account has enabled the auto-responder trigger based on ticket creation, then this allows for the ticket notification email to be sent from our customer’s accounts to these third parties. The notification will also include the Subject added by the creator of these tickets.&#8221;</p>\n<p>Zendesk claims it uses rate limits to prevent a high volume of requests from being created at once, but those limits did not stop Zendesk customers from flooding my inbox with thousands of messages in just a few hours.</p>\n<p>&#8220;We recognize that our systems were leveraged against you in a distributed, many-against-one manner,&#8221; Camoens said. &#8220;We are actively investigating additional preventive measures. We are also advising customers experiencing this type of activity to follow our general security best practices and configure an authenticated ticket creation workflow.&#8221;</p>\n<p>In all of the cases above, the messaging abuse would not have been possible if Zendesk customers validated support request email addresses prior to sending responses. Failing to do so may make it easier for Zendesk clients to handle customer support requests, but it also allows ne&#8217;er-do-wells to sully the sender&#8217;s brand in service of disruptive and malicious email floods.</p>\n",
    "content:encodedSnippet": "Cybercriminals are abusing a widespread lack of authentication in the customer service platform Zendesk to flood targeted email inboxes with menacing messages that come from hundreds of Zendesk corporate customers simultaneously.\nZendesk is an automated help desk service designed to make it simple for people to contact companies for customer support issues. Earlier this week, KrebsOnSecurity started receiving thousands of ticket creation notification messages through Zendesk in rapid succession, each bearing the name of different Zendesk customers, such as CapCom, CompTIA, Discord, GMAC, NordVPN, The Washington Post, and Tinder.\nThe abusive missives sent via Zendesk’s platform can include any subject line chosen by the abusers. In my case, the messages variously warned about a supposed law enforcement investigation involving KrebsOnSecurity.com, or else contained personal insults.\nMoreover, the automated messages that are sent out from this type of abuse all come from customer domain names — not from Zendesk. In the example below, replying to any of the junk customer support responses from The Washington Post’s Zendesk installation shows the reply-to address is help@washpost.com.\n\nOne of dozens of messages sent to me this week by The Washington Post.\n\nNotified about the mass abuse of their platform, Zendesk said the emails were ticket creation notifications from customer accounts that configured their Zendesk instance to allow anyone to submit support requests — including anonymous users.\n“These types of support tickets can be part of a customer’s workflow, where a prior verification is not required to allow them to engage and make use of the Support capabilities,” said Carolyn Camoens, communications director at Zendesk. “Although we recommend our customers to permit only verified users to submit tickets, some Zendesk customers prefer to use an anonymous environment to allow for tickets to be created due to various business reasons.”\nCamoens said requests that can be submitted in an anonymous manner can also make use of an email address of the submitter’s choice.\n“However, this method can also be used for spam requests to be created on behalf of third party email addresses,” Camoens said. “If an account has enabled the auto-responder trigger based on ticket creation, then this allows for the ticket notification email to be sent from our customer’s accounts to these third parties. The notification will also include the Subject added by the creator of these tickets.”\nZendesk claims it uses rate limits to prevent a high volume of requests from being created at once, but those limits did not stop Zendesk customers from flooding my inbox with thousands of messages in just a few hours.\n“We recognize that our systems were leveraged against you in a distributed, many-against-one manner,” Camoens said. “We are actively investigating additional preventive measures. We are also advising customers experiencing this type of activity to follow our general security best practices and configure an authenticated ticket creation workflow.”\nIn all of the cases above, the messaging abuse would not have been possible if Zendesk customers validated support request email addresses prior to sending responses. Failing to do so may make it easier for Zendesk clients to handle customer support requests, but it also allows ne’er-do-wells to sully the sender’s brand in service of disruptive and malicious email floods.",
    "dc:creator": "BrianKrebs",
    "comments": "https://krebsonsecurity.com/2025/10/email-bombs-exploit-lax-authentication-in-zendesk/#comments",
    "content": "Cybercriminals are abusing a widespread lack of authentication in the customer service platform Zendesk to flood targeted email inboxes with menacing messages that come from hundreds of Zendesk corporate customers simultaneously.",
    "contentSnippet": "Cybercriminals are abusing a widespread lack of authentication in the customer service platform Zendesk to flood targeted email inboxes with menacing messages that come from hundreds of Zendesk corporate customers simultaneously.",
    "guid": "https://krebsonsecurity.com/?p=72392",
    "categories": [
      "A Little Sunshine",
      "Latest Warnings",
      "The Coming Storm",
      "Web Fraud 2.0",
      "CapCom",
      "Carolyn Camoens",
      "CompTIA",
      "Discord",
      "GMAC",
      "NordVPN",
      "The Washington Post",
      "Tinder",
      "Zendesk"
    ],
    "isoDate": "2025-10-17T11:26:27.000Z"
  },
  {
    "creator": "Bruce Schneier",
    "title": "A Surprising Amount of Satellite Traffic Is Unencrypted",
    "link": "https://www.schneier.com/blog/archives/2025/10/a-surprising-amount-of-satellite-traffic-is-unencrypted.html",
    "pubDate": "Fri, 17 Oct 2025 11:03:53 +0000",
    "content:encoded": "<p>Here&#8217;s the <a href=\"https://satcom.sysnet.ucsd.edu/\">summary</a>:</p>\n<blockquote><p>We pointed a commercial-off-the-shelf satellite dish at the sky and carried out the most comprehensive public study to date of geostationary satellite communication. A shockingly large amount of sensitive traffic is being broadcast unencrypted, including critical infrastructure, internal corporate and government communications, private citizens&#8217; voice calls and SMS, and consumer Internet traffic from in-flight wifi and mobile networks. This data can be passively observed by anyone with a few hundred dollars of consumer-grade hardware. There are thousands of geostationary satellite transponders globally, and data from a single transponder may be visible from an area as large as 40% of the surface of the earth.</p></blockquote>\n<p>Full <a href=\"https://satcom.sysnet.ucsd.edu/docs/dontlookup_ccs25_fullpaper.pdf\">paper</a>. News <a href=\"https://gizmodo.com/satellites-are-exposing-unprotected-cellphone-and-military-data-study-finds-2000672091\">article</a>.</p>\n",
    "content:encodedSnippet": "Here’s the summary:\nWe pointed a commercial-off-the-shelf satellite dish at the sky and carried out the most comprehensive public study to date of geostationary satellite communication. A shockingly large amount of sensitive traffic is being broadcast unencrypted, including critical infrastructure, internal corporate and government communications, private citizens’ voice calls and SMS, and consumer Internet traffic from in-flight wifi and mobile networks. This data can be passively observed by anyone with a few hundred dollars of consumer-grade hardware. There are thousands of geostationary satellite transponders globally, and data from a single transponder may be visible from an area as large as 40% of the surface of the earth.\n\nFull paper. News article.",
    "dc:creator": "Bruce Schneier",
    "comments": "https://www.schneier.com/blog/archives/2025/10/a-surprising-amount-of-satellite-traffic-is-unencrypted.html#comments",
    "content": "<p>Here&#8217;s the <a href=\"https://satcom.sysnet.ucsd.edu/\">summary</a>:</p>\n<blockquote><p>We pointed a commercial-off-the-shelf satellite dish at the sky and carried out the most comprehensive public study to date of geostationary satellite communication. A shockingly large amount of sensitive traffic is being broadcast unencrypted, including critical infrastructure, internal corporate and government communications, private citizens&#8217; voice calls and SMS, and consumer Internet traffic from in-flight wifi and mobile networks. This data can be passively observed by anyone with a few hundred dollars of consumer-grade hardware. There are thousands of geostationary satellite transponders globally, and data from a single transponder may be visible from an area as large as 40% of the surface of the earth...</p></blockquote>",
    "contentSnippet": "Here’s the summary:\nWe pointed a commercial-off-the-shelf satellite dish at the sky and carried out the most comprehensive public study to date of geostationary satellite communication. A shockingly large amount of sensitive traffic is being broadcast unencrypted, including critical infrastructure, internal corporate and government communications, private citizens’ voice calls and SMS, and consumer Internet traffic from in-flight wifi and mobile networks. This data can be passively observed by anyone with a few hundred dollars of consumer-grade hardware. There are thousands of geostationary satellite transponders globally, and data from a single transponder may be visible from an area as large as 40% of the surface of the earth...",
    "guid": "https://www.schneier.com/?p=71022",
    "categories": [
      "Uncategorized",
      "academic papers",
      "cell phones",
      "data protection",
      "eavesdropping",
      "encryption",
      "infrastructure"
    ],
    "isoDate": "2025-10-17T11:03:53.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Identity Security: Your First and Last Line of Defense",
    "link": "https://thehackernews.com/2025/10/identity-security-your-first-and-last.html",
    "pubDate": "Fri, 17 Oct 2025 16:30:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrf3SNrJtA9ogAsUh8Md4fU4yC0HCkGJbedZ6jQUivAuuAkYP0vrpjnQrjhTE0Bwkf7yEj2fkgxaP6XaAO4R8ARGp30JUatnhiOlnrJqahuczssni_ET7ZJRFaNqDCcAGwjnzs345KXNIl1UJYWIGr1PRNp-8z_9IZ_8k4YL9hKUq7nII0BO6wOA8aRSQ/s1600/is.jpg"
    },
    "content": "The danger isn’t that AI agents have bad days — it’s that they never do. They execute faithfully, even when what they’re executing is a mistake. A single misstep in logic or access can turn flawless automation into a flawless catastrophe.\nThis isn't some dystopian fantasy—it's Tuesday at the office now. We've entered a new phase where autonomous AI agents act with serious system privileges. They",
    "contentSnippet": "The danger isn’t that AI agents have bad days — it’s that they never do. They execute faithfully, even when what they’re executing is a mistake. A single misstep in logic or access can turn flawless automation into a flawless catastrophe.\nThis isn't some dystopian fantasy—it's Tuesday at the office now. We've entered a new phase where autonomous AI agents act with serious system privileges. They",
    "guid": "https://thehackernews.com/2025/10/identity-security-your-first-and-last.html",
    "isoDate": "2025-10-17T11:00:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Researchers Uncover WatchGuard VPN Bug That Could Let Attackers Take Over Devices",
    "link": "https://thehackernews.com/2025/10/researchers-uncover-watchguard-vpn-bug.html",
    "pubDate": "Fri, 17 Oct 2025 14:55:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6wXElZ2cOoqWu0KvQRLkdOMEVEy2OErFfkGWokp1gdNCJH8_MxEnR1RMopVgNuFWuAxiCiP90_YEcQe_XEKxadsliIouePVm63amvt2JV3tORO8EQmNCh2ylwY4eQgJPNWR5aoQMPWdefC1wL_HSYTS1dFnjaAQNF3sXQbkeJxY3q7tCu4j4DaPXpJBPY/s1600/exploit.jpg"
    },
    "content": "Cybersecurity researchers have disclosed details of a recently patched critical security flaw in WatchGuard Fireware that could allow unauthenticated attackers to execute arbitrary code.\nThe vulnerability, tracked as CVE-2025-9242 (CVSS score: 9.3), is described as an out-of-bounds write vulnerability affecting Fireware OS 11.10.2 up to and including 11.12.4_Update1, 12.0 up to and including",
    "contentSnippet": "Cybersecurity researchers have disclosed details of a recently patched critical security flaw in WatchGuard Fireware that could allow unauthenticated attackers to execute arbitrary code.\nThe vulnerability, tracked as CVE-2025-9242 (CVSS score: 9.3), is described as an out-of-bounds write vulnerability affecting Fireware OS 11.10.2 up to and including 11.12.4_Update1, 12.0 up to and including",
    "guid": "https://thehackernews.com/2025/10/researchers-uncover-watchguard-vpn-bug.html",
    "isoDate": "2025-10-17T09:25:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Microsoft Revokes 200 Fraudulent Certificates Used in Rhysida Ransomware Campaign",
    "link": "https://thehackernews.com/2025/10/microsoft-revokes-200-fraudulent.html",
    "pubDate": "Fri, 17 Oct 2025 11:33:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikW56LsCKHPpyfH2AIVdz2Cuy3x2z7pTgiAMRHiyYU0m-wnbxE-8zGrGaamjCX79Ypc9Hz6-JHph78BvlB56sNwYh20cI5LcVuNLrx8C9izVE7Z3o_d6KgHWpoMNENBjinWUUco_7hzc9NwxFtmC__VF0Cfud9e3q61stbWCaHG_5jDh5QEtrcSf8WsJav/s1600/ransomware-certificate.jpg"
    },
    "content": "Microsoft on Thursday disclosed that it revoked more than 200 certificates used by a threat actor it tracks as Vanilla Tempest to fraudulently sign malicious binaries in ransomware attacks.\nThe certificates were \"used in fake Teams setup files to deliver the Oyster backdoor and ultimately deploy Rhysida ransomware,\" the Microsoft Threat Intelligence team said in a post shared on X.\nThe tech",
    "contentSnippet": "Microsoft on Thursday disclosed that it revoked more than 200 certificates used by a threat actor it tracks as Vanilla Tempest to fraudulently sign malicious binaries in ransomware attacks.\nThe certificates were \"used in fake Teams setup files to deliver the Oyster backdoor and ultimately deploy Rhysida ransomware,\" the Microsoft Threat Intelligence team said in a post shared on X.\nThe tech",
    "guid": "https://thehackernews.com/2025/10/microsoft-revokes-200-fraudulent.html",
    "isoDate": "2025-10-17T06:03:00.000Z",
    "itunes": {}
  },
  {
    "creator": "Dan Goodin, Ars Technica",
    "title": "Why the F5 Hack Created an ‘Imminent Threat’ for Thousands of Networks",
    "link": "https://www.wired.com/story/f5-hack-networking-software-big-ip/",
    "pubDate": "Thu, 16 Oct 2025 20:42:29 +0000",
    "dc:creator": "Dan Goodin, Ars Technica",
    "content": "Networking software company F5 disclosed a long-term breach of its systems this week. The fallout could be severe.",
    "contentSnippet": "Networking software company F5 disclosed a long-term breach of its systems this week. The fallout could be severe.",
    "guid": "68f10c80252a49e05299b084",
    "categories": [
      "Security / Cyberattacks and Hacks"
    ],
    "isoDate": "2025-10-16T20:42:29.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "North Korean Hackers Use EtherHiding to Hide Malware Inside Blockchain Smart Contracts",
    "link": "https://thehackernews.com/2025/10/north-korean-hackers-use-etherhiding-to.html",
    "pubDate": "Thu, 16 Oct 2025 20:26:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlAkWqyP-kjx3tsDRbhwCbgbYrCfv0bhsscLPeraWfsWXD0mN158-j8SWyraPNsbVa9_Iv_lMsOFckxYCbc6jtrVSE2qvKzDpErsFoHG-kp5NiCwBuPi72zjnxP1WUqdLiGnBfw4JaDD2QKdPy0PAG4YIhFZxuS6IlbfEjpvgtKVlqaKMz2aSSl_gFfOia/s1600/hacker-blockchain.jpg"
    },
    "content": "A threat actor with ties to the Democratic People's Republic of Korea (aka North Korea) has been observed leveraging the EtherHiding technique to distribute malware and enable cryptocurrency theft, marking the first time a state-sponsored hacking group has embraced the method.\nThe activity has been attributed by Google Threat Intelligence Group (GTIG) to a threat cluster it tracks as UNC5342,",
    "contentSnippet": "A threat actor with ties to the Democratic People's Republic of Korea (aka North Korea) has been observed leveraging the EtherHiding technique to distribute malware and enable cryptocurrency theft, marking the first time a state-sponsored hacking group has embraced the method.\nThe activity has been attributed by Google Threat Intelligence Group (GTIG) to a threat cluster it tracks as UNC5342,",
    "guid": "https://thehackernews.com/2025/10/north-korean-hackers-use-etherhiding-to.html",
    "isoDate": "2025-10-16T14:56:00.000Z",
    "itunes": {}
  }
]