[
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Featured Chrome Browser Extension Caught Intercepting Millions of Users' AI Chats",
    "link": "https://thehackernews.com/2025/12/featured-chrome-browser-extension.html",
    "pubDate": "Mon, 15 Dec 2025 23:16:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXHTpJtgXoQfYQN62WgKv5VQG11DDmx38k3E0UeuLONeGHcNFQXzppCq-HGBEG9kJNyW_2O5q7VAuvoR7aEzUpW1i3iC4PWFxF31qypiXtp8zJqKVCDF6FyxVU8_UTZUU2OKeO6lA-KkXXOk6ASt3KVG4jQ0vuwldlLYHrkeZyoVVr64i5o_nC3Jkpcqyt/s1600/chat-stealer.jpg"
    },
    "content": "A Google Chrome extension with a \"Featured\" badge and six million users has been observed silently gathering every prompt entered by users into artificial intelligence (AI)-powered chatbots like OpenAI ChatGPT, Anthropic Claude, Microsoft Copilot, DeepSeek, Google Gemini, xAI Grok, Meta AI, and Perplexity.\nThe extension in question is Urban VPN Proxy, which has a 4.7 rating on the Google Chrome",
    "contentSnippet": "A Google Chrome extension with a \"Featured\" badge and six million users has been observed silently gathering every prompt entered by users into artificial intelligence (AI)-powered chatbots like OpenAI ChatGPT, Anthropic Claude, Microsoft Copilot, DeepSeek, Google Gemini, xAI Grok, Meta AI, and Perplexity.\nThe extension in question is Urban VPN Proxy, which has a 4.7 rating on the Google Chrome",
    "guid": "https://thehackernews.com/2025/12/featured-chrome-browser-extension.html",
    "isoDate": "2025-12-15T17:46:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "FreePBX Patches Critical SQLi, File-Upload, and AUTHTYPE Bypass Flaws Enabling RCE",
    "link": "https://thehackernews.com/2025/12/freepbx-authentication-bypass-exposed.html",
    "pubDate": "Mon, 15 Dec 2025 20:02:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgv01p712eomGjpG3imw4Fow3C_gFqZq2RwuHvosDk7WWN08kcB3sVc9ZCeOyic9zqqYml63fAppF-DF0gDBXnwMBbtFE7mr81Trn8IQVOmCn4Msnx5pL87XopJicEYXlyFchV5u0hToNbvtRYR0rhG0y5S0cK06UGK8_7HFAbNJylUxYV4BDG3ZupUNh7b/s1600/zeroday.jpg"
    },
    "content": "Multiple security vulnerabilities have been disclosed in the open-source private branch exchange (PBX) platform FreePBX, including a critical flaw that could result in an authentication bypass under certain configurations.\nThe shortcomings, discovered by Horizon3.ai and reported to the project maintainers on September 15, 2025, are listed below -\n\nCVE-2025-61675 (CVSS score: 8.6) - Numerous",
    "contentSnippet": "Multiple security vulnerabilities have been disclosed in the open-source private branch exchange (PBX) platform FreePBX, including a critical flaw that could result in an authentication bypass under certain configurations.\nThe shortcomings, discovered by Horizon3.ai and reported to the project maintainers on September 15, 2025, are listed below -\n\nCVE-2025-61675 (CVSS score: 8.6) - Numerous",
    "guid": "https://thehackernews.com/2025/12/freepbx-authentication-bypass-exposed.html",
    "isoDate": "2025-12-15T14:32:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "⚡ Weekly Recap: Apple 0-Days, WinRAR Exploit, LastPass Fines, .NET RCE, OAuth Scams & More",
    "link": "https://thehackernews.com/2025/12/weekly-recap-apple-0-days-winrar.html",
    "pubDate": "Mon, 15 Dec 2025 17:54:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4BtIjueYhEXgOy1tcq1Lj5PMU8k7riwHK60xURQet4OGQewoPHZzG0F8ImGWDlbidb5KkjTrJFlLbYf4HZAfQcvHcSFOM8uQkOdgzypwcAGxlbba9hRrX4eLDfGhfoipd4wju61DtzXuVXmbDv-XeCRxBLiz-fSeKcq2MCh7dW6eSAquuF5CxjkVqXzjl/s1600/recapp.jpg"
    },
    "content": "If you use a smartphone, browse the web, or unzip files on your computer, you are in the crosshairs this week. Hackers are currently exploiting critical flaws in the daily software we all rely on—and in some cases, they started attacking before a fix was even ready.\nBelow, we list the urgent updates you need to install right now to stop these active threats.\n⚡ Threat of the Week\nApple and",
    "contentSnippet": "If you use a smartphone, browse the web, or unzip files on your computer, you are in the crosshairs this week. Hackers are currently exploiting critical flaws in the daily software we all rely on—and in some cases, they started attacking before a fix was even ready.\nBelow, we list the urgent updates you need to install right now to stop these active threats.\n⚡ Threat of the Week\nApple and",
    "guid": "https://thehackernews.com/2025/12/weekly-recap-apple-0-days-winrar.html",
    "isoDate": "2025-12-15T12:24:00.000Z",
    "itunes": {}
  },
  {
    "creator": "Bruce Schneier",
    "title": "Against the Federal Moratorium on State-Level Regulation of AI",
    "link": "https://www.schneier.com/blog/archives/2025/12/against-the-federal-moratorium-on-state-level-regulation-of-ai.html",
    "pubDate": "Mon, 15 Dec 2025 12:02:15 +0000",
    "content:encoded": "<p>Cast your mind back to May of this year: Congress was in the throes of debate over the massive <a href=\"https://www.congress.gov/bill/119th-congress/house-bill/1\">budget bill</a>. Amidst the many seismic provisions, Senator Ted Cruz <a href=\"https://iapp.org/news/a/policy-analysis-us-house-committee-seeks-moratorium-on-state-AI-rules\">dropped</a> a ticking time bomb of tech policy: a ten-year moratorium on the ability of states to regulate artificial intelligence. To many, this was catastrophic. The few massive AI companies seem to be swallowing our economy whole: their energy demands are overriding household needs, their data demands are overriding creators&#8217; copyright, and their products are triggering mass unemployment as well as new types of clinical <a href=\"https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis\">psychoses</a>. In a moment where Congress is seemingly unable to act to pass any meaningful consumer protections or market regulations, why would we hamstring the one entity evidently capable of doing so&#8212;the states? States that have already enacted consumer protections and other AI regulations, like <a href=\"https://calmatters.org/economy/technology/2025/05/state-ai-regulation-ban/\">California</a>, and those actively debating them, like <a href=\"https://commonwealthbeacon.org/opinion/mass-must-resist-congresss-proposed-moratorium-on-state-ai-regulation/\">Massachusetts</a>, were alarmed. Seventeen Republican governors wrote a <a href=\"https://www.rga.org/republican-governors-praise-one-big-beautiful-bill-urge-congress-allow-states-protect-citizens-misuse-artificial-intelligence/\">letter</a> decrying the idea, and it was ultimately killed in a rare <a href=\"https://www.senate.gov/legislative/LIS/roll_call_votes/vote1191/vote_119_1_00363.htm\">vote</a> of bipartisan near-unanimity.</p>\n<p>The idea is back. Before Thanksgiving, a House Republican leader <a href=\"https://www.techpolicy.press/its-back-congress-gears-up-for-yearend-fight-over-moratorium-on-ai-laws/\">suggested</a> they might slip it into the annual defense spending bill. Then, a draft document <a href=\"https://www.politico.com/news/2025/11/19/white-house-prepares-executive-order-to-block-state-ai-laws-00660719\">leaked</a> outlining the Trump administration&#8217;s intent to enforce the state regulatory ban through executive powers. An outpouring of opposition (including from some <a href=\"https://arstechnica.com/tech-policy/2025/12/republicans-once-again-thwart-trumps-push-to-block-state-ai-laws/\">Republican</a> state leaders) beat back that notion for a few weeks, but on Monday, Trump <a href=\"https://www.politico.com/news/2025/12/08/trump-says-ai-executive-order-limiting-state-rules-coming-this-week-00680557\">posted</a> on social media that the promised Executive Order is indeed coming soon. That would put a growing <a href=\"https://iapp.org/resources/article/us-state-ai-governance-legislation-tracker/\">cohort</a> of states, including California and New York, as well as Republican strongholds like Utah and Texas, in jeopardy.</p>\n<p>The constellation of motivations behind this proposal is clear: conservative ideology, cash, and China.</p>\n<p>The <a href=\"https://www.lawfaremedia.org/article/1-000-ai-bills--time-for-congress-to-get-serious-about-preemption\">intellectual</a> argument in favor of the moratorium is that &#8220;<a href=\"https://www.commerce.senate.gov/2025/9/sen-cruz-ai-policy-should-harness-the-power-of-american-ingenuity\">freedom</a>&#8220;-killing state regulation on AI would create a patchwork that would be difficult for AI companies to comply with, which would slow the pace of innovation needed to win an <a href=\"https://www.nytimes.com/2023/09/28/opinion/ai-safety-ethics-effective.html\">AI arms race</a> with China. AI companies and their investors have been aggressively peddling this narrative for years now, and are increasingly backing it with <a href=\"https://www.theguardian.com/technology/2025/sep/02/ai-industry-pours-millions-into-politics\">exorbitant</a> lobbying dollars. It&#8217;s a handy argument, useful not only to kill regulatory constraints, but also&#8212;companies hope&#8212;to win federal <a href=\"https://garymarcus.substack.com/p/if-you-thought-the-2008-bank-bailout\">bailouts</a> and <a href=\"https://www.theregister.com/2025/10/28/openai_100gw_power_demand/\">energy</a> subsidies.</p>\n<p>Citizens should parse that argument from their own point of view, not Big Tech&#8217;s. Preventing states from regulating AI means that those companies get to tell Washington what they want, but your state representatives are powerless to represent your own interests. Which freedom is more important to you: the freedom for a few near-monopolies to profit from AI, or the freedom for you and your neighbors to demand protections from its abuses?</p>\n<p>There is an element of this that is more partisan than ideological. Vice President J.D. Vance <a href=\"https://reason.org/commentary/next-steps-senate-rejected-ai-regulation-moratorium/\">argued</a> that federal preemption is needed to prevent &#8220;progressive&#8221; states from controlling AI&#8217;s future. This is an indicator of creeping polarization, where Democrats decry the monopolism, bias, and harms attendant to corporate AI and Republicans reflexively take the opposite side. It doesn&#8217;t help that some in the parties also have direct <a href=\"https://www.nytimes.com/2025/11/20/us/politics/howard-lutnick-family-ai.html\">financial interests</a> in the AI supply chain.</p>\n<p>But this does not need to be a partisan wedge issue: both Democrats and Republicans have strong reasons to support state-level AI legislation. Everyone shares an interest in protecting consumers from harm created by Big Tech companies. In leading the charge to kill Cruz&#8217;s initial AI moratorium proposal, Republican Senator Masha Blackburn <a href=\"https://www.politico.com/live-updates/2025/06/30/congress/blackburn-yanks-support-for-ai-moratorium-00434635\">explained</a> that &#8220;This provision could allow Big Tech to continue to exploit kids, creators, and conservatives? we can&#8217;t block states from making laws that protect their citizens.&#8221; More recently, Florida Governor Ron DeSantis wants to <a href=\"https://subscriber.politicopro.com/article/2025/11/florida-ai-desantis-trump-00661793\">regulate AI</a> in his state.</p>\n<p>The often-heard complaint that it is hard to comply with a patchwork of state regulations rings hollow. Pretty much every other consumer-facing industry has managed to deal with local regulation&#8212;automobiles, children&#8217;s toys, food, and drugs&#8212;and those regulations have been effective consumer protections. The AI industry includes some of the most valuable companies globally and has demonstrated the ability to comply with differing regulations around the world, including the EU&#8217;s <a href=\"https://artificialintelligenceact.eu/\">AI</a> and <a href=\"https://gdpr-info.eu/\">data privacy</a> regulations, substantially more onerous than those so far adopted by US states. If we can&#8217;t leverage state regulatory power to shape the AI industry, to what industry could it possibly apply?</p>\n<p>The regulatory superpower that states have here is not size and force, but rather speed and locality. We need the &#8220;laboratories of democracy&#8221; to experiment with different types of regulation that fit the specific needs and interests of their constituents and evolve responsively to the concerns they raise, especially in such a consequential and rapidly changing area such as AI.</p>\n<p>We should embrace the ability of regulation to be a driver&#8212;not a limiter&#8212;of innovation. Regulations don&#8217;t restrict companies from building better products or making more profit; they help channel that innovation in specific ways that protect the public interest. Drug safety regulations don&#8217;t prevent pharma companies from inventing drugs; they force them to invent drugs that are safe and efficacious. States can direct private innovation to serve the public.</p>\n<p>But, most importantly, regulations are needed to prevent the most dangerous impact of AI today: the <a href=\"https://www.fastcompany.com/91428050/ai-democracy-insights-to-remember\">concentration of power</a> associated with <a href=\"https://www.cnbc.com/2025/10/29/nvidia-on-track-to-hit-historic-5-trillion-valuation-amid-ai-rally.html\">trillion-</a><a href=\"https://www.cnbc.com/2025/10/29/nvidia-on-track-to-hit-historic-5-trillion-valuation-amid-ai-rally.html\">dollar</a> AI companies and the power-amplifying technologies they are producing. We outline the specific ways that the use of AI in governance can disrupt existing balances of power, and how to steer those applications towards more equitable balances, in our new book, <a href=\"https://mitpress.mit.edu/9780262049948/rewiring-democracy/\">Rewiring Democracy</a>. In the nearly complete absence of Congressional action on AI over the years, it has swept the world&#8217;s attention; it has become clear that states are the only effective policy levers we have against that concentration of power.</p>\n<p>Instead of impeding states from regulating AI, the federal government should support them to <a href=\"https://www.techpolicy.press/why-us-states-are-the-best-labs-for-public-ai/\">drive AI innovation</a>. If proponents of a moratorium worry that the private sector won&#8217;t deliver what they think is needed to compete in the new global economy, then we should engage government to help generate AI innovations that serve the public and solve the problems most important to people. Following the lead of countries like <a href=\"https://www.swiss-ai.org/apertus\">Switzerland</a>, <a href=\"https://arxiv.org/abs/2401.16182\">France</a>, and <a href=\"https://sea-lion.ai\">Singapore</a>, the US could invest in developing and deploying AI models designed as public goods: transparent, open, and useful for tasks in public administration and governance.</p>\n<p>Maybe you don&#8217;t trust the federal government to build or operate an AI tool that acts in the public interest? We don&#8217;t either. States are a much better place for this innovation to happen because they are closer to the people, they are charged with delivering most government services, they are better aligned with local political sentiments, and they have achieved <a href=\"https://news.gallup.com/poll/512651/americans-trust-local-government-congress-least.aspx\">greater trust</a>. They&#8217;re where we can test, iterate, compare, and contrast regulatory approaches that could inform eventual and better federal policy. And, while the costs of training and operating performance AI tools like large language models have <a href=\"https://aichronicle.co/the-economics-of-ai-why-training-costs-are-plummeting-and-what-it-means-for-the-future/\">declined precipitously</a>, the federal government can play a valuable role here in funding cash-strapped states to lead this kind of innovation.</p>\n<p><em>This essay was written with Nathan E. Sanders, and originally appeared in <a href=\"https://gizmodo.com/against-the-federal-moratorium-on-state-level-regulation-of-ai-2000698390\">Gizmodo</a>.</em></p>\n<p>EDITED TO ADD: Trump signed an <a href=\"https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/\">executive</a> <a href=\"https://www.axios.com/2025/12/11/trump-signs-executive-order-state-ai-laws\">order</a> <a href=\"https://www.npr.org/2025/12/11/nx-s1-5638562/trump-ai-david-sacks-executive-order\">banning</a> <a href=\"https://arstechnica.com/tech-policy/2025/12/trump-tries-to-block-state-ai-laws-himself-after-congress-decided-not-to/\">state-level</a> <a href=\"https://calmatters.org/economy/technology/2025/12/california-ai-regulation-targeted-in-trump-order/\">AI</a> regulations hours after this was published. <a href=\"https://www.csis.org/analysis/targeting-state-ai-laws-undermines-rather-advances-us-technology-leadership\">This</a> <a href=\"https://www.brennancenter.org/our-work/analysis-opinion/congress-shouldnt-stop-states-regulating-ai-especially-no-alternative\">is</a> <a href=\"https://www.theatlantic.com/ideas/2025/12/ai-regulation-moratorium-threat/685216/\">not</a> <a href=\"https://https://www.techpolicy.press/why-trumps-ai-eo-will-be-doa-in-court/\">going</a> to be the last word on the subject.</p>\n",
    "content:encodedSnippet": "Cast your mind back to May of this year: Congress was in the throes of debate over the massive budget bill. Amidst the many seismic provisions, Senator Ted Cruz dropped a ticking time bomb of tech policy: a ten-year moratorium on the ability of states to regulate artificial intelligence. To many, this was catastrophic. The few massive AI companies seem to be swallowing our economy whole: their energy demands are overriding household needs, their data demands are overriding creators’ copyright, and their products are triggering mass unemployment as well as new types of clinical psychoses. In a moment where Congress is seemingly unable to act to pass any meaningful consumer protections or market regulations, why would we hamstring the one entity evidently capable of doing so—the states? States that have already enacted consumer protections and other AI regulations, like California, and those actively debating them, like Massachusetts, were alarmed. Seventeen Republican governors wrote a letter decrying the idea, and it was ultimately killed in a rare vote of bipartisan near-unanimity.\nThe idea is back. Before Thanksgiving, a House Republican leader suggested they might slip it into the annual defense spending bill. Then, a draft document leaked outlining the Trump administration’s intent to enforce the state regulatory ban through executive powers. An outpouring of opposition (including from some Republican state leaders) beat back that notion for a few weeks, but on Monday, Trump posted on social media that the promised Executive Order is indeed coming soon. That would put a growing cohort of states, including California and New York, as well as Republican strongholds like Utah and Texas, in jeopardy.\nThe constellation of motivations behind this proposal is clear: conservative ideology, cash, and China.\nThe intellectual argument in favor of the moratorium is that “freedom“-killing state regulation on AI would create a patchwork that would be difficult for AI companies to comply with, which would slow the pace of innovation needed to win an AI arms race with China. AI companies and their investors have been aggressively peddling this narrative for years now, and are increasingly backing it with exorbitant lobbying dollars. It’s a handy argument, useful not only to kill regulatory constraints, but also—companies hope—to win federal bailouts and energy subsidies.\nCitizens should parse that argument from their own point of view, not Big Tech’s. Preventing states from regulating AI means that those companies get to tell Washington what they want, but your state representatives are powerless to represent your own interests. Which freedom is more important to you: the freedom for a few near-monopolies to profit from AI, or the freedom for you and your neighbors to demand protections from its abuses?\nThere is an element of this that is more partisan than ideological. Vice President J.D. Vance argued that federal preemption is needed to prevent “progressive” states from controlling AI’s future. This is an indicator of creeping polarization, where Democrats decry the monopolism, bias, and harms attendant to corporate AI and Republicans reflexively take the opposite side. It doesn’t help that some in the parties also have direct financial interests in the AI supply chain.\nBut this does not need to be a partisan wedge issue: both Democrats and Republicans have strong reasons to support state-level AI legislation. Everyone shares an interest in protecting consumers from harm created by Big Tech companies. In leading the charge to kill Cruz’s initial AI moratorium proposal, Republican Senator Masha Blackburn explained that “This provision could allow Big Tech to continue to exploit kids, creators, and conservatives? we can’t block states from making laws that protect their citizens.” More recently, Florida Governor Ron DeSantis wants to regulate AI in his state.\nThe often-heard complaint that it is hard to comply with a patchwork of state regulations rings hollow. Pretty much every other consumer-facing industry has managed to deal with local regulation—automobiles, children’s toys, food, and drugs—and those regulations have been effective consumer protections. The AI industry includes some of the most valuable companies globally and has demonstrated the ability to comply with differing regulations around the world, including the EU’s AI and data privacy regulations, substantially more onerous than those so far adopted by US states. If we can’t leverage state regulatory power to shape the AI industry, to what industry could it possibly apply?\nThe regulatory superpower that states have here is not size and force, but rather speed and locality. We need the “laboratories of democracy” to experiment with different types of regulation that fit the specific needs and interests of their constituents and evolve responsively to the concerns they raise, especially in such a consequential and rapidly changing area such as AI.\nWe should embrace the ability of regulation to be a driver—not a limiter—of innovation. Regulations don’t restrict companies from building better products or making more profit; they help channel that innovation in specific ways that protect the public interest. Drug safety regulations don’t prevent pharma companies from inventing drugs; they force them to invent drugs that are safe and efficacious. States can direct private innovation to serve the public.\nBut, most importantly, regulations are needed to prevent the most dangerous impact of AI today: the concentration of power associated with trillion-dollar AI companies and the power-amplifying technologies they are producing. We outline the specific ways that the use of AI in governance can disrupt existing balances of power, and how to steer those applications towards more equitable balances, in our new book, Rewiring Democracy. In the nearly complete absence of Congressional action on AI over the years, it has swept the world’s attention; it has become clear that states are the only effective policy levers we have against that concentration of power.\nInstead of impeding states from regulating AI, the federal government should support them to drive AI innovation. If proponents of a moratorium worry that the private sector won’t deliver what they think is needed to compete in the new global economy, then we should engage government to help generate AI innovations that serve the public and solve the problems most important to people. Following the lead of countries like Switzerland, France, and Singapore, the US could invest in developing and deploying AI models designed as public goods: transparent, open, and useful for tasks in public administration and governance.\nMaybe you don’t trust the federal government to build or operate an AI tool that acts in the public interest? We don’t either. States are a much better place for this innovation to happen because they are closer to the people, they are charged with delivering most government services, they are better aligned with local political sentiments, and they have achieved greater trust. They’re where we can test, iterate, compare, and contrast regulatory approaches that could inform eventual and better federal policy. And, while the costs of training and operating performance AI tools like large language models have declined precipitously, the federal government can play a valuable role here in funding cash-strapped states to lead this kind of innovation.\nThis essay was written with Nathan E. Sanders, and originally appeared in Gizmodo.\nEDITED TO ADD: Trump signed an executive order banning state-level AI regulations hours after this was published. This is not going to be the last word on the subject.",
    "dc:creator": "Bruce Schneier",
    "comments": "https://www.schneier.com/blog/archives/2025/12/against-the-federal-moratorium-on-state-level-regulation-of-ai.html#comments",
    "content": "<p>Cast your mind back to May of this year: Congress was in the throes of debate over the massive <a href=\"https://www.congress.gov/bill/119th-congress/house-bill/1\">budget bill</a>. Amidst the many seismic provisions, Senator Ted Cruz <a href=\"https://iapp.org/news/a/policy-analysis-us-house-committee-seeks-moratorium-on-state-AI-rules\">dropped</a> a ticking time bomb of tech policy: a ten-year moratorium on the ability of states to regulate artificial intelligence. To many, this was catastrophic. The few massive AI companies seem to be swallowing our economy whole: their energy demands are overriding household needs, their data demands are overriding creators&#8217; copyright, and their products are triggering mass unemployment as well as new types of clinical ...</p>",
    "contentSnippet": "Cast your mind back to May of this year: Congress was in the throes of debate over the massive budget bill. Amidst the many seismic provisions, Senator Ted Cruz dropped a ticking time bomb of tech policy: a ten-year moratorium on the ability of states to regulate artificial intelligence. To many, this was catastrophic. The few massive AI companies seem to be swallowing our economy whole: their energy demands are overriding household needs, their data demands are overriding creators’ copyright, and their products are triggering mass unemployment as well as new types of clinical ...",
    "guid": "https://www.schneier.com/?p=71317",
    "categories": [
      "Uncategorized",
      "AI",
      "laws",
      "LLM",
      "regulation"
    ],
    "isoDate": "2025-12-15T12:02:15.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "A Browser Extension Risk Guide After the ShadyPanda Campaign",
    "link": "https://thehackernews.com/2025/12/a-browser-extension-risk-guide-after.html",
    "pubDate": "Mon, 15 Dec 2025 17:25:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAAYMfLInHfns5nX7hk3SOBwCvMca-7usfovrOLFewfHgZtx0972jWlVx8u1MVE0_WqJCnczK6ZFXTZ-ir-grtXiWZcXJw6eGWzTX7supbbmuFOPY2fxfgdqcRAP_dpIW4fGawNEFOdY1wfUVlhaL4lx3_HjP20Ql7NEsL3oVu6s2xceDq0DHz_JyQU2Y/s1600/reco.jpg"
    },
    "content": "In early December 2025, security researchers exposed a cybercrime campaign that had quietly hijacked popular Chrome and Edge browser extensions on a massive scale.\nA threat group dubbed ShadyPanda spent seven years playing the long game, publishing or acquiring harmless extensions, letting them run clean for years to build trust and gain millions of installs, then suddenly flipping them into",
    "contentSnippet": "In early December 2025, security researchers exposed a cybercrime campaign that had quietly hijacked popular Chrome and Edge browser extensions on a massive scale.\nA threat group dubbed ShadyPanda spent seven years playing the long game, publishing or acquiring harmless extensions, letting them run clean for years to build trust and gain millions of installs, then suddenly flipping them into",
    "guid": "https://thehackernews.com/2025/12/a-browser-extension-risk-guide-after.html",
    "isoDate": "2025-12-15T11:55:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Phantom Stealer Spread by ISO Phishing Emails Hitting Russian Finance Sector",
    "link": "https://thehackernews.com/2025/12/phantom-stealer-spread-by-iso-phishing.html",
    "pubDate": "Mon, 15 Dec 2025 14:54:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglC802fHbjYDV11Y3NmxnCVUJfKSNEDRXAtOM722x09s0GmNQF11fP-r7betDJE1NOMAXe9DcIyEGYlWLMFf8S2B3pMRcLtLuGqqYovFNSGfnruQbydlAzRhpYQgKTZrYR68RcpYTJRjUEHTBDLDANrBWUx2zYhtcm28f7njmIuqHXI36W9omWtuD8AeFC/s1600/email.jpg"
    },
    "content": "Cybersecurity researchers have disclosed details of an active phishing campaign that's targeting a wide range of sectors in Russia with phishing emails that deliver Phantom Stealer via malicious ISO optical disc images.\nThe activity, codenamed Operation MoneyMount-ISO by Seqrite Labs, has primarily singled out finance and accounting entities, with those in the procurement, legal, payroll",
    "contentSnippet": "Cybersecurity researchers have disclosed details of an active phishing campaign that's targeting a wide range of sectors in Russia with phishing emails that deliver Phantom Stealer via malicious ISO optical disc images.\nThe activity, codenamed Operation MoneyMount-ISO by Seqrite Labs, has primarily singled out finance and accounting entities, with those in the procurement, legal, payroll",
    "guid": "https://thehackernews.com/2025/12/phantom-stealer-spread-by-iso-phishing.html",
    "isoDate": "2025-12-15T09:24:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "VolkLocker Ransomware Exposed by Hard-Coded Master Key Allowing Free Decryption",
    "link": "https://thehackernews.com/2025/12/volklocker-ransomware-exposed-by-hard.html",
    "pubDate": "Mon, 15 Dec 2025 11:03:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZP3WUgrypoDpL5dUk1TlTjzyLA_HKMjawrIv9xe9Aq6_K1u7ikP5Y7ehR-VuYhvQkb5ZHaKrz2WH_uXqmB5YaQm_t4byXd8uFCIMAqYcx15Zog3Wwt4JLdtlvCwm3nPMFI9eBNdni-5Ac7XjIrJ8JjN1D5MFRIer-S1XyPlIpw1s9ybXB_Lgt4UwJ_UJW/s1600/ransomware-key.jpg"
    },
    "content": "The pro-Russian hacktivist group known as CyberVolk (aka GLORIAMIST) has resurfaced with a new ransomware-as-a-service (RaaS) offering called VolkLocker that suffers from implementation lapses in test artifacts, allowing users to decrypt files without paying an extortion fee.\nAccording to SentinelOne, VolkLocker (aka CyberVolk 2.x) emerged in August 2025 and is capable of targeting both Windows",
    "contentSnippet": "The pro-Russian hacktivist group known as CyberVolk (aka GLORIAMIST) has resurfaced with a new ransomware-as-a-service (RaaS) offering called VolkLocker that suffers from implementation lapses in test artifacts, allowing users to decrypt files without paying an extortion fee.\nAccording to SentinelOne, VolkLocker (aka CyberVolk 2.x) emerged in August 2025 and is capable of targeting both Windows",
    "guid": "https://thehackernews.com/2025/12/volklocker-ransomware-exposed-by-hard.html",
    "isoDate": "2025-12-15T05:33:00.000Z",
    "itunes": {}
  },
  {
    "creator": "Bruce Schneier",
    "title": "Upcoming Speaking Engagements",
    "link": "https://www.schneier.com/blog/archives/2025/12/upcoming-speaking-engagements-51.html",
    "pubDate": "Sun, 14 Dec 2025 17:10:39 +0000",
    "content:encoded": "<p>This is a current list of where and when I am scheduled to speak:</p>\n<ul>\n<li>I’m speaking and signing books at the <a href=\"https://chipublib.bibliocommons.com/events/693b4543ea69de6e000fc092\">Chicago Public Library</a> in Chicago, Illinois, USA, at 6:00 PM CT on February 5, 2026. Details to come.</li>\n<li>I’m speaking at <a href=\"https://www.capricon.org/capricon44/\">Capricon 44</a> in Chicago, Illinois, USA. The convention runs February 5-8, 2026. My speaking time is TBD.</li>\n<li>I’m speaking at the <a href=\"https://mcsc.io/\">Munich Cybersecurity Conference</a> in Munich, Germany on February 12, 2026.</li>\n<li>I’m speaking at <a href=\"https://techlivecyber.wsj.com/?gaa_at=eafs&amp;gaa_n=AWEtsqf9GP4etUdWaqDIATpiE9ycqWMIVoGIzjikYLlJ64hb6H_v1QH9OYhMTxeU51U%3D&amp;gaa_ts=691df89d&amp;gaa_sig=BG9fpWuP-liL7Gi3SJgXHmS02M4ob6lp6nOh94qnwVXCWYNzJxdzOiW365xA8vKeiulrErE8mbXDvKTcqktBtQ%3D%3D\">Tech Live: Cybersecurity</a> in New York City, USA on March 11, 2026.</li>\n<li>I’m giving the Ross Anderson Lecture at the University of Cambridge’s Churchill College on March 19, 2026.</li>\n<li>I’m speaking at RSAC 2026 in San Francisco, California, USA on March 25, 2026.</li>\n</ul>\n<p>The list is maintained on <a href=\"https://www.schneier.com/events/\">this page</a>.</p>\n",
    "content:encodedSnippet": "This is a current list of where and when I am scheduled to speak:\nI’m speaking and signing books at the Chicago Public Library in Chicago, Illinois, USA, at 6:00 PM CT on February 5, 2026. Details to come.\nI’m speaking at Capricon 44 in Chicago, Illinois, USA. The convention runs February 5-8, 2026. My speaking time is TBD.\nI’m speaking at the Munich Cybersecurity Conference in Munich, Germany on February 12, 2026.\nI’m speaking at Tech Live: Cybersecurity in New York City, USA on March 11, 2026.\nI’m giving the Ross Anderson Lecture at the University of Cambridge’s Churchill College on March 19, 2026.\nI’m speaking at RSAC 2026 in San Francisco, California, USA on March 25, 2026.\nThe list is maintained on this page.",
    "dc:creator": "Bruce Schneier",
    "comments": "https://www.schneier.com/blog/archives/2025/12/upcoming-speaking-engagements-51.html#respond",
    "content": "<p>This is a current list of where and when I am scheduled to speak:</p>\n<ul>\n<li>I’m speaking and signing books at the <a href=\"https://chipublib.bibliocommons.com/events/693b4543ea69de6e000fc092\">Chicago Public Library</a> in Chicago, Illinois, USA, at 6:00 PM CT on February 5, 2026. Details to come.</li>\n<li>I’m speaking at <a href=\"https://www.capricon.org/capricon44/\">Capricon 44</a> in Chicago, Illinois, USA. The convention runs February 5-8, 2026. My speaking time is TBD.</li>\n<li>I’m speaking at the <a href=\"https://mcsc.io/\">Munich Cybersecurity Conference</a> in Munich, Germany on February 12, 2026.</li>\n<li>I’m speaking at <a href=\"https://techlivecyber.wsj.com/?gaa_at=eafs&#38;gaa_n=AWEtsqf9GP4etUdWaqDIATpiE9ycqWMIVoGIzjikYLlJ64hb6H_v1QH9OYhMTxeU51U%3D&#38;gaa_ts=691df89d&#38;gaa_sig=BG9fpWuP-liL7Gi3SJgXHmS02M4ob6lp6nOh94qnwVXCWYNzJxdzOiW365xA8vKeiulrErE8mbXDvKTcqktBtQ%3D%3D\">Tech Live: Cybersecurity</a> in New York City, USA on March 11, 2026.</li>\n<li>I’m giving the Ross Anderson Lecture at the University of Cambridge’s Churchill College on March 19, 2026...</li></ul>",
    "contentSnippet": "This is a current list of where and when I am scheduled to speak:\nI’m speaking and signing books at the Chicago Public Library in Chicago, Illinois, USA, at 6:00 PM CT on February 5, 2026. Details to come.\nI’m speaking at Capricon 44 in Chicago, Illinois, USA. The convention runs February 5-8, 2026. My speaking time is TBD.\nI’m speaking at the Munich Cybersecurity Conference in Munich, Germany on February 12, 2026.\nI’m speaking at Tech Live: Cybersecurity in New York City, USA on March 11, 2026.\nI’m giving the Ross Anderson Lecture at the University of Cambridge’s Churchill College on March 19, 2026...",
    "guid": "https://www.schneier.com/?p=71334",
    "categories": [
      "Uncategorized",
      "Schneier news"
    ],
    "isoDate": "2025-12-14T17:10:39.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "CISA Adds Actively Exploited Sierra Wireless Router Flaw Enabling RCE Attacks",
    "link": "https://thehackernews.com/2025/12/cisa-adds-actively-exploited-sierra.html",
    "pubDate": "Sat, 13 Dec 2025 18:03:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEge8ZMStOlyoSh88jViO8HEQmAAac6HNtoVoR-ufVglteBHxv-YilKP9B3sA-BJRw19H_Bflza7JA59RvM0gq34FzgFeagwBFQeuh5OfQqBbXzRbP5cXA4NRhO8ybwpIJDQEmL1rF7TIPG5xHPhFZBxKfbI4Lr5jhllzl0AbElbRUTUypn89gpVj-esiu7Q/s1600/routers.jpg"
    },
    "content": "The U.S. Cybersecurity and Infrastructure Security Agency (CISA) on Friday added a high-severity flaw impacting Sierra Wireless AirLink ALEOS routers to its Known Exploited Vulnerabilities (KEV) catalog, following reports of active exploitation in the wild.\nCVE-2018-4063 (CVSS score: 8.8/9.9) refers to an unrestricted file upload vulnerability that could be exploited to achieve remote code",
    "contentSnippet": "The U.S. Cybersecurity and Infrastructure Security Agency (CISA) on Friday added a high-severity flaw impacting Sierra Wireless AirLink ALEOS routers to its Known Exploited Vulnerabilities (KEV) catalog, following reports of active exploitation in the wild.\nCVE-2018-4063 (CVSS score: 8.8/9.9) refers to an unrestricted file upload vulnerability that could be exploited to achieve remote code",
    "guid": "https://thehackernews.com/2025/12/cisa-adds-actively-exploited-sierra.html",
    "isoDate": "2025-12-13T12:33:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Apple Issues Security Updates After Two WebKit Flaws Found Exploited in the Wild",
    "link": "https://thehackernews.com/2025/12/apple-issues-security-updates-after-two.html",
    "pubDate": "Sat, 13 Dec 2025 11:02:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSwTglRfB0k1FqqL7L502b4j1rqLa9StbW7akfVevnbqc0QsQHryjXRBajtpcOdiOOSUX119hpFSCGkZFuqxXH85aKyUMN2DwE0fKVDojo7uhXkZfEmf80438Wyd6Br5gASDHw6mLQxMc_fDPNGlgA1Op-YDJz8gYx6jqJhB_a9bKY66wuph0plkZdakTm/s1600/apple-update.jpg"
    },
    "content": "Apple on Friday released security updates for iOS, iPadOS, macOS, tvOS, watchOS, visionOS, and its Safari web browser to address two security flaws that it said have been exploited in the wild, one of which is the same flaw that was patched by Google in Chrome earlier this week.\nThe vulnerabilities are listed below -\n\nCVE-2025-43529 (CVSS score: N/A) - A use-after-free vulnerability in WebKit",
    "contentSnippet": "Apple on Friday released security updates for iOS, iPadOS, macOS, tvOS, watchOS, visionOS, and its Safari web browser to address two security flaws that it said have been exploited in the wild, one of which is the same flaw that was patched by Google in Chrome earlier this week.\nThe vulnerabilities are listed below -\n\nCVE-2025-43529 (CVSS score: N/A) - A use-after-free vulnerability in WebKit",
    "guid": "https://thehackernews.com/2025/12/apple-issues-security-updates-after-two.html",
    "isoDate": "2025-12-13T05:32:00.000Z",
    "itunes": {}
  },
  {
    "creator": "Bruce Schneier",
    "title": "Friday Squid Blogging: Giant Squid Eating a Diamondback Squid",
    "link": "https://www.schneier.com/blog/archives/2025/12/friday-squid-blogging-giant-squid-eating-a-diamondback-squid.html",
    "pubDate": "Fri, 12 Dec 2025 22:00:30 +0000",
    "content:encoded": "<p>I have no context for <a href=\"https://www.reddit.com/r/interestingasfuck/comments/1pg0ujj/new_rare_footage_of_a_live_giant_squid_caught/\">this video</a>&#8212;it&#8217;s from Reddit&#8212;but one of the commenters adds some context:</p>\n<blockquote><p>Hey everyone, squid biologist here! Wanted to add some stuff you might find interesting.</p>\n<p>With so many people carrying around cameras, we&#8217;re getting more videos of giant squid at the surface than in previous decades. We&#8217;re also starting to notice a pattern, that around this time of year (peaking in January) we see a bunch of giant squid around Japan. We don&#8217;t know why this is happening. Maybe they gather around there to mate or something? who knows! but since so many people have cameras, those one-off monster-story encounters are now caught on video, like this one (which, btw, rips. This squid looks so healthy, it&#8217;s awesome).</p>\n<p>When we see big (giant or colossal) healthy squid like this, it&#8217;s often because a fisher caught something else (either another squid or sometimes an antarctic toothfish). The squid is attracted to whatever was caught and they hop on the hook and go along for the ride when the target species is reeled in. There are a few colossal squid sightings similar to this from the southern ocean (but fewer people are down there, so fewer cameras, fewer videos). On the original instagram video, a bunch of people are like &#8220;Put it back! Release him!&#8221; etc, but he&#8217;s just enjoying dinner (obviously as the squid swims away at the end).</p></blockquote>\n<p>As usual, you can also use this squid post to talk about the security stories in the news that I haven&#8217;t covered.</p>\n<p><a href=\"https://www.schneier.com/blog/archives/2024/06/new-blog-moderation-policy.html\">Blog moderation policy.</a></p>\n",
    "content:encodedSnippet": "I have no context for this video—it’s from Reddit—but one of the commenters adds some context:\nHey everyone, squid biologist here! Wanted to add some stuff you might find interesting.\nWith so many people carrying around cameras, we’re getting more videos of giant squid at the surface than in previous decades. We’re also starting to notice a pattern, that around this time of year (peaking in January) we see a bunch of giant squid around Japan. We don’t know why this is happening. Maybe they gather around there to mate or something? who knows! but since so many people have cameras, those one-off monster-story encounters are now caught on video, like this one (which, btw, rips. This squid looks so healthy, it’s awesome).\nWhen we see big (giant or colossal) healthy squid like this, it’s often because a fisher caught something else (either another squid or sometimes an antarctic toothfish). The squid is attracted to whatever was caught and they hop on the hook and go along for the ride when the target species is reeled in. There are a few colossal squid sightings similar to this from the southern ocean (but fewer people are down there, so fewer cameras, fewer videos). On the original instagram video, a bunch of people are like “Put it back! Release him!” etc, but he’s just enjoying dinner (obviously as the squid swims away at the end).\n\nAs usual, you can also use this squid post to talk about the security stories in the news that I haven’t covered.\nBlog moderation policy.",
    "dc:creator": "Bruce Schneier",
    "comments": "https://www.schneier.com/blog/archives/2025/12/friday-squid-blogging-giant-squid-eating-a-diamondback-squid.html#comments",
    "content": "<p>I have no context for <a href=\"https://www.reddit.com/r/interestingasfuck/comments/1pg0ujj/new_rare_footage_of_a_live_giant_squid_caught/\">this video</a>&#8212;it&#8217;s from Reddit&#8212;but one of the commenters adds some context:</p>\n<blockquote><p>Hey everyone, squid biologist here! Wanted to add some stuff you might find interesting.</p>\n<p>With so many people carrying around cameras, we&#8217;re getting more videos of giant squid at the surface than in previous decades. We&#8217;re also starting to notice a pattern, that around this time of year (peaking in January) we see a bunch of giant squid around Japan. We don&#8217;t know why this is happening. Maybe they gather around there to mate or something? who knows! but since so many people have cameras, those one-off monster-story encounters are now caught on video, like this one (which, btw, rips. This squid looks so healthy, it&#8217;s awesome)...</p></blockquote>",
    "contentSnippet": "I have no context for this video—it’s from Reddit—but one of the commenters adds some context:\nHey everyone, squid biologist here! Wanted to add some stuff you might find interesting.\nWith so many people carrying around cameras, we’re getting more videos of giant squid at the surface than in previous decades. We’re also starting to notice a pattern, that around this time of year (peaking in January) we see a bunch of giant squid around Japan. We don’t know why this is happening. Maybe they gather around there to mate or something? who knows! but since so many people have cameras, those one-off monster-story encounters are now caught on video, like this one (which, btw, rips. This squid looks so healthy, it’s awesome)...",
    "guid": "https://www.schneier.com/?p=71311",
    "categories": [
      "Uncategorized",
      "squid",
      "video"
    ],
    "isoDate": "2025-12-12T22:00:30.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Fake OSINT and GPT Utility GitHub Repos Spread PyStoreRAT Malware Payloads",
    "link": "https://thehackernews.com/2025/12/fake-osint-and-gpt-utility-github-repos.html",
    "pubDate": "Sat, 13 Dec 2025 00:20:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiYgcFxpYG91gB_qImBj80xopP9yPiTuHXJwTeIXOS92Fsj6dtv4A2LK3aQxVQGKfA6sJ7T5WfeX5CEFBFrPNNH96PY7qjHCbyb7yDH5WrATaFflCN3YgQxQZJlOlKaQLK0nDtbzGwlS0FrwBVm7CwEvPkr_qSbBR1fvXX-xxIZn6XdUspYS7k2c1j87330/s1600/git.jpg"
    },
    "content": "Cybersecurity researchers are calling attention to a new campaign that's leveraging GitHub-hosted Python repositories to distribute a previously undocumented JavaScript-based Remote Access Trojan (RAT) dubbed PyStoreRAT.\n\"These repositories, often themed as development utilities or OSINT tools, contain only a few lines of code responsible for silently downloading a remote HTA file and executing",
    "contentSnippet": "Cybersecurity researchers are calling attention to a new campaign that's leveraging GitHub-hosted Python repositories to distribute a previously undocumented JavaScript-based Remote Access Trojan (RAT) dubbed PyStoreRAT.\n\"These repositories, often themed as development utilities or OSINT tools, contain only a few lines of code responsible for silently downloading a remote HTA file and executing",
    "guid": "https://thehackernews.com/2025/12/fake-osint-and-gpt-utility-github-repos.html",
    "isoDate": "2025-12-12T18:50:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "New Advanced Phishing Kits Use AI and MFA Bypass Tactics to Steal Credentials at Scale",
    "link": "https://thehackernews.com/2025/12/new-advanced-phishing-kits-use-ai-and.html",
    "pubDate": "Fri, 12 Dec 2025 19:34:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4a8r1ltQRVAHGAuHpS5vuzkIAKVgEgZQt0hi_nLCDg6akixRXUC_2sswGCHs8tFV2oLmaZd-YAwL-Yu09qMZMC64PZ2EQNycGfpxLzqwqGXaf1mPfKRZtx4XWLOKPXc1uyH_zVni2tBfNkiOUQL0Cdy4qqTRGu99eo6jDQbNq6O1-lvaZ8ZmKI3r9EG0c/s1600/1000039728.jpg"
    },
    "content": "Cybersecurity researchers have documented four new phishing kits named BlackForce, GhostFrame, InboxPrime AI, and Spiderman that are capable of facilitating credential theft at scale.\nBlackForce, first detected in August 2025, is designed to steal credentials and perform Man-in-the-Browser (MitB) attacks to capture one-time passwords (OTPs) and bypass multi-factor authentication (MFA). The kit",
    "contentSnippet": "Cybersecurity researchers have documented four new phishing kits named BlackForce, GhostFrame, InboxPrime AI, and Spiderman that are capable of facilitating credential theft at scale.\nBlackForce, first detected in August 2025, is designed to steal credentials and perform Man-in-the-Browser (MitB) attacks to capture one-time passwords (OTPs) and bypass multi-factor authentication (MFA). The kit",
    "guid": "https://thehackernews.com/2025/12/new-advanced-phishing-kits-use-ai-and.html",
    "isoDate": "2025-12-12T14:04:00.000Z",
    "itunes": {}
  },
  {
    "creator": "Bruce Schneier",
    "title": "Building Trustworthy AI Agents",
    "link": "https://www.schneier.com/blog/archives/2025/12/building-trustworthy-ai-agents.html",
    "pubDate": "Fri, 12 Dec 2025 12:00:47 +0000",
    "content:encoded": "<p>The promise of personal AI assistants rests on a dangerous assumption: that we can trust systems we haven’t made trustworthy. We can’t. And today’s versions are failing us in predictable ways: pushing us to do things against our own best interests, gaslighting us with doubt about things we are or that we know, and being unable to distinguish between who we are and who we have been. They struggle with incomplete, inaccurate, and partial context: with no standard way to move toward accuracy, no mechanism to correct sources of error, and no accountability when wrong information leads to bad decisions.</p>\n<p>These aren’t edge cases. They’re the result of building AI systems without basic integrity controls. We’re in the third leg of data security&#8212;the old CIA triad. We’re good at availability and working on confidentiality, but we’ve never properly solved integrity. Now AI personalization has exposed the gap by accelerating the harms.</p>\n<p>The scope of the problem is large. A good AI assistant will need to be trained on everything we do and will need access to our most intimate personal interactions. This means an intimacy greater than your relationship with your email provider, your social media account, your cloud storage, or your phone. It requires an AI system that is both discreet and trustworthy when provided with that data. The system needs to be accurate and complete, but it also needs to be able to keep data private: to selectively disclose pieces of it when required, and to keep it secret otherwise. No current AI system is even close to meeting this.</p>\n<p>To further development along these lines, I and others have proposed separating users’ personal data stores from the AI systems that will use them. It makes sense; the engineering expertise that designs and develops AI systems is completely orthogonal to the security expertise that ensures the confidentiality and integrity of data. And by separating them, advances in security can proceed independently from advances in AI.</p>\n<p>What would this sort of personal data store look like? Confidentiality without integrity gives you access to wrong data. Availability without integrity gives you reliable access to corrupted data. Integrity enables the other two to be meaningful. Here are six requirements. They emerge from treating integrity as the organizing principle of security to make AI trustworthy.</p>\n<p>First, it would be broadly accessible as a data repository. We each want this data to include personal data about ourselves, as well as transaction data from our interactions. It would include data we create when interacting with others&#8212;emails, texts, social media posts&#8212;and revealed preference data as inferred by other systems. Some of it would be raw data, and some of it would be processed data: revealed preferences, conclusions inferred by other systems, maybe even raw weights in a personal LLM.</p>\n<p>Second, it would be broadly accessible as a source of data. This data would need to be made accessible to different LLM systems. This can’t be tied to a single AI model. Our AI future will include many different models&#8212;some of them chosen by us for particular tasks, and some thrust upon us by others. We would want the ability for any of those models to use our data.</p>\n<p>Third, it would need to be able to prove the accuracy of data. Imagine one of these systems being used to negotiate a bank loan, or participate in a first-round job interview with an AI recruiter. In these instances, the other party will want both relevant data and some sort of proof that the data are complete and accurate.</p>\n<p>Fourth, it would be under the user’s fine-grained control and audit. This is a deeply detailed personal dossier, and the user would need to have the final say in who could access it, what portions they could access, and under what circumstances. Users would need to be able to grant and revoke this access quickly and easily, and be able to go back in time and see who has accessed it.</p>\n<p>Fifth, it would be secure. The attacks against this system are numerous. There are the obvious read attacks, where an adversary attempts to learn a person’s data. And there are also write attacks, where adversaries add to or change a user’s data. Defending against both is critical; this all implies a complex and robust authentication system.</p>\n<p>Sixth, and finally, it must be easy to use. If we’re envisioning digital personal assistants for everybody, it can’t require specialized security training to use properly.</p>\n<p>I’m not the first to suggest something like this. Researchers have proposed a “Human Context Protocol” (https://papers.ssrn.com/sol3/ papers.cfm?abstract_id=5403981) that would serve as a neutral interface for personal data of this type. And in my capacity at a company called Inrupt, Inc., I have been working on an extension of Tim Berners-Lee’s Solid protocol for distributed data ownership.</p>\n<p>The engineering expertise to build AI systems is orthogonal to the security expertise needed to protect personal data. AI companies optimize for model performance, but data security requires cryptographic verification, access control, and auditable systems. Separating the two makes sense; you can’t ignore one or the other.</p>\n<p>Fortunately, decoupling personal data stores from AI systems means security can advance independently from performance (https:// ieeexplore.ieee.org/document/ 10352412). When you own and control your data store with high integrity, AI can’t easily manipulate you because you see what data it’s using and can correct it. It can’t easily gaslight you because you control the authoritative record of your context. And you determine which historical data are relevant or obsolete. Making this all work is a challenge, but it’s the only way we can have trustworthy AI assistants.</p>\n<p>This essay was originally published in <em>IEEE Security &amp; Privacy</em>.</p>\n",
    "content:encodedSnippet": "The promise of personal AI assistants rests on a dangerous assumption: that we can trust systems we haven’t made trustworthy. We can’t. And today’s versions are failing us in predictable ways: pushing us to do things against our own best interests, gaslighting us with doubt about things we are or that we know, and being unable to distinguish between who we are and who we have been. They struggle with incomplete, inaccurate, and partial context: with no standard way to move toward accuracy, no mechanism to correct sources of error, and no accountability when wrong information leads to bad decisions.\nThese aren’t edge cases. They’re the result of building AI systems without basic integrity controls. We’re in the third leg of data security—the old CIA triad. We’re good at availability and working on confidentiality, but we’ve never properly solved integrity. Now AI personalization has exposed the gap by accelerating the harms.\nThe scope of the problem is large. A good AI assistant will need to be trained on everything we do and will need access to our most intimate personal interactions. This means an intimacy greater than your relationship with your email provider, your social media account, your cloud storage, or your phone. It requires an AI system that is both discreet and trustworthy when provided with that data. The system needs to be accurate and complete, but it also needs to be able to keep data private: to selectively disclose pieces of it when required, and to keep it secret otherwise. No current AI system is even close to meeting this.\nTo further development along these lines, I and others have proposed separating users’ personal data stores from the AI systems that will use them. It makes sense; the engineering expertise that designs and develops AI systems is completely orthogonal to the security expertise that ensures the confidentiality and integrity of data. And by separating them, advances in security can proceed independently from advances in AI.\nWhat would this sort of personal data store look like? Confidentiality without integrity gives you access to wrong data. Availability without integrity gives you reliable access to corrupted data. Integrity enables the other two to be meaningful. Here are six requirements. They emerge from treating integrity as the organizing principle of security to make AI trustworthy.\nFirst, it would be broadly accessible as a data repository. We each want this data to include personal data about ourselves, as well as transaction data from our interactions. It would include data we create when interacting with others—emails, texts, social media posts—and revealed preference data as inferred by other systems. Some of it would be raw data, and some of it would be processed data: revealed preferences, conclusions inferred by other systems, maybe even raw weights in a personal LLM.\nSecond, it would be broadly accessible as a source of data. This data would need to be made accessible to different LLM systems. This can’t be tied to a single AI model. Our AI future will include many different models—some of them chosen by us for particular tasks, and some thrust upon us by others. We would want the ability for any of those models to use our data.\nThird, it would need to be able to prove the accuracy of data. Imagine one of these systems being used to negotiate a bank loan, or participate in a first-round job interview with an AI recruiter. In these instances, the other party will want both relevant data and some sort of proof that the data are complete and accurate.\nFourth, it would be under the user’s fine-grained control and audit. This is a deeply detailed personal dossier, and the user would need to have the final say in who could access it, what portions they could access, and under what circumstances. Users would need to be able to grant and revoke this access quickly and easily, and be able to go back in time and see who has accessed it.\nFifth, it would be secure. The attacks against this system are numerous. There are the obvious read attacks, where an adversary attempts to learn a person’s data. And there are also write attacks, where adversaries add to or change a user’s data. Defending against both is critical; this all implies a complex and robust authentication system.\nSixth, and finally, it must be easy to use. If we’re envisioning digital personal assistants for everybody, it can’t require specialized security training to use properly.\nI’m not the first to suggest something like this. Researchers have proposed a “Human Context Protocol” (https://papers.ssrn.com/sol3/ papers.cfm?abstract_id=5403981) that would serve as a neutral interface for personal data of this type. And in my capacity at a company called Inrupt, Inc., I have been working on an extension of Tim Berners-Lee’s Solid protocol for distributed data ownership.\nThe engineering expertise to build AI systems is orthogonal to the security expertise needed to protect personal data. AI companies optimize for model performance, but data security requires cryptographic verification, access control, and auditable systems. Separating the two makes sense; you can’t ignore one or the other.\nFortunately, decoupling personal data stores from AI systems means security can advance independently from performance (https:// ieeexplore.ieee.org/document/ 10352412). When you own and control your data store with high integrity, AI can’t easily manipulate you because you see what data it’s using and can correct it. It can’t easily gaslight you because you control the authoritative record of your context. And you determine which historical data are relevant or obsolete. Making this all work is a challenge, but it’s the only way we can have trustworthy AI assistants.\nThis essay was originally published in IEEE Security & Privacy.",
    "dc:creator": "Bruce Schneier",
    "comments": "https://www.schneier.com/blog/archives/2025/12/building-trustworthy-ai-agents.html#comments",
    "content": "<p>The promise of personal AI assistants rests on a dangerous assumption: that we can trust systems we haven’t made trustworthy. We can’t. And today’s versions are failing us in predictable ways: pushing us to do things against our own best interests, gaslighting us with doubt about things we are or that we know, and being unable to distinguish between who we are and who we have been. They struggle with incomplete, inaccurate, and partial context: with no standard way to move toward accuracy, no mechanism to correct sources of error, and no accountability when wrong information leads to bad decisions...</p>",
    "contentSnippet": "The promise of personal AI assistants rests on a dangerous assumption: that we can trust systems we haven’t made trustworthy. We can’t. And today’s versions are failing us in predictable ways: pushing us to do things against our own best interests, gaslighting us with doubt about things we are or that we know, and being unable to distinguish between who we are and who we have been. They struggle with incomplete, inaccurate, and partial context: with no standard way to move toward accuracy, no mechanism to correct sources of error, and no accountability when wrong information leads to bad decisions...",
    "guid": "https://www.schneier.com/?p=71323",
    "categories": [
      "Uncategorized",
      "AI",
      "data privacy",
      "LLM",
      "privacy",
      "trust"
    ],
    "isoDate": "2025-12-12T12:00:47.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "Securing GenAI in the Browser: Policy, Isolation, and Data Controls That Actually Work",
    "link": "https://thehackernews.com/2025/12/securing-genai-in-browser-policy.html",
    "pubDate": "Fri, 12 Dec 2025 15:48:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDv3XARtOUivvjEnErljioTinVDnawx7b3HwjUiP4I_0_JvCPo8jro-fI8ZJHESGp8sG0SMYH4BgfZqMQMeGA95kuzhe3z3m6GgMCtBdqRtLJ1WRKUHTJv8XoRLh8H7UXi3wMH0O9BhfsBsLyRFdsv53qh-TYgOwFr3PzpdsKcZ4QryozVIoGGJhVKP_c/s1600/ai.jpg"
    },
    "content": "The browser has become the main interface to GenAI for most enterprises: from web-based LLMs and copilots, to GenAI‑powered extensions and agentic browsers like ChatGPT Atlas. Employees are leveraging the power of GenAI to draft emails, summarize documents, work on code, and analyze data, often by copying/pasting sensitive information directly into prompts or uploading files.&nbsp;\nTraditional",
    "contentSnippet": "The browser has become the main interface to GenAI for most enterprises: from web-based LLMs and copilots, to GenAI‑powered extensions and agentic browsers like ChatGPT Atlas. Employees are leveraging the power of GenAI to draft emails, summarize documents, work on code, and analyze data, often by copying/pasting sensitive information directly into prompts or uploading files. \nTraditional",
    "guid": "https://thehackernews.com/2025/12/securing-genai-in-browser-policy.html",
    "isoDate": "2025-12-12T10:18:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "New React RSC Vulnerabilities Enable DoS and Source Code Exposure",
    "link": "https://thehackernews.com/2025/12/new-react-rsc-vulnerabilities-enable.html",
    "pubDate": "Fri, 12 Dec 2025 14:25:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQz6FV9qkD3ZQ34_-_te7LAvOtZNopi3bQRTQq3TCojcBmPoef7tp-tgxFhffuFYyC5kvbdfEJMIBjwMn-rPElYlnpKG9kj89lPIAde8h_lYZRKq6a4mfiH6VWDXwIqFwimchyQ8M7RUvLd2_cAUi2aqcDyXZI_uDJcjfl1SyScuC96LV9RPhcDan3FJpQ/s1600/react-flaws.jpg"
    },
    "content": "The React team has released fixes for two new types of flaws in React Server Components (RSC) that, if successfully exploited, could result in denial-of-service (DoS) or source code exposure.\nThe team said the issues were found by the security community while attempting to exploit the patches released for CVE-2025-55182 (CVSS score: 10.0), a critical bug in RSC that has since been weaponized in",
    "contentSnippet": "The React team has released fixes for two new types of flaws in React Server Components (RSC) that, if successfully exploited, could result in denial-of-service (DoS) or source code exposure.\nThe team said the issues were found by the security community while attempting to exploit the patches released for CVE-2025-55182 (CVSS score: 10.0), a critical bug in RSC that has since been weaponized in",
    "guid": "https://thehackernews.com/2025/12/new-react-rsc-vulnerabilities-enable.html",
    "isoDate": "2025-12-12T08:55:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "React2Shell Exploitation Escalates into Large-Scale Global Attacks, Forcing Emergency Mitigation",
    "link": "https://thehackernews.com/2025/12/react2shell-exploitation-escalates-into.html",
    "pubDate": "Fri, 12 Dec 2025 14:11:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhhnft_k0pTnAA0gVIW1pPi9QlZ_5qU92WBv8wfMA9644HkYIF5DMGYcdZjGyDmHPm9WA5IoE1uLal-fqzhvX96S5gLUNTS38ggHySqo50T-brLivYF2HW2dDKYqfDJlu8kPGOuN5zVpy9w8kuC51Zz6Nsq3gQCdZ19snMvA-tfDAsFGryH6Qa1C1HMwS8M/s1600/react2shell-hacking.jpg"
    },
    "content": "The U.S. Cybersecurity and Infrastructure Security Agency (CISA) has urged federal agencies to patch the recent React2Shell vulnerability by December 12, 2025, amid reports of widespread exploitation.\nThe critical vulnerability, tracked as CVE-2025-55182 (CVSS score: 10.0), affects the React Server Components (RSC) Flight protocol. The underlying cause of the issue is an unsafe deserialization",
    "contentSnippet": "The U.S. Cybersecurity and Infrastructure Security Agency (CISA) has urged federal agencies to patch the recent React2Shell vulnerability by December 12, 2025, amid reports of widespread exploitation.\nThe critical vulnerability, tracked as CVE-2025-55182 (CVSS score: 10.0), affects the React Server Components (RSC) Flight protocol. The underlying cause of the issue is an unsafe deserialization",
    "guid": "https://thehackernews.com/2025/12/react2shell-exploitation-escalates-into.html",
    "isoDate": "2025-12-12T08:41:00.000Z",
    "itunes": {}
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "CISA Flags Actively Exploited GeoServer XXE Flaw in Updated KEV Catalog",
    "link": "https://thehackernews.com/2025/12/cisa-flags-actively-exploited-geoserver.html",
    "pubDate": "Fri, 12 Dec 2025 10:31:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBN5buANAAdi4AundLfRH7dgLMFWvHQosALn-UulNEQC0ZZxUmTJUHQyS3meXA8rVRya72n3Wl-DsJRMM6VluFoSZSdm4Ha5SijPW09wRzOE2rpQmfh0JW113iEosXFEiel84OVcf-KWHWf4IjtWwxRd0lqdzPfsvf5qrP9bKcG6TCVEEy12h24ncJChnL/s1600/geo-server.jpg"
    },
    "content": "The U.S. Cybersecurity and Infrastructure Security Agency (CISA) on Thursday added a high-severity security flaw impacting OSGeo GeoServer to its Known Exploited Vulnerabilities (KEV) catalog, based on evidence of active exploitation in the wild.\nThe vulnerability in question is CVE-2025-58360 (CVSS score: 8.2), an unauthenticated XML External Entity (XXE) flaw that affects all versions prior to",
    "contentSnippet": "The U.S. Cybersecurity and Infrastructure Security Agency (CISA) on Thursday added a high-severity security flaw impacting OSGeo GeoServer to its Known Exploited Vulnerabilities (KEV) catalog, based on evidence of active exploitation in the wild.\nThe vulnerability in question is CVE-2025-58360 (CVSS score: 8.2), an unauthenticated XML External Entity (XXE) flaw that affects all versions prior to",
    "guid": "https://thehackernews.com/2025/12/cisa-flags-actively-exploited-geoserver.html",
    "isoDate": "2025-12-12T05:01:00.000Z",
    "itunes": {}
  },
  {
    "creator": "Bruce Schneier",
    "title": "AIs Exploiting Smart Contracts",
    "link": "https://www.schneier.com/blog/archives/2025/12/ais-exploiting-smart-contracts.html",
    "pubDate": "Thu, 11 Dec 2025 17:06:05 +0000",
    "content:encoded": "<p>I have long maintained that smart contracts are a dumb idea: that a human process is actually a security feature.</p>\n<p>Here&#8217;s some <a href=\"https://red.anthropic.com/2025/smart-contracts/\">interesting research</a> on training AIs to automatically exploit smart contracts:</p>\n<blockquote><p>AI models are increasingly good at cyber tasks, as we&#8217;ve <a href=\"https://red.anthropic.com/2025/ai-for-cyber-defenders/\">written about before</a>. But what is the economic impact of these capabilities? In a recent <a href=\"https://www.matsprogram.org/\">MATS</a> and Anthropic Fellows project, our scholars investigated this question by evaluating AI agents&#8217; ability to exploit smart contracts on <a href=\"https://github.com/safety-research/SmartContract-bench\">Smart CONtracts Exploitation benchmark (SCONE-bench)</a>­a new benchmark they built comprising 405 contracts that were actually exploited between 2020 and 2025. On contracts exploited after the latest knowledge cutoffs (June 2025 for Opus 4.5 and March 2025 for other models), Claude Opus 4.5, Claude Sonnet 4.5, and GPT-5 developed exploits collectively worth $4.6 million, establishing a concrete lower bound for the economic harm these capabilities could enable. Going beyond retrospective analysis, we evaluated both Sonnet 4.5 and GPT-5 in simulation against 2,849 recently deployed contracts without any known vulnerabilities. Both agents uncovered two novel zero-day vulnerabilities and produced exploits worth $3,694, with GPT-5 doing so at an API cost of $3,476. This demonstrates as a proof-of-concept that profitable, real-world autonomous exploitation is technically feasible, a finding that underscores the need for proactive adoption of AI for defense.</p></blockquote>\n",
    "content:encodedSnippet": "I have long maintained that smart contracts are a dumb idea: that a human process is actually a security feature.\nHere’s some interesting research on training AIs to automatically exploit smart contracts:\nAI models are increasingly good at cyber tasks, as we’ve written about before. But what is the economic impact of these capabilities? In a recent MATS and Anthropic Fellows project, our scholars investigated this question by evaluating AI agents’ ability to exploit smart contracts on Smart CONtracts Exploitation benchmark (SCONE-bench)­a new benchmark they built comprising 405 contracts that were actually exploited between 2020 and 2025. On contracts exploited after the latest knowledge cutoffs (June 2025 for Opus 4.5 and March 2025 for other models), Claude Opus 4.5, Claude Sonnet 4.5, and GPT-5 developed exploits collectively worth $4.6 million, establishing a concrete lower bound for the economic harm these capabilities could enable. Going beyond retrospective analysis, we evaluated both Sonnet 4.5 and GPT-5 in simulation against 2,849 recently deployed contracts without any known vulnerabilities. Both agents uncovered two novel zero-day vulnerabilities and produced exploits worth $3,694, with GPT-5 doing so at an API cost of $3,476. This demonstrates as a proof-of-concept that profitable, real-world autonomous exploitation is technically feasible, a finding that underscores the need for proactive adoption of AI for defense.",
    "dc:creator": "Bruce Schneier",
    "comments": "https://www.schneier.com/blog/archives/2025/12/ais-exploiting-smart-contracts.html#comments",
    "content": "<p>I have long maintained that smart contracts are a dumb idea: that a human process is actually a security feature.</p>\n<p>Here&#8217;s some <a href=\"https://red.anthropic.com/2025/smart-contracts/\">interesting research</a> on training AIs to automatically exploit smart contracts:</p>\n<blockquote><p>AI models are increasingly good at cyber tasks, as we&#8217;ve <a href=\"https://red.anthropic.com/2025/ai-for-cyber-defenders/\">written about before</a>. But what is the economic impact of these capabilities? In a recent <a href=\"https://www.matsprogram.org/\">MATS</a> and Anthropic Fellows project, our scholars investigated this question by evaluating AI agents&#8217; ability to exploit smart contracts on <a href=\"https://github.com/safety-research/SmartContract-bench\">Smart CONtracts Exploitation benchmark (SCONE-bench)</a>­a new benchmark they built comprising 405 contracts that were actually exploited between 2020 and 2025. On contracts exploited after the latest knowledge cutoffs (June 2025 for Opus 4.5 and March 2025 for other models), Claude Opus 4.5, Claude Sonnet 4.5, and GPT-5 developed exploits collectively worth $4.6 million, establishing a concrete lower bound for the economic harm these capabilities could enable. Going beyond retrospective analysis, we evaluated both Sonnet 4.5 and GPT-5 in simulation against 2,849 recently deployed contracts without any known vulnerabilities. Both agents uncovered two novel zero-day vulnerabilities and produced exploits worth $3,694, with GPT-5 doing so at an API cost of $3,476. This demonstrates as a proof-of-concept that profitable, real-world autonomous exploitation is technically feasible, a finding that underscores the need for proactive adoption of AI for defense...</p></blockquote>",
    "contentSnippet": "I have long maintained that smart contracts are a dumb idea: that a human process is actually a security feature.\nHere’s some interesting research on training AIs to automatically exploit smart contracts:\nAI models are increasingly good at cyber tasks, as we’ve written about before. But what is the economic impact of these capabilities? In a recent MATS and Anthropic Fellows project, our scholars investigated this question by evaluating AI agents’ ability to exploit smart contracts on Smart CONtracts Exploitation benchmark (SCONE-bench)­a new benchmark they built comprising 405 contracts that were actually exploited between 2020 and 2025. On contracts exploited after the latest knowledge cutoffs (June 2025 for Opus 4.5 and March 2025 for other models), Claude Opus 4.5, Claude Sonnet 4.5, and GPT-5 developed exploits collectively worth $4.6 million, establishing a concrete lower bound for the economic harm these capabilities could enable. Going beyond retrospective analysis, we evaluated both Sonnet 4.5 and GPT-5 in simulation against 2,849 recently deployed contracts without any known vulnerabilities. Both agents uncovered two novel zero-day vulnerabilities and produced exploits worth $3,694, with GPT-5 doing so at an API cost of $3,476. This demonstrates as a proof-of-concept that profitable, real-world autonomous exploitation is technically feasible, a finding that underscores the need for proactive adoption of AI for defense...",
    "guid": "https://www.schneier.com/?p=71315",
    "categories": [
      "Uncategorized",
      "academic papers",
      "AI",
      "blockchain",
      "exploits"
    ],
    "isoDate": "2025-12-11T17:06:05.000Z"
  },
  {
    "creator": "info@thehackernews.com (The Hacker News)",
    "title": "ThreatsDay Bulletin: Spyware Alerts, Mirai Strikes, Docker Leaks, ValleyRAT Rootkit — and 20 More Stories",
    "link": "https://thehackernews.com/2025/12/threatsday-bulletin-spyware-alerts.html",
    "pubDate": "Thu, 11 Dec 2025 19:10:00 +0530",
    "author": "info@thehackernews.com (The Hacker News)",
    "enclosure": {
      "length": "12216320",
      "type": "image/jpeg",
      "url": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgcBEIOMDHFHbMxpqG8wwxxLRZJ4TavL-Z4AoU09ldjKg_8LpfxabE_BKPBfWUHK8GClxPJ2r3BRs078HZ8KwPrMHVHS1m5ZcCdPNR9mtUD5SZfz6WE3FLp6KnoT6HY2UhC-uV7CxqoFYiPyJw_4V0r0A5-vgSI0Znq74OKWlQxr6morZEwQUBaZd89AVg9/s1600/threatsday.jpg"
    },
    "content": "This week’s cyber stories show how fast the online world can turn risky. Hackers are sneaking malware into movie downloads, browser add-ons, and even software updates people trust. Tech giants and governments are racing to plug new holes while arguing over privacy and control. And researchers keep uncovering just how much of our digital life is still wide open.\nThe new Threatsday Bulletin",
    "contentSnippet": "This week’s cyber stories show how fast the online world can turn risky. Hackers are sneaking malware into movie downloads, browser add-ons, and even software updates people trust. Tech giants and governments are racing to plug new holes while arguing over privacy and control. And researchers keep uncovering just how much of our digital life is still wide open.\nThe new Threatsday Bulletin",
    "guid": "https://thehackernews.com/2025/12/threatsday-bulletin-spyware-alerts.html",
    "isoDate": "2025-12-11T13:40:00.000Z",
    "itunes": {}
  }
]